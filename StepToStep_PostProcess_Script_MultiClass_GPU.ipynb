{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Prediction and Postprocessing Code for Multiple Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import copy\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import time\n",
    "import tifffile as tiff\n",
    "import cv2\n",
    "import re\n",
    "\n",
    "\n",
    "from math import sqrt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib.patches import Ellipse\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from pycocotools.coco import COCO\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.spatial import ConvexHull, cKDTree\n",
    "from scipy.stats import norm\n",
    "from shapely.geometry import LineString, MultiPoint, Polygon, MultiPolygon\n",
    "from shapely.ops import unary_union\n",
    "from skimage import io, measure, morphology\n",
    "from skimage.draw import polygon, polygon2mask\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from ultralytics import YOLO\n",
    "import concurrent.futures\n",
    "import cupy as cp\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from scipy.spatial.qhull import QhullError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original models dictionary\n",
    "models = {\n",
    "    \"FCC_Model\": \"D:\\\\Machine_Learning\\\\CV Script\\\\Models\\\\FCC_2024\\\\weights\\\\best.pt\",\n",
    "    \"BCC_Model\": \"D:\\\\Machine_Learning\\\\CV Script\\Models\\\\BCC_1024\\\\weights\\\\best.pt\",\n",
    "    \"HCP_Model\": \"D:\\\\Machine_Learning\\\\CV Script\\Models\\\\HCP_1024\\\\weights\\\\best.pt\",\n",
    "    \"Combined_FCC_BCC_HCP_Model\": \"D:\\\\Machine_Learning\\\\CV Script\\\\Models\\\\Combined_FCC_BCC_HCP_1024\\\\weights\\\\best.pt\",\n",
    "    \"Combined_FCC_BCC_Model\": \"D:\\\\Machine_Learning\\\\CV Script\\\\Models\\\\Combined_FCC_BCC_1024\\\\weights\\\\best.pt\",\n",
    "    \"Combined_FCC_HCP_Model\": \"D:\\\\Machine_Learning\\\\CV Script\\\\Models\\\\Combined_FCC_HCP_1024\\\\weights\\\\best.pt\",\n",
    "    \"Combined_HCP_BCC_Model\": \"D:\\\\Machine_Learning\\\\CV Script\\\\Models\\\\Combined_BCC_HCP_1024\\\\weights\\\\best.pt\",\n",
    "    \"SwiggleSlip_Model\": \"D:\\\\Machine_Learning\\\\CV Script\\\\Models\\\\SwiggleSlip_1024\\\\weights\\\\best.pt\"\n",
    "}\n",
    "\n",
    "# Shortcut mapping\n",
    "shortcut_mapping = {\n",
    "    \"FCC\": \"FCC_Model\",\n",
    "    \"BCC\": \"BCC_Model\",\n",
    "    \"HCP\": \"HCP_Model\",\n",
    "    \"FBH\": \"Combined_FCC_BCC_HCP_Model\",\n",
    "    \"FB\": \"Combined_FCC_BCC_Model\",\n",
    "    \"FH\": \"Combined_FCC_HCP_Model\",\n",
    "    \"BH\": \"Combined_HCP_BCC_Model\",\n",
    "    \"Swiggle\": \"SwiggleSlip_Model\"\n",
    "}\n",
    "\n",
    "# Function to retrieve model path by shortcut\n",
    "def get_model_path(shortcut):\n",
    "    model_key = shortcut_mapping.get(shortcut)\n",
    "    if model_key:\n",
    "        return models[model_key]\n",
    "    else:\n",
    "        raise ValueError(f\"Shortcut '{shortcut}' not recognized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Image Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define initial paths and variables\n",
    "image_paths =  [\n",
    "                \" \"\n",
    "                ]\n",
    "\n",
    "base_output_folder = ' '  # Base output folder for all results\n",
    "\n",
    "model_path = get_model_path(\" \") # FCC, BCC, HCP or Combinations abbreviated as FBH, FB, FH, or BH.\n",
    "conversion_factor = 22.46 # To convert intensity to nanometers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to do Initial Predictions\n",
    "def apply_previous_predictions(image_path, previous_json_path, output_folder, dilation_distance=5):\n",
    "\n",
    "    # Check if the previous predictions JSON file exists\n",
    "    if os.path.exists(previous_json_path):\n",
    "        print(f\"Loading previous predictions from {previous_json_path}\")\n",
    "\n",
    "        # Load the COCO JSON file\n",
    "        with open(previous_json_path, 'r') as f:\n",
    "            coco_data = json.load(f)\n",
    "\n",
    "        # Load the image\n",
    "        image = tiff.imread(image_path)\n",
    "        image_shape = image.shape\n",
    "\n",
    "        # Create a blank mask with the same shape as the image\n",
    "        if image.ndim == 2:  # Grayscale\n",
    "            mask = np.zeros_like(image, dtype=np.uint8)\n",
    "            fill_value = 0  # Zero value for masking\n",
    "        else:  # RGB\n",
    "            mask = np.zeros((image_shape[0], image_shape[1], 3), dtype=np.uint8)\n",
    "            fill_value = (0, 0, 0)  # Zero value for masking\n",
    "\n",
    "        print(f\"Image shape: {image_shape}\")\n",
    "        print(f\"Number of annotations: {len(coco_data['annotations'])}\")\n",
    "\n",
    "        # Draw the masks on the blank image\n",
    "        for annotation in coco_data[\"annotations\"]:\n",
    "            original_segmentation = annotation[\"segmentation\"][0]  # Assuming one polygon per annotation\n",
    "            dilated_segmentation = dilate_polygon(original_segmentation, dilation_distance, image_shape)\n",
    "            if dilated_segmentation:\n",
    "                rr, cc = polygon(np.array(dilated_segmentation)[:, 1], np.array(dilated_segmentation)[:, 0], image_shape)\n",
    "                mask[rr, cc] = 1  # Mark the mask region\n",
    "\n",
    "        print(\"Mask created. Applying mask to the original image...\")\n",
    "\n",
    "        # Apply the mask to the original image\n",
    "        if image.ndim == 2:\n",
    "            masked_image = np.where(mask == 1, fill_value, image)\n",
    "        else:\n",
    "            masked_image = np.where(mask == 1, fill_value, image)\n",
    "\n",
    "        print(\"Mask applied. Saving the masked image...\")\n",
    "\n",
    "        # Construct output file name\n",
    "        base_name = os.path.basename(image_path)\n",
    "        new_file_name = f\"masked_{base_name}\"\n",
    "        output_path = os.path.join(output_folder, new_file_name)\n",
    "\n",
    "        # Save the masked image in the same format\n",
    "        tiff.imwrite(output_path, masked_image)\n",
    "\n",
    "        print(f\"Masked image saved to {output_path}\")\n",
    "\n",
    "        return output_path\n",
    "    print(f\"No previous predictions found for {image_path}\")\n",
    "    return image_path\n",
    "\n",
    "def preprocess(image_path, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    # Extract filename from the image path\n",
    "    filename = os.path.basename(image_path)\n",
    "    \n",
    "    # Add '_prepro' before the file extension in the filename\n",
    "    name, ext = os.path.splitext(filename)\n",
    "    new_filename = f\"{name}_prepro{ext}\"\n",
    "\n",
    "    # Construct the save path\n",
    "    save_path = os.path.join(output_folder, new_filename)\n",
    "\n",
    "    # Read the image\n",
    "    img = io.imread(image_path)\n",
    "    img[np.isnan(img)] = 0\n",
    "    img = (img * 255).clip(0, 255)\n",
    "    io.imsave(save_path, img)\n",
    "\n",
    "    return save_path\n",
    "\n",
    "def pad_image(image_path, output_folder, min_padding=256):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    base_name = os.path.basename(image_path)\n",
    "    new_file_name = os.path.splitext(base_name)[0] + \"_padded\" + os.path.splitext(base_name)[1]\n",
    "    output_path = os.path.join(output_folder, new_file_name)\n",
    "\n",
    "    with Image.open(image_path) as img:\n",
    "        width, height = img.size\n",
    "        padded_width = width + 2 * min_padding\n",
    "        padded_height = height + 2 * min_padding\n",
    "\n",
    "        new_width = math.ceil(padded_width / 1024) * 1024\n",
    "        new_height = math.ceil(padded_height / 1024) * 1024\n",
    "\n",
    "        new_img = Image.new(\"RGB\", (new_width, new_height), (0, 0, 0))\n",
    "        new_img.paste(img, ((new_width - width) // 2, (new_height - height) // 2))\n",
    "\n",
    "        new_img.save(output_path)\n",
    "    \n",
    "    top_padding = (new_height - height) // 2\n",
    "    left_padding = (new_width - width) // 2\n",
    "    return output_path, top_padding, top_padding, left_padding, left_padding\n",
    "\n",
    "def slice_image_to_1024(image_path, save_path, counter, shift_distance):\n",
    "    with Image.open(image_path) as img:\n",
    "        width, height = img.size\n",
    "\n",
    "        # Calculate the number of shifts horizontally and vertically\n",
    "        num_shifts_x = width // 1024\n",
    "        num_shifts_y = height // 1024\n",
    "\n",
    "        # Calculate shift based on counter, wrapping around using modulo arithmetic\n",
    "        shift_x = (counter % num_shifts_x) * shift_distance\n",
    "        shift_y = (counter // num_shifts_x) * shift_distance\n",
    "\n",
    "        # Ensure the shift does not exceed the image boundaries\n",
    "        shift_x = min(shift_x, width - 1024)\n",
    "        shift_y = min(shift_y, height - 1024)\n",
    "\n",
    "        # Update save path to include counter\n",
    "        save_path_with_counter = os.path.join(save_path, f\"slices_{counter}\")\n",
    "        if not os.path.exists(save_path_with_counter):\n",
    "            os.makedirs(save_path_with_counter)\n",
    "\n",
    "        # Calculate the number of slices in both dimensions\n",
    "        cols = (width - shift_x) // 1024\n",
    "        rows = (height - shift_y) // 1024\n",
    "\n",
    "        # Slice the image and save each piece\n",
    "        for row in range(rows):\n",
    "            for col in range(cols):\n",
    "                left = col * 1024 + shift_x\n",
    "                upper = row * 1024 + shift_y\n",
    "                right = left + 1024\n",
    "                lower = upper + 1024\n",
    "\n",
    "                slice_img = img.crop((left, upper, right, lower))\n",
    "\n",
    "                if slice_img.size == (1024, 1024):\n",
    "                    slice_img.save(os.path.join(save_path_with_counter, f\"{left}_{upper}.jpg\"))\n",
    "    \n",
    "    return save_path_with_counter\n",
    "\n",
    "def mask_to_polygon(mask, image_size, border_threshold=128, min_size=100):\n",
    "    \n",
    "    # Label the mask to identify distinct regions\n",
    "    labeled_mask, num_labels = measure.label(mask, background=0, return_num=True)\n",
    "\n",
    "    # If no regions are found, return None\n",
    "    if num_labels == 0:\n",
    "        return None\n",
    "\n",
    "    # Keep only the largest continuous region\n",
    "    largest_region = morphology.remove_small_objects(labeled_mask, min_size=min_size)\n",
    "    largest_region_mask = largest_region > 0\n",
    "\n",
    "    # Check the size of the largest region\n",
    "    if np.sum(largest_region_mask) < min_size:\n",
    "        return None\n",
    "\n",
    "    # Find contours from the binary mask of the largest region\n",
    "    contours = measure.find_contours(largest_region_mask, 0.5)\n",
    "\n",
    "    # Convert contours to polygon format and compute convex hull if needed\n",
    "    polygons = []\n",
    "    for contour in contours:\n",
    "        contour = np.flip(contour, axis=1)  # Flip from (row, col) to (x, y) format\n",
    "        contour = contour.round().astype(int)\n",
    "\n",
    "        # Check if contour is valid for convex hull computation\n",
    "        if len(np.unique(contour, axis=0)) < 3:\n",
    "            # Contour is invalid for convex hull, skip to next contour\n",
    "            continue\n",
    "\n",
    "        # Calculate the area of the original polygon\n",
    "        try:\n",
    "            original_area = ConvexHull(contour).volume\n",
    "        except QhullError:\n",
    "            # If ConvexHull computation fails, skip to next contour\n",
    "            continue\n",
    "\n",
    "        # Calculate the convex hull of the polygon\n",
    "        hull = ConvexHull(contour).vertices\n",
    "        convex_hull_polygon = contour[hull]\n",
    "\n",
    "        # Calculate the area of the convex hull\n",
    "        try:\n",
    "            convex_hull_area = ConvexHull(convex_hull_polygon).volume\n",
    "        except QhullError:\n",
    "            # If ConvexHull computation fails, use the original contour\n",
    "            polygons.append(contour.flatten().tolist())\n",
    "            continue\n",
    "\n",
    "        # Compare areas and decide whether to keep the convex hull\n",
    "        if (convex_hull_area - original_area) / original_area < 0.1:\n",
    "            polygons.append(convex_hull_polygon.flatten().tolist())\n",
    "        else:\n",
    "            polygons.append(contour.flatten().tolist())\n",
    "\n",
    "    return polygons\n",
    "\n",
    "def calculate_area_and_bbox(polygon):\n",
    "    \n",
    "    # Convert the flattened list to a 2D array of points\n",
    "    poly_array = np.array(polygon).reshape(-1, 2)\n",
    "\n",
    "    # Calculate bounding box\n",
    "    x_min = np.min(poly_array[:, 0])\n",
    "    x_max = np.max(poly_array[:, 0])\n",
    "    y_min = np.min(poly_array[:, 1])\n",
    "    y_max = np.max(poly_array[:, 1])\n",
    "    bbox = [int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)]\n",
    "\n",
    "    # Calculate area using Shoelace formula\n",
    "    x = poly_array[:, 0]\n",
    "    y = poly_array[:, 1]\n",
    "    area = 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n",
    "    area = int(area)\n",
    "\n",
    "    return area, bbox\n",
    "\n",
    "def process_images(input_folder, output_folder, model, counter, min_size, area_threshold):\n",
    "    coco_output = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [\n",
    "            {\"id\": 1, \"name\": \"Class_0\"},\n",
    "            {\"id\": 2, \"name\": \"Class_1\"}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    annotation_id = 1\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.lower().endswith(('.jpg', '.png', '.tif', '.tiff')):\n",
    "            image_path = os.path.join(input_folder, filename)\n",
    "            with Image.open(image_path) as img:\n",
    "                width, height = img.size\n",
    "\n",
    "                image_id = len(coco_output[\"images\"]) + 1\n",
    "                coco_output[\"images\"].append({\n",
    "                    \"id\": int(image_id),\n",
    "                    \"width\": int(width),\n",
    "                    \"height\": int(height),\n",
    "                    \"file_name\": filename\n",
    "                })\n",
    "\n",
    "                results = model(img, imgsz=1024, conf=0.7, device=[0])\n",
    "                \n",
    "                for r in results:\n",
    "                    if r.masks is not None and hasattr(r.masks, 'data'):\n",
    "                        masks_numpy = [mask.cpu().numpy() for mask in r.masks.data]\n",
    "\n",
    "                        cls_indices = r.boxes.cls.cpu().numpy().astype(int) if hasattr(r.boxes, 'cls') else [0]*len(masks_numpy)\n",
    "\n",
    "                        for i, mask in enumerate(masks_numpy):\n",
    "                            class_idx = cls_indices[i]\n",
    "                            category_id = int(class_idx + 1)\n",
    "                            polygon = mask_to_polygon(\n",
    "                                mask, \n",
    "                                image_size=(width, height), \n",
    "                                border_threshold=0, \n",
    "                                min_size=min_size\n",
    "                            )\n",
    "\n",
    "                            if polygon is not None:\n",
    "                                for poly in polygon:\n",
    "                                    area, bbox = calculate_area_and_bbox(poly)\n",
    "                                    # Cast area and bbox values to Python int\n",
    "                                    area = int(area)\n",
    "                                    bbox = [int(val) for val in bbox]\n",
    "\n",
    "                                    # Also ensure polygon coordinates are all ints\n",
    "                                    poly = [int(coord) for coord in poly]\n",
    "\n",
    "                                    if area >= area_threshold:\n",
    "                                        coco_output[\"annotations\"].append({\n",
    "                                            \"id\": int(annotation_id),\n",
    "                                            \"image_id\": int(image_id),\n",
    "                                            \"category_id\": int(category_id),\n",
    "                                            \"segmentation\": [poly],\n",
    "                                            \"area\": int(area),\n",
    "                                            \"bbox\": bbox,\n",
    "                                            \"iscrowd\": 0\n",
    "                                        })\n",
    "                                        annotation_id += 1\n",
    "\n",
    "    output_json = os.path.join(output_folder, f\"slice_predictions_{counter}.json\")\n",
    "    with open(output_json, 'w') as f:\n",
    "        json.dump(coco_output, f)  # Should now serialize without error\n",
    "\n",
    "    return output_json\n",
    "\n",
    "def convert_coco_to_large_image(coco_file, output_folder, padded_image_path, counter=0):\n",
    "    # Load the large image to get its size\n",
    "    with Image.open(padded_image_path) as large_img:\n",
    "        large_image_size = large_img.size\n",
    "\n",
    "    # Extract the file name of the large image\n",
    "    large_image_file_name = os.path.basename(padded_image_path)\n",
    "\n",
    "    # Load the COCO data\n",
    "    with open(coco_file, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Prepare the large image annotations structure\n",
    "    large_image_annotations = {\n",
    "        \"images\": [{\"id\": 1, \"width\": large_image_size[0], \"height\": large_image_size[1], \"file_name\": large_image_file_name}],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": coco_data[\"categories\"]\n",
    "    }\n",
    "\n",
    "    annotation_id = 1\n",
    "    for annotation in coco_data[\"annotations\"]:\n",
    "        image_id = annotation[\"image_id\"]\n",
    "        image_info = next((img for img in coco_data[\"images\"] if img[\"id\"] == image_id), None)\n",
    "        if image_info:\n",
    "            # Extract x, y offsets from the image name\n",
    "            # The slicing process produces filenames like \"left_top.jpg\"\n",
    "            # Ensure the file name contains the offsets as intended.\n",
    "            # Typically, the original code expected filenames like \"X_Y.jpg\" where X and Y are integers.\n",
    "            # If so:\n",
    "            x_offset, y_offset = map(int, image_info[\"file_name\"].split('.')[0].split('_'))\n",
    "\n",
    "            seg = annotation[\"segmentation\"]\n",
    "\n",
    "            # Ensure segmentation is flat: some COCO formats store polygons as [[x0,y0,x1,y1,...]]\n",
    "            # If there's only one polygon and it's nested, flatten it:\n",
    "            if len(seg) == 1 and isinstance(seg[0], list):\n",
    "                seg = seg[0]\n",
    "\n",
    "            # Now seg should be a flat list of coordinates: [x0, y0, x1, y1, ...]\n",
    "            # Add offsets to each coordinate\n",
    "            # Convert coord to int to avoid JSON serialization issues later\n",
    "            adjusted_seg = [int(coord + (x_offset if i % 2 == 0 else y_offset)) for i, coord in enumerate(seg)]\n",
    "\n",
    "            # Calculate area and bbox (make sure these are ints)\n",
    "            area, bbox = calculate_area_and_bbox(adjusted_seg)\n",
    "            area = int(area)\n",
    "            bbox = [int(val) for val in bbox]\n",
    "\n",
    "            # Create new annotation for the large image\n",
    "            large_image_annotations[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": 1,\n",
    "                \"category_id\": int(annotation[\"category_id\"]),\n",
    "                \"segmentation\": [adjusted_seg],\n",
    "                \"area\": area,\n",
    "                \"bbox\": bbox,\n",
    "                \"iscrowd\": int(annotation[\"iscrowd\"])\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "    # Construct output file name with counter\n",
    "    output_file_path = os.path.join(output_folder, f\"full_predictions_{counter}.json\")\n",
    "\n",
    "    # Save the adjusted annotations as a COCO JSON file\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        json.dump(large_image_annotations, f)\n",
    "\n",
    "    return output_file_path\n",
    "\n",
    "def dilate_polygon(polygon, dilation_distance, image_shape):\n",
    "    # Ensure the polygon is in the correct format: list of points where each point is a tuple (x, y)\n",
    "    polygon = [(polygon[i], polygon[i + 1]) for i in range(0, len(polygon), 2)]\n",
    "\n",
    "    # Create an empty mask\n",
    "    mask = np.zeros(image_shape, dtype=np.uint8)\n",
    "\n",
    "    # Draw the polygon on the mask\n",
    "    cv2.fillPoly(mask, [np.array(polygon, dtype=np.int32)], 1)\n",
    "\n",
    "    # Create a kernel for dilation\n",
    "    kernel = np.ones((dilation_distance*2+1, dilation_distance*2+1), np.uint8)\n",
    "\n",
    "    # Dilate the mask\n",
    "    dilated_mask = cv2.dilate(mask, kernel)\n",
    "\n",
    "    # Find contours in the dilated mask\n",
    "    contours, _ = cv2.findContours(dilated_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Assuming the largest contour is the dilated polygon\n",
    "    if contours:\n",
    "        dilated_contour = contours[0]\n",
    "        # Flatten the dilated contour to a list of points\n",
    "        dilated_polygon = dilated_contour.reshape((-1, 2)).tolist()\n",
    "    else:\n",
    "        dilated_polygon = []\n",
    "\n",
    "    return dilated_polygon\n",
    "\n",
    "def mask_image_with_coco_masks(image_path, coco_json_path, output_folder, counter, dilation_distance=5):\n",
    "    # Load the COCO JSON file\n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    image_shape = (image.height, image.width)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Iterate over annotations and apply each mask\n",
    "    for annotation in coco_data[\"annotations\"]:\n",
    "        original_segmentation = annotation[\"segmentation\"][0]  # Assuming one polygon per annotation\n",
    "\n",
    "        # Dilate the polygon\n",
    "        dilated_segmentation = dilate_polygon(original_segmentation, dilation_distance, image_shape)\n",
    "\n",
    "        # Flatten the list of tuples into a flat list of coordinates\n",
    "        flat_dilated_segmentation = [coord for point in dilated_segmentation for coord in point]\n",
    "\n",
    "        # Draw the dilated mask with black color\n",
    "        draw.polygon(flat_dilated_segmentation, fill=(0, 0, 0))\n",
    "\n",
    "    # Construct output file name with counter\n",
    "    base_name = os.path.basename(image_path)\n",
    "    new_file_name = f\"masked_{counter}\" + os.path.splitext(base_name)[1]\n",
    "    output_path = os.path.join(output_folder, new_file_name)\n",
    "\n",
    "    # Save the masked image\n",
    "    image.save(output_path)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "def compile_coco_json(output_folder, output_json, image_name):\n",
    "    compiled_annotations = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": []  # This should be populated based on your categories\n",
    "    }\n",
    "\n",
    "    # Get image dimensions\n",
    "    image_path = os.path.join(output_folder, image_name)\n",
    "    with Image.open(image_path) as img:\n",
    "        width, height = img.size\n",
    "\n",
    "    compiled_annotations[\"images\"].append({\"id\": 1, \"file_name\": image_name, \"width\": width, \"height\": height})\n",
    "\n",
    "    annotation_id = 1\n",
    "    for json_file in glob.glob(os.path.join(output_folder, \"full_predictions_*.json\")):\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            # Assuming categories are the same across all files and already populated\n",
    "            for annotation in data[\"annotations\"]:\n",
    "                # Update annotation ID and set image_id to 1\n",
    "                annotation[\"id\"] = annotation_id\n",
    "                annotation[\"image_id\"] = 1\n",
    "                compiled_annotations[\"annotations\"].append(annotation)\n",
    "                annotation_id += 1\n",
    "\n",
    "            # If categories are not populated, update them from the first file\n",
    "            if not compiled_annotations[\"categories\"]:\n",
    "                compiled_annotations[\"categories\"] = data[\"categories\"]\n",
    "\n",
    "    output_path = os.path.join(output_folder, output_json)\n",
    "\n",
    "    # Save the compiled annotations to a single JSON file\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(compiled_annotations, f)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def visualize_coco_masks(image_path, coco_json_path, output_path, figure_size=(50, 50), font_size=20):\n",
    "    # Load the COCO JSON file\n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Define colors per category_id\n",
    "    # Adjust these RGB tuples for different classes\n",
    "    category_colors = {\n",
    "        1: (255, 0, 0),   # Red for Class_0\n",
    "        2: (0, 255, 0)    # Green for Class_1\n",
    "    }\n",
    "\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert('RGB')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Load a font for drawing text\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", size=font_size)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    # Iterate over annotations and draw each segmentation\n",
    "    for annotation in coco_data[\"annotations\"]:\n",
    "        segmentation = annotation[\"segmentation\"][0]  # Assuming one polygon per annotation\n",
    "        cat_id = annotation[\"category_id\"]\n",
    "        color = category_colors.get(cat_id, (255, 255, 255))  # Default to white if not found\n",
    "\n",
    "        # Draw the segmentation polygon\n",
    "        draw.polygon(segmentation, outline=color, fill=None)\n",
    "\n",
    "        # Calculate the centroid of the mask\n",
    "        centroid_x = sum(segmentation[::2]) / (len(segmentation) / 2)\n",
    "        centroid_y = sum(segmentation[1::2]) / (len(segmentation) / 2)\n",
    "\n",
    "        # Draw the index of the mask and category ID near the centroid\n",
    "        label_text = f\"ID:{annotation['id']} C:{cat_id}\"\n",
    "        draw.text((centroid_x + 5, centroid_y), label_text, fill=color, font=font)\n",
    "\n",
    "    # Display the image in a larger figure\n",
    "    plt.figure(figsize=figure_size)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Save the image with masks\n",
    "    image.save(output_path)\n",
    "\n",
    "def visualize_coco_masks_bbox(image_path, coco_json_path, output_path, figure_size=(50, 50), font_size=20):\n",
    "    # Load the COCO JSON file\n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Define colors per category_id\n",
    "    category_colors = {\n",
    "        1: (255, 0, 0),   # Red for Class_0\n",
    "        2: (0, 255, 0)    # Green for Class_1\n",
    "    }\n",
    "\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert('RGB')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Load a font for drawing text\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", size=font_size)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    # Iterate over annotations and draw each segmentation and bounding box\n",
    "    for annotation in coco_data[\"annotations\"]:\n",
    "        segmentation = annotation[\"segmentation\"][0]  # Assuming one polygon per annotation\n",
    "        bbox = annotation[\"bbox\"]\n",
    "        cat_id = annotation[\"category_id\"]\n",
    "        color = category_colors.get(cat_id, (255, 255, 255))\n",
    "\n",
    "        # Draw the segmentation polygon\n",
    "        draw.polygon(segmentation, outline=color, fill=None)\n",
    "\n",
    "        # Draw the bounding box\n",
    "        bbox_coords = [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]]\n",
    "        draw.rectangle(bbox_coords, outline=color, width=2)\n",
    "\n",
    "        # Calculate the centroid of the mask for placing the index text\n",
    "        centroid_x = sum(segmentation[::2]) / (len(segmentation) / 2)\n",
    "        centroid_y = sum(segmentation[1::2]) / (len(segmentation) / 2)\n",
    "\n",
    "        # Draw the annotation ID and category near the centroid\n",
    "        label_text = f\"ID:{annotation['id']} C:{cat_id}\"\n",
    "        draw.text((centroid_x + 5, centroid_y), label_text, fill=color, font=font)\n",
    "\n",
    "    # Display the image in a larger figure\n",
    "    plt.figure(figsize=figure_size)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Save the image with masks and bounding boxes\n",
    "    image.save(output_path)\n",
    "\n",
    "def visualize_coco_masks_bbox_black(image_path, coco_json_path, output_path, figure_size=(50, 50), font_size=20, black=False, bbox=False):\n",
    "    # Load the COCO JSON file\n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Define colors per category_id\n",
    "    category_colors = {\n",
    "        1: (255, 0, 0),   # Red for Class_0\n",
    "        2: (0, 255, 0)    # Green for Class_1\n",
    "    }\n",
    "\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert('RGB')\n",
    "\n",
    "    # If black background requested\n",
    "    if black:\n",
    "        # Create a black canvas of the same size\n",
    "        black_img = Image.new('RGB', image.size, (0, 0, 0))\n",
    "        image = black_img\n",
    "\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Load a font for drawing text\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", size=font_size)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    # Iterate over annotations and draw each segmentation\n",
    "    for annotation in coco_data[\"annotations\"]:\n",
    "        segmentation = annotation[\"segmentation\"][0]  # Assuming one polygon per annotation\n",
    "        cat_id = annotation[\"category_id\"]\n",
    "        color = category_colors.get(cat_id, (255, 255, 255))\n",
    "\n",
    "        # Draw the segmentation polygon\n",
    "        draw.polygon(segmentation, outline=color, fill=None)\n",
    "\n",
    "        if bbox:\n",
    "            # Draw the bounding box if requested\n",
    "            bbox_coords = annotation[\"bbox\"]\n",
    "            bbox_coords = [bbox_coords[0], bbox_coords[1], bbox_coords[0] + bbox_coords[2], bbox_coords[1] + bbox_coords[3]]\n",
    "            draw.rectangle(bbox_coords, outline=color, width=2)\n",
    "\n",
    "        # Calculate centroid for placing ID and category info\n",
    "        centroid_x = sum(segmentation[::2]) / (len(segmentation) / 2)\n",
    "        centroid_y = sum(segmentation[1::2]) / (len(segmentation) / 2)\n",
    "        label_text = f\"ID:{annotation['id']} C:{cat_id}\"\n",
    "        draw.text((centroid_x + 5, centroid_y), label_text, fill=color, font=font)\n",
    "\n",
    "    # Display the image in a larger figure\n",
    "    plt.figure(figsize=figure_size)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Save the image\n",
    "    image.save(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Clean Annotations\n",
    "def clean_annotations(image_path, json_path, output_path, intensity_threshold=1):\n",
    "    #Load the image using PIL and convert to numpy array for processing\n",
    "    try:\n",
    "        pil_image = Image.open(image_path).convert('L')\n",
    "        image = np.array(pil_image)\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(f\"Failed to load image at path {image_path}: {str(e)}\")\n",
    "\n",
    "    # Load the JSON data\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    annotations = data.get('annotations', [])\n",
    "\n",
    "    # Calculate the average intensity of the entire image\n",
    "    average_intensity_image = np.mean(image)\n",
    "    print(\"Average intensity of the entire image:\", average_intensity_image)\n",
    "\n",
    "    # Dictionary for class-specific intensity thresholds, if desired.\n",
    "    # Adjust values if some classes require different minimum intensity thresholds.\n",
    "    class_thresholds = {\n",
    "        1: intensity_threshold,  # Class_0\n",
    "        2: intensity_threshold   # Class_1\n",
    "    }\n",
    "\n",
    "    filtered_annotations = []\n",
    "    for annotation in annotations:\n",
    "        if 'segmentation' not in annotation or not isinstance(annotation['segmentation'][0], list):\n",
    "            continue\n",
    "\n",
    "        category_id = annotation.get('category_id', 1)  # Default to 1 if not present\n",
    "        # Print category_id for debugging/confirmation\n",
    "        print(f\"Processing Annotation ID {annotation['id']} with Category ID {category_id}\")\n",
    "\n",
    "        # Create a mask for the current annotation\n",
    "        mask = np.zeros(image.shape, dtype=np.uint8)\n",
    "        for polygon in annotation['segmentation']:\n",
    "            points = np.array(polygon, dtype=np.int32).reshape((-1, 1, 2))\n",
    "            cv2.fillPoly(mask, [points], 255)\n",
    "\n",
    "        if np.count_nonzero(mask) == 0:\n",
    "            print(f\"Annotation ID {annotation['id']} has an empty mask. Check polygon coordinates.\")\n",
    "            continue\n",
    "\n",
    "        # Extract the pixel values where the mask is applied\n",
    "        masked_pixels = image[mask > 0]\n",
    "        if masked_pixels.size == 0:\n",
    "            print(f\"Annotation ID {annotation['id']} has a zero-area mask after application.\")\n",
    "            continue\n",
    "\n",
    "        # Calculate the average intensity of the pixels within the mask\n",
    "        average_intensity_mask = np.mean(masked_pixels)\n",
    "\n",
    "        # Use class-specific intensity threshold if available\n",
    "        category_threshold = class_thresholds.get(category_id, intensity_threshold)\n",
    "\n",
    "        # Compare the mask's average intensity to the image's average intensity scaled by threshold\n",
    "        if average_intensity_mask >= average_intensity_image * category_threshold:\n",
    "            filtered_annotations.append(annotation)\n",
    "        else:\n",
    "            print(f\"Annotation ID {annotation['id']} removed due to low intensity for Category {category_id}.\")\n",
    "\n",
    "    # Update the JSON data with filtered annotations\n",
    "    data['annotations'] = filtered_annotations\n",
    "\n",
    "    # Save the cleaned data to a JSON file\n",
    "    with open(output_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "    print(f\"Cleaned annotations saved to {output_path}\")\n",
    "\n",
    "def convert_seconds_to_hms(seconds):\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "    return int(hours), int(minutes), seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for Merging\n",
    "def calculate_polygon_area(polygon):\n",
    "    x = [point[0] for point in polygon]\n",
    "    y = [point[1] for point in polygon]\n",
    "    return 0.5 * abs(sum(x[i] * y[(i + 1) % len(polygon)] for i in range(len(polygon))) -\n",
    "                     sum(y[i] * x[(i + 1) % len(polygon)] for i in range(len(polygon))))\n",
    "\n",
    "def calculate_centroid(points):\n",
    "    area = C_x = C_y = 0.0\n",
    "    for i in range(len(points)):\n",
    "        j = (i + 1) % len(points)\n",
    "        cross = points[i][0] * points[j][1] - points[j][0] * points[i][1]\n",
    "        area += cross\n",
    "        C_x += (points[i][0] + points[j][0]) * cross\n",
    "        C_y += (points[i][1] + points[j][1]) * cross\n",
    "    area *= 0.5\n",
    "    if area != 0:\n",
    "        C_x /= (6 * area)\n",
    "        C_y /= (6 * area)\n",
    "    return (C_x, C_y)\n",
    "\n",
    "def calculate_principal_axes(points):\n",
    "    points_array = np.array(points)\n",
    "    centroid = points_array.mean(axis=0)\n",
    "    centered_points = points_array - centroid\n",
    "    covariance_matrix = np.cov(centered_points.T)  # Note the transpose for correct dimensionality\n",
    "    eigenvalues, _ = np.linalg.eig(covariance_matrix)\n",
    "    axes_lengths = np.sqrt(eigenvalues)\n",
    "    major_axis_length, minor_axis_length = np.sort(axes_lengths)[::-1]\n",
    "    return major_axis_length, minor_axis_length\n",
    "\n",
    "def calculate_orientation(points):\n",
    "    points_array = np.array(points)\n",
    "    centroid = points_array.mean(axis=0)\n",
    "    centered_points = points_array - centroid\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(centered_points)\n",
    "    major_axis_vector = pca.components_[0]  # First principal component\n",
    "    angle = np.arctan2(major_axis_vector[1], major_axis_vector[0])\n",
    "    return np.degrees(angle) % 360\n",
    "\n",
    "def create_polygon_from_points(points):\n",
    "    return Polygon(points)\n",
    "\n",
    "def calculate_intersection_area(poly1, poly2):\n",
    "    intersection = poly1.intersection(poly2)\n",
    "    return intersection.area\n",
    "\n",
    "def calculate_line_overlap_length(poly1, poly2):\n",
    "    # Ensure polygons are valid and simplifying them might resolve some edge cases\n",
    "    poly1 = poly1.simplify(0.01, preserve_topology=True)\n",
    "    poly2 = poly2.simplify(0.01, preserve_topology=True)\n",
    "\n",
    "    centroid1 = poly1.centroid.coords[0]\n",
    "    centroid2 = poly2.centroid.coords[0]\n",
    "    line = LineString([centroid1, centroid2])\n",
    "\n",
    "    intersection_line = line.intersection(poly1.intersection(poly2))\n",
    "    \n",
    "    # If there's no intersection or if it's a Point, there's no overlap length\n",
    "    if intersection_line.is_empty or intersection_line.geom_type == 'Point':\n",
    "        return 0\n",
    "\n",
    "    # Make sure we're calculating the length of a LineString\n",
    "    if isinstance(intersection_line, LineString):\n",
    "        return intersection_line.length\n",
    "    elif hasattr(intersection_line, 'geoms'):  # It's a MultiLineString or similar\n",
    "        return sum(geom.length for geom in intersection_line.geoms)\n",
    "    else:\n",
    "        # Log unexpected geometry types\n",
    "        print(f\"Unexpected geometry type: {intersection_line.geom_type}\")\n",
    "        return 0\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    return np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
    "\n",
    "def angle_difference(angle1, angle2):\n",
    "    direct_diff = abs(angle1 - angle2) % 360\n",
    "    reverse_diff = abs(180 - direct_diff)\n",
    "    return direct_diff, reverse_diff\n",
    "\n",
    "def convex_hull_area(points1, points2):\n",
    "    points = np.vstack((points1, points2))\n",
    "    hull = ConvexHull(points)\n",
    "    return hull.volume\n",
    "\n",
    "def combine_areas(area1, area2):\n",
    "    return area1 + area2\n",
    "\n",
    "def plot_orientation(ax, centroid, length, angle, color):\n",
    "    # This function adds an orientation line to the plot based on centroid, length, and angle\n",
    "    rad = np.deg2rad(angle)\n",
    "    x = [centroid[0], centroid[0] + length * np.cos(rad)]\n",
    "    y = [centroid[1], centroid[1] + length * np.sin(rad)]\n",
    "    ax.plot(x, y, color=color, linestyle='dashed')\n",
    "\n",
    "def visualize_annotations(ann1, ann2):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Plot the polygons\n",
    "    poly1 = create_polygon_from_points(ann1['polygon_points'])\n",
    "    poly2 = create_polygon_from_points(ann2['polygon_points'])\n",
    "    \n",
    "    x1, y1 = poly1.exterior.xy\n",
    "    x2, y2 = poly2.exterior.xy\n",
    "    \n",
    "    ax.fill(x1, y1, alpha=0.5, fc='red', label=f'Annotation {ann1[\"annotation_id\"]}')\n",
    "    ax.fill(x2, y2, alpha=0.5, fc='blue', label=f'Annotation {ann2[\"annotation_id\"]}')\n",
    "    \n",
    "    # Plot orientation vectors\n",
    "    plot_orientation(ax, ann1['centroid'], ann1['principal_axes_lengths']['major_axis_length'], ann1['orientation'], 'red')\n",
    "    plot_orientation(ax, ann2['centroid'], ann2['principal_axes_lengths']['major_axis_length'], ann2['orientation'], 'blue')\n",
    "    \n",
    "    # Settings and labels\n",
    "    ax.set_xlabel('X coordinate')\n",
    "    ax.set_ylabel('Y coordinate')\n",
    "    ax.set_title('Annotation Comparison')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "def process_coco_json(file_path):\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "    annotations = data.get('annotations', [])\n",
    "    results = []\n",
    "\n",
    "    # First, parse all annotations and calculate necessary geometric properties\n",
    "    for annotation in annotations:\n",
    "        if 'segmentation' in annotation:\n",
    "            # Store category_id\n",
    "            category_id = annotation.get('category_id', 1)\n",
    "            for polygon_data in annotation['segmentation']:\n",
    "                if isinstance(polygon_data, list):\n",
    "                    # Flatten polygon data if nested\n",
    "                    if all(isinstance(elem, list) for elem in polygon_data):\n",
    "                        polygon_data = [coord for sublist in polygon_data for coord in sublist]\n",
    "                    polygon_points = [(polygon_data[i], polygon_data[i + 1]) for i in range(0, len(polygon_data), 2)]\n",
    "                    area = calculate_polygon_area(polygon_points)\n",
    "                    centroid = calculate_centroid(polygon_points)\n",
    "                    orientation = calculate_orientation(polygon_points)\n",
    "                    major_axis_length, minor_axis_length = calculate_principal_axes(polygon_points)\n",
    "                    result = {\n",
    "                        'annotation_id': annotation['id'],\n",
    "                        'category_id': category_id,  # Store category_id\n",
    "                        'area': area,\n",
    "                        'bbox': annotation['bbox'],\n",
    "                        'centroid': centroid,\n",
    "                        'orientation': orientation,\n",
    "                        'principal_axes_lengths': {\n",
    "                            'major_axis_length': major_axis_length,\n",
    "                            'minor_axis_length': minor_axis_length\n",
    "                        },\n",
    "                        'polygon_points': polygon_points,\n",
    "                        'nearby_ids': [],\n",
    "                        'segmentation': annotation['segmentation']\n",
    "                    }\n",
    "                    results.append(result)\n",
    "\n",
    "    # Calculate proximity of annotations\n",
    "    for i, ann1 in enumerate(results):\n",
    "        for j, ann2 in enumerate(results):\n",
    "            if i != j:\n",
    "                distance = euclidean_distance(ann1['centroid'], ann2['centroid'])\n",
    "                if distance < 3 * ann1['principal_axes_lengths']['major_axis_length']:\n",
    "                    ann1['nearby_ids'].append(ann2['annotation_id'])\n",
    "\n",
    "    return results\n",
    "\n",
    "def merge_annotations(ann1, ann2):\n",
    "    # Combine polygon points\n",
    "    all_points = ann1['polygon_points'] + ann2['polygon_points']\n",
    "\n",
    "    multipoint = MultiPoint(all_points)\n",
    "    convex_hull = multipoint.convex_hull\n",
    "\n",
    "    if convex_hull.geom_type == 'Polygon':\n",
    "        hull_points_list = list(convex_hull.exterior.coords)\n",
    "    else:\n",
    "        # If not a polygon, fallback to all_points\n",
    "        hull_points_list = all_points\n",
    "\n",
    "    new_area = calculate_polygon_area(hull_points_list)\n",
    "    new_centroid = calculate_centroid(hull_points_list)\n",
    "    new_orientation = calculate_orientation(hull_points_list)\n",
    "    new_major_axis_length, new_minor_axis_length = calculate_principal_axes(hull_points_list)\n",
    "    bbox = [int(coord) for coord in convex_hull.bounds]\n",
    "\n",
    "    # Determine category_id:\n",
    "    # Assuming we only merge annotations of the same category.\n",
    "    # If not, add logic to determine the resulting category.\n",
    "    category_id = ann1['category_id']\n",
    "\n",
    "    ancestors = set(ann1.get('ancestors', [ann1['annotation_id']]) + ann2.get('ancestors', [ann2['annotation_id']]))\n",
    "\n",
    "    return {\n",
    "        'annotation_id': f\"merged-{ann1['annotation_id']}-{ann2['annotation_id']}\",\n",
    "        'category_id': category_id,\n",
    "        'area': new_area,\n",
    "        'centroid': new_centroid,\n",
    "        'orientation': new_orientation,\n",
    "        'principal_axes_lengths': {\n",
    "            'major_axis_length': new_major_axis_length,\n",
    "            'minor_axis_length': new_minor_axis_length\n",
    "        },\n",
    "        'polygon_points': hull_points_list,\n",
    "        'segmentation': [coord for point in hull_points_list for coord in point],\n",
    "        'bbox': bbox,\n",
    "        'ancestors': list(ancestors)\n",
    "    }\n",
    "\n",
    "def check_compatibility_single(ann1, ann2, categories_to_merge=None):\n",
    "    # If categories_to_merge is provided, ensure both annotations are in these categories\n",
    "    if categories_to_merge is not None:\n",
    "        if ann1['category_id'] not in categories_to_merge or ann2['category_id'] not in categories_to_merge:\n",
    "            return False  # Don't merge if either annotation's category is not in allowed list\n",
    "\n",
    "    # Prevent merging if any ancestors are the same\n",
    "    if set(ann1.get('ancestors', [ann1['annotation_id']])) & set(ann2.get('ancestors', [ann2['annotation_id']])):\n",
    "        return False\n",
    "    \n",
    "    poly1 = Polygon(ann1['polygon_points'])\n",
    "    poly2 = Polygon(ann2['polygon_points'])\n",
    "\n",
    "    intersection = poly1.intersection(poly2)\n",
    "    intersection_area = intersection.area\n",
    "\n",
    "    overlap_percent_ann1 = intersection_area / ann1['area'] if ann1['area'] > 0 else 0\n",
    "    overlap_percent_ann2 = intersection_area / ann2['area'] if ann2['area'] > 0 else 0\n",
    "\n",
    "    if overlap_percent_ann1 >= 0.5 or overlap_percent_ann2 >= 0.5:\n",
    "        return True\n",
    "\n",
    "    major_axis_length_ann1 = ann1['principal_axes_lengths']['major_axis_length']\n",
    "    minor_axis_length_ann1 = ann1['principal_axes_lengths']['minor_axis_length']\n",
    "    diagonal_ratio_ann1 = minor_axis_length_ann1 / major_axis_length_ann1 if major_axis_length_ann1 > 0 else 0\n",
    "\n",
    "    major_axis_length_ann2 = ann2['principal_axes_lengths']['major_axis_length']\n",
    "    minor_axis_length_ann2 = ann2['principal_axes_lengths']['minor_axis_length']\n",
    "    diagonal_ratio_ann2 = minor_axis_length_ann2 / major_axis_length_ann2 if major_axis_length_ann2 > 0 else 0\n",
    "\n",
    "    ignore_orientation = diagonal_ratio_ann1 > 0.33 or diagonal_ratio_ann2 > 0.33\n",
    "\n",
    "    overlap_length = calculate_line_overlap_length(poly1, poly2)\n",
    "    dist = euclidean_distance(ann1['centroid'], ann2['centroid'])\n",
    "    direct_diff, reverse_diff = angle_difference(ann1['orientation'], ann2['orientation'])\n",
    "    orientation_diff = min(direct_diff, reverse_diff)\n",
    "    convex_hull_ratio = convex_hull_area(np.array(ann1['polygon_points']), np.array(ann2['polygon_points'])) / combine_areas(ann1['area'], ann2['area'])\n",
    "\n",
    "    return (dist < 3 * max(major_axis_length_ann1, major_axis_length_ann2) and\n",
    "            (ignore_orientation or orientation_diff <= 5) and\n",
    "            convex_hull_ratio < 1.3 and\n",
    "            dist > ((major_axis_length_ann1 + major_axis_length_ann2 - overlap_length)))\n",
    "\n",
    "def progressive_merge_annotations(annotations, categories_to_merge=None):\n",
    "    # If categories_to_merge is not specified, assume all categories are mergable\n",
    "    # Otherwise, merging will only happen among these categories.\n",
    "    if categories_to_merge is None:\n",
    "        categories_to_merge = []\n",
    "\n",
    "    points = np.array([ann['centroid'] for ann in annotations])\n",
    "    kd_tree = cKDTree(points)\n",
    "\n",
    "    remaining_annotations = copy.deepcopy(annotations)\n",
    "    merged = True\n",
    "\n",
    "    while merged:\n",
    "        merged = False\n",
    "        new_annotations = []\n",
    "        new_points = []\n",
    "        skip_indices = set()\n",
    "\n",
    "        for i, ann1 in enumerate(remaining_annotations):\n",
    "            if i in skip_indices:\n",
    "                continue\n",
    "\n",
    "            # Only attempt merges if ann1 is in the allowed categories\n",
    "            if ann1['category_id'] in categories_to_merge:\n",
    "                # Query close neighbors\n",
    "                indices = kd_tree.query(points[i], k=50)[1]\n",
    "                indices = [index for index in indices if index < len(remaining_annotations) and index not in skip_indices and index != i]\n",
    "\n",
    "                for j in indices:\n",
    "                    ann2 = remaining_annotations[j]\n",
    "\n",
    "                    if check_compatibility_single(ann1, ann2, categories_to_merge=categories_to_merge):\n",
    "                        merged_annotation = merge_annotations(ann1, ann2)\n",
    "                        # Update properties\n",
    "                        merged_annotation['segmentation'] = [coord for point in merged_annotation['polygon_points'] for coord in point]\n",
    "                        merged_annotation['centroid'] = calculate_centroid(merged_annotation['polygon_points'])\n",
    "                        x_coords, y_coords = zip(*merged_annotation['polygon_points'])\n",
    "                        merged_annotation['bbox'] = [\n",
    "                            int(min(x_coords)),\n",
    "                            int(min(y_coords)),\n",
    "                            int(max(x_coords) - min(x_coords)),\n",
    "                            int(max(y_coords) - min(y_coords))\n",
    "                        ]\n",
    "                        merged_annotation['nearby_ids'] = list(set(ann1.get('nearby_ids', []) + ann2.get('nearby_ids', [])) - {ann1['annotation_id'], ann2['annotation_id']})\n",
    "                        \n",
    "                        new_annotations.append(merged_annotation)\n",
    "                        new_points.append([\n",
    "                            merged_annotation['bbox'][0] + merged_annotation['bbox'][2] / 2, \n",
    "                            merged_annotation['bbox'][1] + merged_annotation['bbox'][3] / 2\n",
    "                        ])\n",
    "\n",
    "                        skip_indices.update({i, j})\n",
    "                        merged = True\n",
    "                        break\n",
    "\n",
    "                if i not in skip_indices:\n",
    "                    # ann1 not merged\n",
    "                    new_annotations.append(ann1)\n",
    "                    new_points.append(points[i])\n",
    "            else:\n",
    "                # ann1 category not in merge list, skip merging and just pass through\n",
    "                new_annotations.append(ann1)\n",
    "                new_points.append(points[i])\n",
    "\n",
    "        points = np.array(new_points)\n",
    "        kd_tree = cKDTree(points)\n",
    "        remaining_annotations = new_annotations\n",
    "\n",
    "    return remaining_annotations\n",
    "\n",
    "def flatten_segmentation(segmentation):\n",
    "    if isinstance(segmentation, list):\n",
    "        if all(isinstance(item, list) for item in segmentation):\n",
    "            return [coord for sublist in segmentation for coord in sublist]\n",
    "        elif all(isinstance(item, (int, float)) for item in segmentation):\n",
    "            return segmentation\n",
    "    return None\n",
    "\n",
    "def update_json_file(annotations, file_path, image_path):\n",
    "    # We will no longer force a single category_id since annotations can have multiple categories\n",
    "    with Image.open(image_path) as img:\n",
    "        width, height = img.size\n",
    "        file_name = os.path.basename(image_path)\n",
    "\n",
    "    # Collect category IDs from annotations\n",
    "    category_ids = sorted(set(ann['category_id'] for ann in annotations))\n",
    "    # For simplicity, we do not have category names here, you might want to add them \n",
    "    # if you have them available. For now, just name them generically.\n",
    "    categories = [{\"id\": cid, \"name\": f\"category_{cid}\", \"supercategory\": \"none\"} for cid in category_ids]\n",
    "\n",
    "    coco_data = {\n",
    "        \"images\": [{\"id\": 1, \"width\": width, \"height\": height, \"file_name\": file_name}],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": categories\n",
    "    }\n",
    "\n",
    "    for idx, ann in enumerate(annotations, start=1):\n",
    "        standardized_segmentation = flatten_segmentation(ann['segmentation'])\n",
    "        if standardized_segmentation is not None:\n",
    "            coco_ann = {\n",
    "                \"id\": idx,\n",
    "                \"image_id\": 1,\n",
    "                \"category_id\": ann['category_id'],  # Use the annotation's own category_id\n",
    "                \"segmentation\": [standardized_segmentation],\n",
    "                \"area\": ann['area'],\n",
    "                \"bbox\": ann['bbox'],\n",
    "                \"iscrowd\": 0\n",
    "            }\n",
    "            coco_data['annotations'].append(coco_ann)\n",
    "        else:\n",
    "            print(f\"Error in segmentation format for annotation {idx}, unable to process.\")\n",
    "\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(coco_data, f, indent=4)\n",
    "\n",
    "def check_specific_annotations(annotations, id1, id2):\n",
    "    # Retrieve the annotations by IDs\n",
    "    ann1 = next((ann for ann in annotations if ann['annotation_id'] == id1), None)\n",
    "    ann2 = next((ann for ann in annotations if ann['annotation_id'] == id2), None)\n",
    "\n",
    "    if ann1 is None or ann2 is None:\n",
    "        print(f\"Annotation not found for given IDs: {id1}, {id2}\")\n",
    "        return\n",
    "    \n",
    "    # Convert points to polygons\n",
    "    poly1 = create_polygon_from_points(ann1['polygon_points'])\n",
    "    poly2 = create_polygon_from_points(ann2['polygon_points'])\n",
    "    \n",
    "    # Perform compatibility check\n",
    "    is_compatible = check_compatibility_single(ann1, ann2)\n",
    "    \n",
    "    # Print details of the compatibility checks\n",
    "    print(f\"Checking compatibility between Annotation {id1} and Annotation {id2}:\")\n",
    "    print(f\"Compatible: {is_compatible}\")\n",
    "\n",
    "    # Use pre-calculated principal axes lengths to compute the diagonal ratio\n",
    "    major_axis_length_ann1 = ann1['principal_axes_lengths']['major_axis_length']\n",
    "    minor_axis_length_ann1 = ann1['principal_axes_lengths']['minor_axis_length']\n",
    "    diagonal_ratio_ann1 = minor_axis_length_ann1 / major_axis_length_ann1 if major_axis_length_ann1 > 0 else 0\n",
    "\n",
    "    major_axis_length_ann2 = ann2['principal_axes_lengths']['major_axis_length']\n",
    "    minor_axis_length_ann2 = ann2['principal_axes_lengths']['minor_axis_length']\n",
    "    diagonal_ratio_ann2 = minor_axis_length_ann2 / major_axis_length_ann2 if major_axis_length_ann2 > 0 else 0\n",
    "\n",
    "    # Check if diagonal ratio for either polygon exceeds 0.8\n",
    "    ignore_orientation = diagonal_ratio_ann1 > 0.33 or diagonal_ratio_ann2 > 0.33    \n",
    "\n",
    "    # Additional details\n",
    "    intersection_area = calculate_intersection_area(poly1, poly2)\n",
    "    overlap_length = calculate_line_overlap_length(poly1, poly2)\n",
    "    dist = euclidean_distance(ann1['centroid'], ann2['centroid'])\n",
    "    direct_diff, reverse_diff = angle_difference(ann1['orientation'], ann2['orientation'])\n",
    "    orientation_diff = min(direct_diff, reverse_diff)\n",
    "    convex_hull_ratio = convex_hull_area(np.array(ann1['polygon_points']), np.array(ann2['polygon_points'])) / combine_areas(ann1['area'], ann2['area'])\n",
    "    dist_Threshold = ann1['principal_axes_lengths']['major_axis_length'] + ann2['principal_axes_lengths']['major_axis_length'] - overlap_length\n",
    "\n",
    "    print(f\"Intersection Area: {intersection_area}\")\n",
    "    print(f\"Overlap Length: {overlap_length}\")\n",
    "    print('----------')\n",
    "    print(f\"Distance Thresholds: {dist_Threshold}\")\n",
    "    print(f\"Distance Between Centroids: {dist}\")\n",
    "    print(f\"Should be greater than Threshold\")\n",
    "    print('----------')\n",
    "    print(f'Ignore Orientation:{ignore_orientation}')\n",
    "    print(f'Orientation Threshold: 5 degrees')\n",
    "    print(f\"Orientation Difference: {orientation_diff}\")\n",
    "    print(f\"Direct Difference: {direct_diff}\")\n",
    "    print(f\"Reverse Difference: {reverse_diff}\")\n",
    "    print(f\"Should be less than Threshold\")\n",
    "    print('----------')\n",
    "    print(f\"Convex Hull Threshold: 1.3\")\n",
    "    print(f\"Convex Hull Ratio: {convex_hull_ratio}\")\n",
    "    print(f\"Should be less than Threshold\")\n",
    "\n",
    "    # Optional: Visualize the annotations for visual inspection\n",
    "    visualize_annotations(ann1, ann2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjustment Functions\n",
    "def add_annotations_to_coco_json(file1_path, file2_path, output_path):\n",
    "    # Load the first file\n",
    "    with open(file1_path, 'r') as f:\n",
    "        data1 = json.load(f)\n",
    "\n",
    "    # If the second file doesn't exist, just save the first file's data to output\n",
    "    if not os.path.exists(file2_path):\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(data1, f, indent=4)\n",
    "        return data1\n",
    "\n",
    "    # Load the second file\n",
    "    with open(file2_path, 'r') as f:\n",
    "        data2 = json.load(f)\n",
    "\n",
    "    # Ensure that both files have 'annotations', 'images', and 'categories' keys\n",
    "    data1.setdefault('annotations', [])\n",
    "    data1.setdefault('images', [])\n",
    "    data1.setdefault('categories', [])\n",
    "    data2.setdefault('annotations', [])\n",
    "    data2.setdefault('images', [])\n",
    "    data2.setdefault('categories', [])\n",
    "\n",
    "    # Build dictionaries to map category names to IDs for data2\n",
    "    data2_categories_by_name = {cat['name']: cat['id'] for cat in data2['categories']}\n",
    "    # If no categories exist in data2, start fresh\n",
    "    max_category_id = max([cat['id'] for cat in data2['categories']], default=0)\n",
    "\n",
    "    # Re-map category_ids in data1 to match data2\n",
    "    category_id_map = {}\n",
    "    for cat in data1['categories']:\n",
    "        cat_name = cat['name']\n",
    "        if cat_name in data2_categories_by_name:\n",
    "            # Category exists in data2\n",
    "            category_id_map[cat['id']] = data2_categories_by_name[cat_name]\n",
    "        else:\n",
    "            # Need to add this category to data2\n",
    "            max_category_id += 1\n",
    "            new_cat_id = max_category_id\n",
    "            data2['categories'].append({\n",
    "                \"id\": new_cat_id,\n",
    "                \"name\": cat_name,\n",
    "                \"supercategory\": cat.get(\"supercategory\", \"none\")\n",
    "            })\n",
    "            data2_categories_by_name[cat_name] = new_cat_id\n",
    "            category_id_map[cat['id']] = new_cat_id\n",
    "\n",
    "    # Now re-map the category_id of each annotation in data1 using category_id_map\n",
    "    for annotation in data1['annotations']:\n",
    "        old_cat_id = annotation['category_id']\n",
    "        if old_cat_id in category_id_map:\n",
    "            annotation['category_id'] = category_id_map[old_cat_id]\n",
    "        else:\n",
    "            # If not found, default to a known category or raise an error\n",
    "            # Here we raise an error because every category should be mapped\n",
    "            raise ValueError(f\"No category mapping found for category_id {old_cat_id}\")\n",
    "\n",
    "    # Start IDs from the maximum found in data2 + 1 for annotations and images\n",
    "    max_annotation_id = max([ann['id'] for ann in data2['annotations']], default=0) + 1\n",
    "    max_image_id = max([img['id'] for img in data2['images']], default=0) + 1\n",
    "\n",
    "    # Update image IDs in data1\n",
    "    image_id_map = {}\n",
    "    for image in data1['images']:\n",
    "        original_id = image['id']\n",
    "        new_id = max_image_id\n",
    "        image_id_map[original_id] = new_id\n",
    "        image['id'] = new_id\n",
    "        max_image_id += 1\n",
    "\n",
    "    # Update annotation IDs and their image references in data1\n",
    "    for annotation in data1['annotations']:\n",
    "        original_id = annotation['id']\n",
    "        new_id = max_annotation_id\n",
    "        annotation['id'] = new_id\n",
    "        # Update image_id to the mapped ID\n",
    "        if annotation['image_id'] in image_id_map:\n",
    "            annotation['image_id'] = image_id_map[annotation['image_id']]\n",
    "        else:\n",
    "            raise ValueError(f\"No image mapping found for image_id {annotation['image_id']}\")\n",
    "\n",
    "        max_annotation_id += 1\n",
    "\n",
    "    # Append data1's annotations and images to data2\n",
    "    data2['annotations'].extend(data1['annotations'])\n",
    "    data2['images'].extend(data1['images'])\n",
    "    # Categories are already merged and updated in data2\n",
    "\n",
    "    # Save the updated data\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(data2, f, indent=4)\n",
    "\n",
    "    return data2\n",
    "\n",
    "def adjust_coco_annotations(coco_json_path, top_padding, bottom_padding, left_padding, right_padding, output_json_path):\n",
    "    with open(coco_json_path) as file:\n",
    "        coco_data = json.load(file)\n",
    "\n",
    "    # Adjust image dimensions\n",
    "    for image in coco_data['images']:\n",
    "        image['height'] -= (top_padding + bottom_padding)\n",
    "        image['width'] -= (left_padding + right_padding)\n",
    "\n",
    "    # Adjust annotation coordinates\n",
    "    for annotation in coco_data['annotations']:\n",
    "        bbox = annotation['bbox']\n",
    "        bbox[0] -= left_padding\n",
    "        bbox[1] -= top_padding\n",
    "        annotation['bbox'] = bbox\n",
    "\n",
    "        if 'segmentation' in annotation:\n",
    "            new_segmentation = []\n",
    "            for segment in annotation['segmentation']:\n",
    "                if isinstance(segment, list):\n",
    "                    new_polygon = []\n",
    "                    # Iterate in pairs\n",
    "                    points = iter(segment)\n",
    "                    for x, y in zip(points, points):\n",
    "                        new_x = x - left_padding\n",
    "                        new_y = y - top_padding\n",
    "                        new_polygon.extend([new_x, new_y])\n",
    "                    new_segmentation.append(new_polygon)\n",
    "            annotation['segmentation'] = new_segmentation\n",
    "\n",
    "    with open(output_json_path, 'w') as file:\n",
    "        json.dump(coco_data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics Functions\n",
    "def load_json(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def load_coco_annotations(annotation_path):\n",
    "    coco = COCO(annotation_path)\n",
    "    return coco\n",
    "\n",
    "def calc_bounds(mean, std_dev, upper_multiplier, lower_multiplier):\n",
    "    return mean + upper_multiplier * std_dev, mean - lower_multiplier * std_dev\n",
    "\n",
    "def filter_coordinates(x, y, z, rounds, background_value=0):\n",
    "    filtered_x, filtered_y, filtered_z = cp.array(x), cp.array(y), cp.array(z)\n",
    "    for round_details in rounds:\n",
    "        mask = filtered_z > background_value\n",
    "        filtered_x, filtered_y, filtered_z = filtered_x[mask], filtered_y[mask], filtered_z[mask]\n",
    "\n",
    "        mean_z, std_dev_z = cp.mean(filtered_z), cp.std(filtered_z)\n",
    "        upper_z, lower_z = calc_bounds(mean_z, std_dev_z, *round_details['z'])\n",
    "        z_mask = (filtered_z <= upper_z) & (filtered_z >= lower_z)\n",
    "\n",
    "        mean_x, std_dev_x = cp.mean(filtered_x[z_mask]), cp.std(filtered_x[z_mask])\n",
    "        upper_x, lower_x = calc_bounds(mean_x, std_dev_x, *round_details['x'])\n",
    "        x_mask = (filtered_x[z_mask] <= upper_x) & (filtered_x[z_mask] >= lower_x)\n",
    "\n",
    "        mean_y, std_dev_y = cp.mean(filtered_y[z_mask][x_mask]), cp.std(filtered_y[z_mask][x_mask])\n",
    "        upper_y, lower_y = calc_bounds(mean_y, std_dev_y, *round_details['y'])\n",
    "        y_mask = (filtered_y[z_mask][x_mask] <= upper_y) & (filtered_y[z_mask][x_mask] >= lower_y)\n",
    "\n",
    "        filtered_x, filtered_y, filtered_z = filtered_x[z_mask][x_mask][y_mask], filtered_y[z_mask][x_mask][y_mask], filtered_z[z_mask][x_mask][y_mask]\n",
    "\n",
    "    return filtered_x, filtered_y, filtered_z\n",
    "\n",
    "def trim_noise_based_on_coordinate(coordinate, z_values, upper_std_dev, lower_std_dev, background_value=0):\n",
    "    unique_coords = cp.unique(coordinate)\n",
    "    filtered_indices = []\n",
    "    for unique_val in unique_coords:\n",
    "        local_mask = coordinate == unique_val\n",
    "        local_z_values = z_values[local_mask]\n",
    "        if local_z_values.size == 0:\n",
    "            continue\n",
    "\n",
    "        local_mean, local_std = cp.mean(local_z_values), cp.std(local_z_values)\n",
    "        upper, lower = calc_bounds(local_mean, local_std, upper_std_dev, lower_std_dev)\n",
    "        \n",
    "        valid_indices = cp.where((local_z_values <= upper) & (local_z_values >= max(lower, background_value)))[0]\n",
    "        \n",
    "        if valid_indices.size > 0:\n",
    "            filtered_indices.append(valid_indices)\n",
    "    \n",
    "    if not filtered_indices:\n",
    "        return cp.array([])\n",
    "    \n",
    "    return cp.concatenate(filtered_indices)\n",
    "\n",
    "def filter_gaussian_test(x, y, z, final_adjust=0.66, background_value=0):\n",
    "    rounds = [\n",
    "        {'z': (3, 1), 'x': (2, 2), 'y': (2, 2)},\n",
    "        {'z': (3.25, 1), 'x': (2, 2), 'y': (2, 2)},\n",
    "        {'z': (3.5, 1), 'x': (2, 2), 'y': (2, 2)},\n",
    "        {'z': (3.75, 1), 'x': (2, 2), 'y': (2, 2)},\n",
    "        {'z': (4, 1), 'x': (2, 2), 'y': (2, 2)},\n",
    "        {'z': (3.85, 0.96), 'x': (2, 2), 'y': (2, 2)},\n",
    "        {'z': (3.7, 0.92), 'x': (2, 2), 'y': (2, 2)},\n",
    "        {'z': (3.55, 0.88), 'x': (2, 2), 'y': (2, 2)},\n",
    "        {'z': (3.4, 0.84), 'x': (2, 2), 'y': (2, 2)},\n",
    "        {'z': (3.25, 0.8), 'x': (1.5, 1.5), 'y': (1.5, 1.5)},\n",
    "    ]\n",
    "    \n",
    "    x_filtered, y_filtered, z_filtered = filter_coordinates(x, y, z, rounds, background_value)\n",
    "\n",
    "    filtered_indices_x = trim_noise_based_on_coordinate(x_filtered, z_filtered, final_adjust, 5 * final_adjust)\n",
    "    filtered_indices_y = trim_noise_based_on_coordinate(y_filtered, z_filtered, final_adjust, 5 * final_adjust)\n",
    "    \n",
    "    if filtered_indices_x.size > 0 and filtered_indices_y.size > 0:\n",
    "        intersection_indices = cp.intersect1d(filtered_indices_x, filtered_indices_y)\n",
    "    else:\n",
    "        intersection_indices = cp.array([])\n",
    "\n",
    "    max_z = cp.max(z_filtered[intersection_indices]) if intersection_indices.size > 0 else None\n",
    "    return max_z\n",
    "\n",
    "def extract_surface_data(coco, image_path, annotation_id):\n",
    "    annotation = coco.loadAnns(annotation_id)[0]\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    mask = Image.new('L', (image.width, image.height), 0)\n",
    "    if 'segmentation' in annotation and annotation['segmentation']:\n",
    "        for segment in annotation['segmentation']:\n",
    "            ImageDraw.Draw(mask).polygon(segment, outline=1, fill=1)\n",
    "    mask = cp.array(mask)\n",
    "\n",
    "    region = cp.where(mask == 1)\n",
    "    image_np = cp.array(image)\n",
    "    pixel_values = image_np[region[0], region[1]] if image_np.ndim == 2 else image_np[region[0], region[1], 0]\n",
    "    \n",
    "    return region[1], region[0], pixel_values\n",
    "\n",
    "def calculate_centroid_seg(segmentation):\n",
    "    x = cp.array(segmentation[0::2])\n",
    "    y = cp.array(segmentation[1::2])\n",
    "    return float(cp.mean(x)), float(cp.mean(y))\n",
    "\n",
    "def calculate_principal_diagonal_from_segmentation(segmentation):\n",
    "    x, y = cp.array(segmentation[0::2]), cp.array(segmentation[1::2])\n",
    "    coordinates = cp.vstack((x, y)).T\n",
    "    pca = PCA(n_components=1)\n",
    "    # PCA in sklearn needs CPU arrays\n",
    "    pca.fit(coordinates.get()) \n",
    "    projections = cp.dot(coordinates, cp.array(pca.components_[0]))\n",
    "    return float(cp.max(projections) - cp.min(projections))\n",
    "\n",
    "def create_nan_masked_image(coco, image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image_np = cp.array(image, dtype=cp.float32)\n",
    "    overlap_mask = cp.zeros((image_np.shape[0], image_np.shape[1]), dtype=cp.uint8)\n",
    "    \n",
    "    for annotation in coco.anns.values():\n",
    "        mask = Image.new('L', (image.width, image.height), 0)\n",
    "        if 'segmentation' in annotation and annotation['segmentation']:\n",
    "            for segment in annotation['segmentation']:\n",
    "                ImageDraw.Draw(mask).polygon(segment, outline=1, fill=1)\n",
    "        overlap_mask += cp.array(mask)\n",
    "\n",
    "    image_nan_masked = cp.where(overlap_mask > 1, cp.nan, image_np)\n",
    "    return image_nan_masked\n",
    "\n",
    "def process_annotation(annotation, coco, image_path, image_nan_masked):\n",
    "    annotation_id = annotation['id']\n",
    "    category_id = annotation.get('category_id', 1)  # Make sure category_id is recorded\n",
    "\n",
    "    x, y, z = extract_surface_data(coco, image_path, annotation_id)\n",
    "\n",
    "    y_np = y.get() if isinstance(y, cp.ndarray) else y\n",
    "    x_np = x.get() if isinstance(x, cp.ndarray) else x\n",
    "\n",
    "    z_nan_masked = cp.array([image_nan_masked[y_np[i], x_np[i]] for i in range(len(x_np))])\n",
    "\n",
    "    segmentation = annotation['segmentation'][0]\n",
    "    centroid_x, centroid_y = calculate_centroid_seg(segmentation)\n",
    "    principal_diagonal = calculate_principal_diagonal_from_segmentation(segmentation)\n",
    "\n",
    "    fitted_max_value = filter_gaussian_test(x, y, z_nan_masked, final_adjust=0.66, background_value=0)\n",
    "    fitted_max_value_nm = fitted_max_value * conversion_factor if fitted_max_value is not None else None\n",
    "\n",
    "    return {\n",
    "        'Annotation ID': annotation_id,\n",
    "        'Category ID': category_id,\n",
    "        'Centroid X': centroid_x,\n",
    "        'Centroid Y': centroid_y,\n",
    "        'Principal Diagonal': principal_diagonal,\n",
    "        'Fitted Max Value': fitted_max_value,\n",
    "        'Fitted Max Value (nm)': fitted_max_value_nm\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization Functions\n",
    "def create_black_duplicate(image_path):\n",
    "    with Image.open(image_path) as img:\n",
    "        img_array = np.array(img)\n",
    "        shape = img_array.shape\n",
    "        black_array = np.zeros(shape, dtype=img_array.dtype)\n",
    "        black_image = Image.fromarray(black_array)\n",
    "        base, extension = os.path.splitext(image_path)\n",
    "        black_image_path = f\"{base}_black{extension}\"\n",
    "        black_image.save(black_image_path)\n",
    "        return black_image_path\n",
    "\n",
    "def get_color_from_colormap(value, min_value, max_value):\n",
    "    if isinstance(min_value, cp.ndarray):\n",
    "        min_value = min_value.get()\n",
    "    if isinstance(max_value, cp.ndarray):\n",
    "        max_value = max_value.get()\n",
    "    if isinstance(value, cp.ndarray):\n",
    "        value = value.get()\n",
    "\n",
    "    norm = mcolors.Normalize(vmin=min_value, vmax=max_value)\n",
    "    cmap = cm.rainbow\n",
    "    rgba_color = cmap(norm(value))\n",
    "    color = tuple(int(255 * x) for x in rgba_color[:3])\n",
    "    return color\n",
    "\n",
    "def visualize_nodes_on_image_with_colormap(image_path, df_clean, output_path, black=False):\n",
    "    if black:\n",
    "        image_path = create_black_duplicate(image_path)\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGBA\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    min_value = df_clean['Fitted Max Value'].min()\n",
    "    max_value = df_clean['Fitted Max Value'].max()\n",
    "\n",
    "    norm_values = (df_clean['Fitted Max Value'] - min_value) / (max_value - min_value)\n",
    "\n",
    "    # Define shapes for categories\n",
    "    category_shapes = {\n",
    "        1: 'ellipse',\n",
    "        2: 'rectangle'\n",
    "        # Add more mappings as needed, e.g., 3: 'triangle'\n",
    "    }\n",
    "\n",
    "    for idx, row in df_clean.iterrows():\n",
    "        x, y = row['Centroid X'], row['Centroid Y']\n",
    "        value = row['Fitted Max Value']\n",
    "        category_id = row.get('Category ID', 1)  # Default to 1 if not present\n",
    "        color = get_color_from_colormap(value, min_value, max_value)\n",
    "\n",
    "        norm_value = norm_values.iloc[idx]\n",
    "        size = 10 + 20 * norm_value\n",
    "        bbox = [x - size, y - size, x + size, y + size]\n",
    "\n",
    "        shape = category_shapes.get(category_id, 'ellipse')  # Default to ellipse\n",
    "        if shape == 'ellipse':\n",
    "            draw.ellipse(bbox, fill=color)\n",
    "        elif shape == 'rectangle':\n",
    "            draw.rectangle(bbox, fill=color)\n",
    "        else:\n",
    "            # If you add a 'triangle' shape:\n",
    "            # Triangles need three points, for example:\n",
    "            # triangle_points = [(x, y - size), (x - size, y + size), (x + size, y + size)]\n",
    "            # draw.polygon(triangle_points, fill=color)\n",
    "            draw.ellipse(bbox, fill=color)  # fallback to ellipse\n",
    "\n",
    "    image = image.convert(\"RGB\")\n",
    "    image.save(output_path)\n",
    "\n",
    "    plt.figure(figsize=(30, 30))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_and_connect_nodes(image_path, df_clean, output_path, black=False):\n",
    "    if black:\n",
    "        image_path = create_black_duplicate(image_path)\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGBA\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    min_value = df_clean['Fitted Max Value'].min()\n",
    "    max_value = df_clean['Fitted Max Value'].max()\n",
    "\n",
    "    coords = df_clean[['Centroid X', 'Centroid Y']].values\n",
    "    nbrs = NearestNeighbors(n_neighbors=6, algorithm='ball_tree').fit(coords)\n",
    "    distances, indices = nbrs.kneighbors(coords)\n",
    "\n",
    "    df_clean['Connections'] = [list(ind[1:]) for ind in indices]\n",
    "\n",
    "    category_shapes = {\n",
    "        1: 'ellipse',\n",
    "        2: 'rectangle'\n",
    "        # Add more shapes if needed\n",
    "    }\n",
    "\n",
    "    for idx, row in df_clean.iterrows():\n",
    "        x, y = row['Centroid X'], row['Centroid Y']\n",
    "        value = row['Fitted Max Value']\n",
    "        category_id = row.get('Category ID', 1)\n",
    "        color = get_color_from_colormap(value, min_value, max_value)\n",
    "\n",
    "        # Draw connections first\n",
    "        for neighbor_idx in row['Connections']:\n",
    "            neighbor_x = df_clean.loc[neighbor_idx, 'Centroid X']\n",
    "            neighbor_y = df_clean.loc[neighbor_idx, 'Centroid Y']\n",
    "            draw.line((x, y, neighbor_x, neighbor_y), fill=color, width=2)\n",
    "\n",
    "        size = 10 + 20 * ((value - min_value) / (max_value - min_value))\n",
    "        bbox = [x - size, y - size, x + size, y + size]\n",
    "\n",
    "        shape = category_shapes.get(category_id, 'ellipse')\n",
    "        if shape == 'ellipse':\n",
    "            draw.ellipse(bbox, fill=color)\n",
    "        elif shape == 'rectangle':\n",
    "            draw.rectangle(bbox, fill=color)\n",
    "        else:\n",
    "            # fallback if needed\n",
    "            draw.ellipse(bbox, fill=color)\n",
    "\n",
    "    image = image.convert(\"RGB\")\n",
    "    image.save(output_path)\n",
    "\n",
    "    plt.figure(figsize=(30, 30))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def create_colorbar(min_value, max_value, colormap):\n",
    "    if isinstance(min_value, cp.ndarray):\n",
    "        min_value = min_value.get()\n",
    "    if isinstance(max_value, cp.ndarray):\n",
    "        max_value = max_value.get()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 1))\n",
    "    fig.subplots_adjust(bottom=0.5)\n",
    "\n",
    "    norm = mcolors.Normalize(vmin=min_value, vmax=max_value)\n",
    "    cb = cm.ScalarMappable(norm=norm, cmap=colormap)\n",
    "    cb.set_array([])\n",
    "    cbar = plt.colorbar(cb, orientation='horizontal', cax=ax)\n",
    "    cbar.set_label('Slip Intensity Values (nm)')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_coco_masks_bbox_black(image_path, coco_json_path, output_path, figure_size=(50, 50), font_size=20, black=False, bbox=False):\n",
    "    if black:\n",
    "        image_path = create_black_duplicate(image_path)\n",
    "    \n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", size=font_size)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    # Define a category color mapping (example: 2 classes)\n",
    "    category_colors = {\n",
    "        1: (255, 0, 0),   # Red for Category 1\n",
    "        2: (0, 255, 0)    # Green for Category 2\n",
    "    }\n",
    "\n",
    "    for annotation in coco_data[\"annotations\"]:\n",
    "        segmentation = annotation[\"segmentation\"][0]\n",
    "        category_id = annotation.get(\"category_id\", 1)\n",
    "        \n",
    "        # Use category_colors if category_id exists, else default\n",
    "        color = category_colors.get(category_id, (255, 255, 255))  # Default to white if not found\n",
    "\n",
    "        draw.polygon(segmentation, outline=color, fill=None)\n",
    "        centroid_x = sum(segmentation[::2]) / (len(segmentation) / 2)\n",
    "        centroid_y = sum(segmentation[1::2]) / (len(segmentation) / 2)\n",
    "        draw.text((centroid_x + 5, centroid_y), str(annotation['id']), fill=color, font=font)\n",
    "\n",
    "        if bbox:\n",
    "            bbox_coords = [\n",
    "                annotation[\"bbox\"][0], annotation[\"bbox\"][1],\n",
    "                annotation[\"bbox\"][0] + annotation[\"bbox\"][2],\n",
    "                annotation[\"bbox\"][1] + annotation[\"bbox\"][3]\n",
    "            ]\n",
    "            draw.rectangle(bbox_coords, outline=color, width=2)\n",
    "\n",
    "    plt.figure(figsize=figure_size)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    image.save(output_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the maximum number of pixels that can be loaded.\n",
    "Image.MAX_IMAGE_PIXELS = None  # This disables the decompression bomb check entirely.\n",
    "# Alternatively, set a new limit that's specific to your image size, if known and reasonable\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "# Specify which categories to merge\n",
    "# For example, merge only category_id=1:\n",
    "categories_to_merge = [1]  \n",
    "# If you have multiple classes and you want to merge only a subset, list them here.\n",
    "\n",
    "for image_path in image_paths:\n",
    "    output_folder = Path(base_output_folder) / Path(image_path).stem\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    #--------------------------- Initial Predictions -------------------------------------------------\n",
    "    raw_image_path = image_path\n",
    "\n",
    "    #Find previous Annotations from prior Steps\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    previous_json_path = os.path.join(os.path.dirname(image_path), f\"{base_name}_predictions.json\")\n",
    "    print(previous_json_path)\n",
    "\n",
    "    #Preprocess the Image\n",
    "    raw_image_path = apply_previous_predictions(image_path, previous_json_path, output_folder)\n",
    "    preprocessed_image_path = preprocess(raw_image_path, output_folder)\n",
    "    preprocessed_original_image_path = preprocess(image_path, output_folder)\n",
    "\n",
    "    #Pad the Image\n",
    "    padded_image_path, top_padding, bottom_padding, left_padding, right_padding = pad_image(preprocessed_image_path, output_folder, min_padding=256)\n",
    "    final_image_path = padded_image_path\n",
    "    image_name = os.path.basename(final_image_path)\n",
    "\n",
    "    # Loop through the steps for counters 0 to 64 (currently set to 10 for demonstration)\n",
    "    for counter in range(10):\n",
    "        # Slice the Image\n",
    "        shift_distance = 128\n",
    "        slice_folder_path = slice_image_to_1024(padded_image_path, output_folder, counter, shift_distance)\n",
    "\n",
    "        # Predict on Image\n",
    "        model = YOLO(model_path)\n",
    "        model.to('cuda')\n",
    "        output_json_path = process_images(slice_folder_path, output_folder, model, counter, min_size=200, area_threshold=30)\n",
    "\n",
    "        # Convert to large image\n",
    "        output_file_path = convert_coco_to_large_image(output_json_path, output_folder, padded_image_path, counter)\n",
    "\n",
    "        # Mask the Image\n",
    "        masked_image_path = mask_image_with_coco_masks(padded_image_path, output_file_path, output_folder, counter, dilation_distance=5)\n",
    "\n",
    "        # Update the padded_image_path for the next iteration\n",
    "        padded_image_path = masked_image_path\n",
    "\n",
    "    output_json = 'Final_Compiled_Predictions.json'\n",
    "    final_compiled_predictions_path = compile_coco_json(output_folder, output_json, image_name)\n",
    "\n",
    "    #--------------------------- Visualize Initial Predictions -------------------------------------------------\n",
    "    initial_masked_image_path = os.path.join(output_folder, \"Initial_Masked_Image.jpg\")\n",
    "    visualize_coco_masks(final_image_path, final_compiled_predictions_path, initial_masked_image_path, figure_size=(100, 100), font_size=20)\n",
    "\n",
    "    #-------------------------- Remove Padding from the Images --------------------------------------------------\n",
    "    coco_json_path = final_compiled_predictions_path\n",
    "    file_name = 'Final_Compiled_Predictions_Adjusted.json'\n",
    "    adjusted_predictions_path = os.path.join(output_folder, file_name)\n",
    "    adjust_coco_annotations(coco_json_path, top_padding, bottom_padding, left_padding, right_padding, adjusted_predictions_path)\n",
    "\n",
    "    #-------------------------- Add in Previous Predictions -------------------------------------------------\n",
    "    full_adjusted_predictions_path = os.path.join(output_folder, 'Final_Compiled_Predictions_Cleaned.json')\n",
    "    add_annotations_to_coco_json(adjusted_predictions_path, previous_json_path, full_adjusted_predictions_path)\n",
    "\n",
    "    masked_image_path = os.path.join(output_folder, \"Masked_Image.jpg\")\n",
    "    visualize_coco_masks(preprocessed_original_image_path, full_adjusted_predictions_path, masked_image_path, figure_size=(100, 100), font_size=20)\n",
    "    \n",
    "    #--------------------------- Clean Initial Predictions -------------------------------------------------\n",
    "    clean_annotations(preprocessed_original_image_path, full_adjusted_predictions_path, full_adjusted_predictions_path, intensity_threshold=1)\n",
    "\n",
    "    clean_masked_image_path = os.path.join(output_folder, \"Clean_Masked_Image.jpg\")\n",
    "    visualize_coco_masks(preprocessed_original_image_path, full_adjusted_predictions_path, clean_masked_image_path, figure_size=(100, 100), font_size=20)\n",
    "\n",
    "    #--------------------------- Merge Predictions -------------------------------------------------\n",
    "    full_adjusted_cleaned_predictions_path = os.path.join(output_folder, 'Final_Compiled_Predictions_Cleaned_Adjusted.json')\n",
    "\n",
    "    # Load initial annotations\n",
    "    start_time_merging = time.perf_counter()\n",
    "\n",
    "    annotations = process_coco_json(full_adjusted_predictions_path)\n",
    "    print('Intial Number of Features:', len(annotations))\n",
    "\n",
    "    # Pass categories_to_merge to only merge specified categories\n",
    "    final_annotations = progressive_merge_annotations(annotations, categories_to_merge=categories_to_merge)\n",
    "    print('Final Number of Features:', len(final_annotations))\n",
    "\n",
    "    # Because update_json_file no longer takes a single category_id, we no longer pass category_id directly here.\n",
    "    # The final_annotations now contain multiple categories. They will be preserved as is.\n",
    "    update_json_file(final_annotations, full_adjusted_cleaned_predictions_path, preprocessed_original_image_path)\n",
    "\n",
    "    print(\"Combination complete. Updated annotations saved.\")\n",
    "\n",
    "    end_time_merging = time.perf_counter()\n",
    "    elapsed_seconds = end_time_merging - start_time_merging\n",
    "    hours, minutes, seconds = convert_seconds_to_hms(elapsed_seconds)\n",
    "    print(f\"Execution time for Merging: {hours} hour(s), {minutes} minute(s), {seconds:.2f} second(s)\")\n",
    "\n",
    "    #-------------------------- Visualize the Merged Predictions -------------------------------------------------\n",
    "    final_masked_image_cleaned_path = os.path.join(output_folder, \"Initial_Masked_Image_Cleaned.jpg\")\n",
    "    visualize_coco_masks(preprocessed_original_image_path, full_adjusted_cleaned_predictions_path, final_masked_image_cleaned_path ,figure_size=(100, 100), font_size=20)\n",
    "\n",
    "    final_masked_image_cleaned_bbox_path = os.path.join(output_folder, \"Initial_Masked_Image_Cleaned_bbox.jpg\")\n",
    "    visualize_coco_masks_bbox(preprocessed_original_image_path, full_adjusted_cleaned_predictions_path, final_masked_image_cleaned_bbox_path ,figure_size=(100, 100), font_size=20)\n",
    "\n",
    "    #-------------------------- Statistics Postprocessing --------------------------------------------------\n",
    "    start_time_stats = time.perf_counter()\n",
    "    data = load_json(full_adjusted_cleaned_predictions_path)\n",
    "    coco = load_coco_annotations(full_adjusted_cleaned_predictions_path)\n",
    "    data_list = []\n",
    "\n",
    "    NaN_Image_name = 'nan_masked_image.tif'\n",
    "    mask_save_path = os.path.join(output_folder, NaN_Image_name)\n",
    "\n",
    "    # Create or load the NaN-masked image\n",
    "    image_nan_masked = tiff.imread(mask_save_path) if os.path.exists(mask_save_path) else create_nan_masked_image(coco, image_path)\n",
    "    if not os.path.exists(mask_save_path):\n",
    "        tiff.imwrite(mask_save_path, image_nan_masked.get(), dtype=np.float32)\n",
    "\n",
    "    # Collect Annotations \n",
    "    annotation_ids_to_analyze = []\n",
    "    annotations = data['annotations'] if not annotation_ids_to_analyze else [\n",
    "        annotation for annotation in data['annotations'] if annotation['id'] in annotation_ids_to_analyze\n",
    "    ]\n",
    "\n",
    "    # Parallel processing of annotations with concurrent.futures\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_annotation, annotation, coco, image_path, image_nan_masked) for annotation in annotations]\n",
    "        data_list = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "\n",
    "    # Convert CuPy arrays in data_list to NumPy if necessary\n",
    "    data_list = [\n",
    "        {k: (v.get() if isinstance(v, cp.ndarray) else v) for k, v in item.items()}\n",
    "        for item in data_list\n",
    "    ]\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    filename = 'BlN_Stats.csv'\n",
    "    BLN_Stats_Path = os.path.join(output_folder, filename)\n",
    "    df = pd.DataFrame(data_list)\n",
    "    df = df.sort_values(by='Annotation ID')\n",
    "    df.to_csv(BLN_Stats_Path, index=False)\n",
    "\n",
    "    end_time_stats = time.perf_counter()\n",
    "    elapsed_seconds = end_time_stats - start_time_stats\n",
    "    hours, minutes, seconds = convert_seconds_to_hms(elapsed_seconds)\n",
    "    print(f\"Execution time for Statistics: {hours} hour(s), {minutes} minute(s), {seconds:.2f} second(s)\")\n",
    "\n",
    "    #------------------------- Node Visualization ---------------------------------------------------\n",
    "    df_clean = df.dropna(subset=['Centroid X', 'Centroid Y', 'Fitted Max Value'])\n",
    "    df_clean = df_clean.reset_index(drop=True)\n",
    "\n",
    "    # Visualize nodes (with shapes based on category)\n",
    "    filename = 'Node_Overlay.jpg'\n",
    "    output_path = f'{output_folder}/{filename}'\n",
    "    visualize_nodes_on_image_with_colormap(preprocessed_original_image_path, df_clean, output_path, black=True)\n",
    "\n",
    "    #------------------------- Node Visualization with Connections --------------------------------------------------\n",
    "    df_clean = df.dropna(subset=['Centroid X', 'Centroid Y', 'Fitted Max Value'])\n",
    "    df_clean = df_clean.reset_index(drop=True)\n",
    "    \n",
    "    filename = 'Node_Overlay_with_Connections.jpg'\n",
    "    output_path = f'{output_folder}/{filename}'\n",
    "    visualize_and_connect_nodes(preprocessed_original_image_path, df_clean, output_path, black=False)\n",
    "    filename = 'Node_Overlay_with_Connections_Black.jpg'\n",
    "    output_path = f'{output_folder}/{filename}'\n",
    "    visualize_and_connect_nodes(preprocessed_original_image_path, df_clean, output_path, black=True)\n",
    "\n",
    "    #------------------------- Display the colorbar ---------------------------------------------------\n",
    "    min_value = df_clean['Fitted Max Value'].min() * 22.46\n",
    "    max_value = df_clean['Fitted Max Value'].max() * 22.46\n",
    "    create_colorbar(min_value, max_value, cm.rainbow)\n",
    "\n",
    "    #------------------------- Final Visualizations --------------------------------------------------\n",
    "    final_masked_image_cleaned_adjusted_path = os.path.join(output_folder, \"Final_Masked_Image_Cleaned_Adjusted.jpg\")\n",
    "    visualize_coco_masks_bbox_black(preprocessed_original_image_path, full_adjusted_cleaned_predictions_path, final_masked_image_cleaned_adjusted_path ,figure_size=(100, 100), font_size=20, black=False, bbox=False)\n",
    "\n",
    "    final_masked_image_cleaned_adjusted_bbox_path = os.path.join(output_folder, \"Final_Masked_Image_Cleaned_Adjusted_bbox.jpg\")\n",
    "    visualize_coco_masks_bbox_black(preprocessed_original_image_path, full_adjusted_cleaned_predictions_path, final_masked_image_cleaned_adjusted_bbox_path ,figure_size=(100, 100), font_size=20, black=False, bbox=True)\n",
    "\n",
    "    final_masked_image_cleaned_adjusted_black_path = os.path.join(output_folder, \"Final_Masked_Image_Cleaned_Adjusted_black.jpg\")\n",
    "    visualize_coco_masks_bbox_black(preprocessed_original_image_path, full_adjusted_cleaned_predictions_path, final_masked_image_cleaned_adjusted_black_path ,figure_size=(100, 100), font_size=20, black=True, bbox=False)\n",
    "\n",
    "    final_masked_image_cleaned_adjusted_bbox_black_path = os.path.join(output_folder, \"Final_Masked_Image_Cleaned_Adjusted_bbox_black.jpg\")\n",
    "    visualize_coco_masks_bbox_black(preprocessed_original_image_path, full_adjusted_cleaned_predictions_path, final_masked_image_cleaned_adjusted_bbox_black_path ,figure_size=(100, 100), font_size=20, black=True, bbox=True)\n",
    "\n",
    "    #------------------------- Create Histogram --------------------------------------------------\n",
    "    df_clean['Converted Max Value'] = df_clean['Fitted Max Value'] * conversion_factor\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.violinplot(y='Converted Max Value', data=df_clean, inner=None, color='red')\n",
    "    sns.stripplot(y='Converted Max Value', data=df_clean, size=4, jitter=True, color='k', edgecolor='k', alpha=0.7)\n",
    "\n",
    "    ax = plt.gca()\n",
    "    violins = [child for child in ax.get_children() if isinstance(child, PolyCollection)]\n",
    "    for violin in violins:\n",
    "        paths = violin.get_paths()\n",
    "        for path in paths:\n",
    "            vertices = path.vertices\n",
    "            median = np.median(vertices[:, 0])\n",
    "            vertices[vertices[:, 0] > median, 0] = median\n",
    "\n",
    "    plt.title('Distribution of Maximum Slip Intensity Across the Dataset')\n",
    "    plt.ylabel('Max Slip Intensity (nm)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    #------------------------- Organize Files ---------------------------------------------------\n",
    "    base_dir = output_folder\n",
    "\n",
    "    clean_results_files = [\n",
    "        \"Final_Compiled_Predictions_Cleaned_Adjusted.json\",\n",
    "        \"Final_Masked_Image_Cleaned_Adjusted.jpg\",\n",
    "        \"Final_Masked_Image_Cleaned_Adjusted_bbox.jpg\",\n",
    "        \"Final_Masked_Image_Cleaned_Adjusted_black.jpg\",\n",
    "        \"Final_Masked_Image_Cleaned_Adjusted_bbox_black.jpg\",\n",
    "        \"BlN_Stats.csv\",\n",
    "        \"Node_Overlay_with_Connections.jpg\",\n",
    "        \"Node_Overlay_with_Connections_Black.jpg\"\n",
    "    ]\n",
    "\n",
    "    clean_results_dir = os.path.join(base_dir, 'Clean_Results')\n",
    "    os.makedirs(clean_results_dir, exist_ok=True)\n",
    "\n",
    "    processing_files_dir = os.path.join(base_dir, 'Processing_Files')\n",
    "    os.makedirs(processing_files_dir, exist_ok=True)\n",
    "\n",
    "    for file_name in clean_results_files:\n",
    "        source_path = os.path.join(base_dir, file_name)\n",
    "        if os.path.exists(source_path):\n",
    "            shutil.move(source_path, os.path.join(clean_results_dir, file_name))\n",
    "\n",
    "    for item in os.listdir(base_dir):\n",
    "        item_path = os.path.join(base_dir, item)\n",
    "        if item_path not in [clean_results_dir, processing_files_dir]:\n",
    "            shutil.move(item_path, os.path.join(processing_files_dir, item))\n",
    "\n",
    "    json_file_path = os.path.join(clean_results_dir, \"Final_Compiled_Predictions_Cleaned_Adjusted.json\")\n",
    "    if os.path.exists(json_file_path):\n",
    "        base_name = Path(image_path).stem\n",
    "        match = re.search(r'(\\d+)(?!.*\\d)', base_name)\n",
    "        if match:\n",
    "            number = int(match.group(0)) + 1\n",
    "            new_base_name = re.sub(r'(\\d+)(?!.*\\d)', str(number), base_name)\n",
    "            new_file_name = f\"{new_base_name}_predictions.json\"\n",
    "            \n",
    "            destination_path = os.path.join(base_output_folder, new_file_name)\n",
    "            shutil.copy(json_file_path, destination_path)\n",
    "\n",
    "    print(\"Files have been organized.\")\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    elapsed_seconds = end_time - start_time\n",
    "    hours, minutes, seconds = convert_seconds_to_hms(elapsed_seconds)\n",
    "    print(f\"Execution time: {hours} hour(s), {minutes} minute(s), {seconds:.2f} second(s)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
