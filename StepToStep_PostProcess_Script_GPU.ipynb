{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab8cde74",
   "metadata": {},
   "source": [
    "# Modified Post-Processing Script for Multiple Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4392f367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cmbean2\\AppData\\Local\\Temp\\ipykernel_14220\\4097302254.py:43: DeprecationWarning: Please use `QhullError` from the `scipy.spatial` namespace, the `scipy.spatial.qhull` namespace is deprecated.\n",
      "  from scipy.spatial.qhull import QhullError\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import copy\n",
    "import glob\n",
    "import json\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import shutil\n",
    "import time\n",
    "import tifffile as tiff\n",
    "import cv2\n",
    "import re\n",
    "\n",
    "\n",
    "from math import sqrt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.collections import PolyCollection\n",
    "from matplotlib.patches import Ellipse\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from pycocotools.coco import COCO\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.spatial import ConvexHull, cKDTree\n",
    "from scipy.stats import norm\n",
    "from shapely.geometry import LineString, MultiPoint, Polygon, MultiPolygon\n",
    "from shapely.ops import unary_union\n",
    "from skimage import io, measure, morphology\n",
    "from skimage.draw import polygon, polygon2mask\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from ultralytics import YOLO\n",
    "import concurrent.futures\n",
    "import cupy as cp\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from scipy.spatial.qhull import QhullError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1fabc4",
   "metadata": {},
   "source": [
    "## Model Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "decc07a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original models dictionary\n",
    "models = {\n",
    "    \"FCC_Model\": \"D:\\\\\\Machine_Learning\\\\CV Script\\\\Models\\\\FCC_1024\\\\weights\\\\best.pt\",\n",
    "    \"BCC_Model\": \"D:\\\\Machine_Learning\\\\CV Script\\\\Models\\\\BCC_1024\\\\weights\\\\best.pt\",\n",
    "    \"HCP_Model\": \"D:\\\\Machine_Learning\\\\CV Script\\\\Models\\\\HCP_1024\\\\weights\\\\best.pt\",\n",
    "    \"Combined_FCC_BCC_HCP_Model\": \"D:\\\\Machine_Learning\\\\CV Script\\\\Models\\\\Combined_FCC_BCC_HCP_1024\\\\weights\\\\best.pt\",\n",
    "    \"Combined_FCC_BCC_Model\": \"D:\\\\Machine_Learning\\\\CV Script\\\\Models\\\\Combined_FCC_BCC_1024\\\\weights\\\\best.pt\",\n",
    "    \"Combined_FCC_HCP_Model\": \"D:\\\\Machine_Learning\\\\CV Script\\\\Models\\\\Combined_FCC_HCP_1024\\\\weights\\\\best.pt\",\n",
    "    \"Combined_HCP_BCC_Model\": \"D:\\\\Machine_Learning\\\\CV Script\\\\Models\\\\Combined_BCC_HCP_1024\\\\weights\\\\best.pt\"\n",
    "}\n",
    "\n",
    "# Shortcut mapping\n",
    "shortcut_mapping = {\n",
    "    \"FCC\": \"FCC_Model\",\n",
    "    \"BCC\": \"BCC_Model\",\n",
    "    \"HCP\": \"HCP_Model\",\n",
    "    \"FBH\": \"Combined_FCC_BCC_HCP_Model\",\n",
    "    \"FB\": \"Combined_FCC_BCC_Model\",\n",
    "    \"FH\": \"Combined_FCC_HCP_Model\",\n",
    "    \"BH\": \"Combined_HCP_BCC_Model\"\n",
    "}\n",
    "\n",
    "# Function to retrieve model path by shortcut\n",
    "def get_model_path(shortcut):\n",
    "    model_key = shortcut_mapping.get(shortcut)\n",
    "    if model_key:\n",
    "        return models[model_key]\n",
    "    else:\n",
    "        raise ValueError(f\"Shortcut '{shortcut}' not recognized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8863aa",
   "metadata": {},
   "source": [
    "# Set up Image Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cdfed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define initial paths and variables\n",
    "image_paths =  [\n",
    "               \n",
    "\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_I3_BlN_step1.tif\",\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_I3_BlN_step2.tif\",\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_I3_BlN_step3.tif\",\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_I3_BlN_step4.tif\",\n",
    "\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_I4_BlN_step1.tif\",\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_I4_BlN_step2.tif\",\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_I4_BlN_step3.tif\",\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_I4_BlN_step4.tif\",\n",
    "\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_R1_BlN_step1.tif\",\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_R1_BlN_step2.tif\",\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_R1_BlN_step3.tif\",\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_R1_BlN_step4.tif\",\n",
    "\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_R2_BlN_step1.tif\",\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_R2_BlN_step2.tif\",\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_R2_BlN_step3.tif\",\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_R2_BlN_step4.tif\",\n",
    "\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_R3_BlN_step1.tif\",\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_R3_BlN_step2.tif\",\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_R3_BlN_step3.tif\",\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_R3_BlN_step4.tif\",\n",
    "\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_R4_BlN_step1.tif\",\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_R4_BlN_step2.tif\",\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_R4_BlN_step3.tif\",\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_R4_BlN_step4.tif\",\n",
    "\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_R5_BlN_step1.tif\",\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_R5_BlN_step2.tif\",\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_R5_BlN_step3.tif\",\n",
    "               \"D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\A024S2A_R5_BlN_step4.tif\",\n",
    "\n",
    "               ]\n",
    "\n",
    "base_output_folder = 'D:\\\\Machine_Learning\\\\DARPA_Graphs_Reruns\\\\A024_316L_to_In625_T1\\\\'  # Base output folder for all results\n",
    "\n",
    "model_path = get_model_path(\"FCC\") # FCC, BCC, HCP or Combinations abbreviated as FBH, FB, FH, or BH.\n",
    "conversion_factor = 22.46 # To convert intensity to nanometers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06b4581",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "272f86a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to do Initial Predictions\n",
    "def apply_previous_predictions(image_path, previous_json_path, output_folder, dilation_distance=5):\n",
    "\n",
    "    # Check if the previous predictions JSON file exists\n",
    "    if os.path.exists(previous_json_path):\n",
    "        print(f\"Loading previous predictions from {previous_json_path}\")\n",
    "\n",
    "        # Load the COCO JSON file\n",
    "        with open(previous_json_path, 'r') as f:\n",
    "            coco_data = json.load(f)\n",
    "\n",
    "        # Load the image\n",
    "        image = tiff.imread(image_path)\n",
    "        image_shape = image.shape\n",
    "\n",
    "        # Create a blank mask with the same shape as the image\n",
    "        if image.ndim == 2:  # Grayscale\n",
    "            mask = np.zeros_like(image, dtype=np.uint8)\n",
    "            fill_value = 0  # Zero value for masking\n",
    "        else:  # RGB\n",
    "            mask = np.zeros((image_shape[0], image_shape[1], 3), dtype=np.uint8)\n",
    "            fill_value = (0, 0, 0)  # Zero value for masking\n",
    "\n",
    "        print(f\"Image shape: {image_shape}\")\n",
    "        print(f\"Number of annotations: {len(coco_data['annotations'])}\")\n",
    "\n",
    "        # Draw the masks on the blank image\n",
    "        for annotation in coco_data[\"annotations\"]:\n",
    "            original_segmentation = annotation[\"segmentation\"][0]  # Assuming one polygon per annotation\n",
    "            dilated_segmentation = dilate_polygon(original_segmentation, dilation_distance, image_shape)\n",
    "            if dilated_segmentation:\n",
    "                rr, cc = polygon(np.array(dilated_segmentation)[:, 1], np.array(dilated_segmentation)[:, 0], image_shape)\n",
    "                mask[rr, cc] = 1  # Mark the mask region\n",
    "\n",
    "        print(\"Mask created. Applying mask to the original image...\")\n",
    "\n",
    "        # Apply the mask to the original image\n",
    "        if image.ndim == 2:\n",
    "            masked_image = np.where(mask == 1, fill_value, image)\n",
    "        else:\n",
    "            masked_image = np.where(mask == 1, fill_value, image)\n",
    "\n",
    "        print(\"Mask applied. Saving the masked image...\")\n",
    "\n",
    "        # Construct output file name\n",
    "        base_name = os.path.basename(image_path)\n",
    "        new_file_name = f\"masked_{base_name}\"\n",
    "        output_path = os.path.join(output_folder, new_file_name)\n",
    "\n",
    "        # Save the masked image in the same format\n",
    "        tiff.imwrite(output_path, masked_image)\n",
    "\n",
    "        print(f\"Masked image saved to {output_path}\")\n",
    "\n",
    "        return output_path\n",
    "    print(f\"No previous predictions found for {image_path}\")\n",
    "    return image_path\n",
    "\n",
    "def preprocess(image_path, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    # Extract filename from the image path\n",
    "    filename = os.path.basename(image_path)\n",
    "    \n",
    "    # Add '_prepro' before the file extension in the filename\n",
    "    name, ext = os.path.splitext(filename)\n",
    "    new_filename = f\"{name}_prepro{ext}\"\n",
    "\n",
    "    # Construct the save path\n",
    "    save_path = os.path.join(output_folder, new_filename)\n",
    "\n",
    "    # Read the image\n",
    "    img = io.imread(image_path)\n",
    "    img[np.isnan(img)] = 0\n",
    "    img = (img * 255).clip(0, 255)\n",
    "    io.imsave(save_path, img)\n",
    "\n",
    "    return save_path\n",
    "\n",
    "def pad_image(image_path, output_folder, min_padding=256):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    base_name = os.path.basename(image_path)\n",
    "    new_file_name = os.path.splitext(base_name)[0] + \"_padded\" + os.path.splitext(base_name)[1]\n",
    "    output_path = os.path.join(output_folder, new_file_name)\n",
    "\n",
    "    with Image.open(image_path) as img:\n",
    "        width, height = img.size\n",
    "        padded_width = width + 2 * min_padding\n",
    "        padded_height = height + 2 * min_padding\n",
    "\n",
    "        new_width = math.ceil(padded_width / 1024) * 1024\n",
    "        new_height = math.ceil(padded_height / 1024) * 1024\n",
    "\n",
    "        new_img = Image.new(\"RGB\", (new_width, new_height), (0, 0, 0))\n",
    "        new_img.paste(img, ((new_width - width) // 2, (new_height - height) // 2))\n",
    "\n",
    "        new_img.save(output_path)\n",
    "    \n",
    "    top_padding = (new_height - height) // 2\n",
    "    left_padding = (new_width - width) // 2\n",
    "    return output_path, top_padding, top_padding, left_padding, left_padding\n",
    "\n",
    "def slice_image_to_1024(image_path, save_path, counter, shift_distance):\n",
    "    with Image.open(image_path) as img:\n",
    "        width, height = img.size\n",
    "\n",
    "        # Calculate the number of shifts horizontally and vertically\n",
    "        num_shifts_x = width // 1024\n",
    "        num_shifts_y = height // 1024\n",
    "\n",
    "        # Calculate shift based on counter, wrapping around using modulo arithmetic\n",
    "        shift_x = (counter % num_shifts_x) * shift_distance\n",
    "        shift_y = (counter // num_shifts_x) * shift_distance\n",
    "\n",
    "        # Ensure the shift does not exceed the image boundaries\n",
    "        shift_x = min(shift_x, width - 1024)\n",
    "        shift_y = min(shift_y, height - 1024)\n",
    "\n",
    "        # Update save path to include counter\n",
    "        save_path_with_counter = os.path.join(save_path, f\"slices_{counter}\")\n",
    "        if not os.path.exists(save_path_with_counter):\n",
    "            os.makedirs(save_path_with_counter)\n",
    "\n",
    "        # Calculate the number of slices in both dimensions\n",
    "        cols = (width - shift_x) // 1024\n",
    "        rows = (height - shift_y) // 1024\n",
    "\n",
    "        # Slice the image and save each piece\n",
    "        for row in range(rows):\n",
    "            for col in range(cols):\n",
    "                left = col * 1024 + shift_x\n",
    "                upper = row * 1024 + shift_y\n",
    "                right = left + 1024\n",
    "                lower = upper + 1024\n",
    "\n",
    "                slice_img = img.crop((left, upper, right, lower))\n",
    "\n",
    "                if slice_img.size == (1024, 1024):\n",
    "                    slice_img.save(os.path.join(save_path_with_counter, f\"{left}_{upper}.jpg\"))\n",
    "    \n",
    "    return save_path_with_counter\n",
    "\n",
    "def mask_to_polygon(mask, image_size, border_threshold=128, min_size=100):\n",
    "    \n",
    "    # Label the mask to identify distinct regions\n",
    "    labeled_mask, num_labels = measure.label(mask, background=0, return_num=True)\n",
    "\n",
    "    # If no regions are found, return None\n",
    "    if num_labels == 0:\n",
    "        return None\n",
    "\n",
    "    # Keep only the largest continuous region\n",
    "    largest_region = morphology.remove_small_objects(labeled_mask, min_size=min_size)\n",
    "    largest_region_mask = largest_region > 0\n",
    "\n",
    "    # Check the size of the largest region\n",
    "    if np.sum(largest_region_mask) < min_size:\n",
    "        return None\n",
    "\n",
    "    # Find contours from the binary mask of the largest region\n",
    "    contours = measure.find_contours(largest_region_mask, 0.5)\n",
    "\n",
    "    # Convert contours to polygon format and compute convex hull if needed\n",
    "    polygons = []\n",
    "    for contour in contours:\n",
    "        contour = np.flip(contour, axis=1)  # Flip from (row, col) to (x, y) format\n",
    "        contour = contour.round().astype(int)\n",
    "\n",
    "        # Check if contour is valid for convex hull computation\n",
    "        if len(np.unique(contour, axis=0)) < 3:\n",
    "            # Contour is invalid for convex hull, skip to next contour\n",
    "            continue\n",
    "\n",
    "        # Calculate the area of the original polygon\n",
    "        try:\n",
    "            original_area = ConvexHull(contour).volume\n",
    "        except QhullError:\n",
    "            # If ConvexHull computation fails, skip to next contour\n",
    "            continue\n",
    "\n",
    "        # Calculate the convex hull of the polygon\n",
    "        hull = ConvexHull(contour).vertices\n",
    "        convex_hull_polygon = contour[hull]\n",
    "\n",
    "        # Calculate the area of the convex hull\n",
    "        try:\n",
    "            convex_hull_area = ConvexHull(convex_hull_polygon).volume\n",
    "        except QhullError:\n",
    "            # If ConvexHull computation fails, use the original contour\n",
    "            polygons.append(contour.flatten().tolist())\n",
    "            continue\n",
    "\n",
    "        # Compare areas and decide whether to keep the convex hull\n",
    "        if (convex_hull_area - original_area) / original_area < 0.1:\n",
    "            polygons.append(convex_hull_polygon.flatten().tolist())\n",
    "        else:\n",
    "            polygons.append(contour.flatten().tolist())\n",
    "\n",
    "    return polygons\n",
    "\n",
    "def calculate_area_and_bbox(polygon):\n",
    "    \n",
    "    # Convert the flattened list to a 2D array of points\n",
    "    poly_array = np.array(polygon).reshape(-1, 2)\n",
    "\n",
    "    # Calculate bounding box\n",
    "    x_min = np.min(poly_array[:, 0])\n",
    "    x_max = np.max(poly_array[:, 0])\n",
    "    y_min = np.min(poly_array[:, 1])\n",
    "    y_max = np.max(poly_array[:, 1])\n",
    "    bbox = [int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)]\n",
    "\n",
    "    # Calculate area using Shoelace formula\n",
    "    x = poly_array[:, 0]\n",
    "    y = poly_array[:, 1]\n",
    "    area = 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n",
    "    area = int(area)\n",
    "\n",
    "    return area, bbox\n",
    "\n",
    "def process_images(input_folder, output_folder, model, counter, min_size, area_threshold):\n",
    "    # Initialize COCO format dictionary\n",
    "    coco_output = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": []  # Fill in category details as per your model\n",
    "    }\n",
    "\n",
    "    annotation_id = 1\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):  # Consider other image formats if needed\n",
    "            image_path = os.path.join(input_folder, filename)\n",
    "            with Image.open(image_path) as img:\n",
    "                width, height = img.size\n",
    "\n",
    "                # Add image info to COCO\n",
    "                image_id = len(coco_output[\"images\"]) + 1\n",
    "                coco_output[\"images\"].append({\n",
    "                    \"id\": image_id,\n",
    "                    \"width\": width,\n",
    "                    \"height\": height,\n",
    "                    \"file_name\": filename\n",
    "                })\n",
    "\n",
    "                # Perform prediction\n",
    "                results = model(img, imgsz=1024, conf=0.7, device = [0])  # Adjust as necessary\n",
    "                for r in results:\n",
    "                    # Check for masks presence\n",
    "                    if r.masks is not None and hasattr(r.masks, 'data'):\n",
    "                        masks_numpy = [mask.cpu().numpy() for mask in r.masks.data]\n",
    "\n",
    "                        # Process each mask\n",
    "                        for mask in masks_numpy:\n",
    "                            polygon = mask_to_polygon(mask, image_size=(width, height), border_threshold=0, min_size=min_size)\n",
    "                            if polygon is not None:\n",
    "                                for poly in polygon:\n",
    "                                    area, bbox = calculate_area_and_bbox(poly)\n",
    "                                    # Check if the area meets the minimum threshold before adding the annotation\n",
    "                                    if area >= area_threshold:\n",
    "                                        coco_output[\"annotations\"].append({\n",
    "                                            \"id\": annotation_id,\n",
    "                                            \"image_id\": image_id,\n",
    "                                            \"category_id\": 1,  # Update based on your model's categories\n",
    "                                            \"segmentation\": poly,\n",
    "                                            \"area\": area,\n",
    "                                            \"bbox\": bbox,\n",
    "                                            \"iscrowd\": 0\n",
    "                                        })\n",
    "                                        annotation_id += 1\n",
    "\n",
    "    # Construct output file name with counter\n",
    "    output_json = os.path.join(output_folder, f\"slice_predictions_{counter}.json\")\n",
    "\n",
    "    # Write COCO data to JSON file\n",
    "    with open(output_json, 'w') as f:\n",
    "        json.dump(coco_output, f)\n",
    "\n",
    "    return output_json\n",
    "\n",
    "def convert_coco_to_large_image(coco_file, output_folder, padded_image_path, counter=0):\n",
    "    # Load the large image to get its size\n",
    "    with Image.open(padded_image_path) as large_img:\n",
    "        large_image_size = large_img.size\n",
    "\n",
    "    # Extract the file name of the large image\n",
    "    large_image_file_name = os.path.basename(padded_image_path)\n",
    "\n",
    "    with open(coco_file, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    large_image_annotations = {\n",
    "        \"images\": [{\"id\": 1, \"width\": large_image_size[0], \"height\": large_image_size[1], \"file_name\": large_image_file_name}],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": coco_data[\"categories\"]\n",
    "    }\n",
    "\n",
    "    annotation_id = 1\n",
    "    for annotation in coco_data[\"annotations\"]:\n",
    "        image_id = annotation[\"image_id\"]\n",
    "        image_info = next((img for img in coco_data[\"images\"] if img[\"id\"] == image_id), None)\n",
    "        if image_info:\n",
    "            # Extract x, y offsets from the image name\n",
    "            x_offset, y_offset = map(int, image_info[\"file_name\"].split('.')[0].split('_'))\n",
    "\n",
    "            # Adjust segmentation coordinates\n",
    "            seg = annotation[\"segmentation\"]\n",
    "            adjusted_seg = [coord + (x_offset if i % 2 == 0 else y_offset) for i, coord in enumerate(seg)]\n",
    "            area, bbox = calculate_area_and_bbox(adjusted_seg)\n",
    "\n",
    "            # Create new annotation for the large image\n",
    "            large_image_annotations[\"annotations\"].append({\n",
    "                \"id\": annotation_id,\n",
    "                \"image_id\": 1,\n",
    "                \"category_id\": annotation[\"category_id\"],\n",
    "                \"segmentation\": [adjusted_seg],\n",
    "                \"area\": area, \n",
    "                \"bbox\": bbox, \n",
    "                \"iscrowd\": annotation[\"iscrowd\"]\n",
    "            })\n",
    "            annotation_id += 1\n",
    "\n",
    "    # Construct output file name with counter\n",
    "    output_file_path = os.path.join(output_folder, f\"full_predictions_{counter}.json\")\n",
    "\n",
    "    # Save the new annotations as a COCO JSON file\n",
    "    with open(output_file_path, 'w') as f:\n",
    "        json.dump(large_image_annotations, f)\n",
    "    \n",
    "    return output_file_path\n",
    "\n",
    "def dilate_polygon(polygon, dilation_distance, image_shape):\n",
    "    # Ensure the polygon is in the correct format: list of points where each point is a tuple (x, y)\n",
    "    polygon = [(polygon[i], polygon[i + 1]) for i in range(0, len(polygon), 2)]\n",
    "\n",
    "    # Create an empty mask\n",
    "    mask = np.zeros(image_shape, dtype=np.uint8)\n",
    "\n",
    "    # Draw the polygon on the mask\n",
    "    cv2.fillPoly(mask, [np.array(polygon, dtype=np.int32)], 1)\n",
    "\n",
    "    # Create a kernel for dilation\n",
    "    kernel = np.ones((dilation_distance*2+1, dilation_distance*2+1), np.uint8)\n",
    "\n",
    "    # Dilate the mask\n",
    "    dilated_mask = cv2.dilate(mask, kernel)\n",
    "\n",
    "    # Find contours in the dilated mask\n",
    "    contours, _ = cv2.findContours(dilated_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Assuming the largest contour is the dilated polygon\n",
    "    if contours:\n",
    "        dilated_contour = contours[0]\n",
    "        # Flatten the dilated contour to a list of points\n",
    "        dilated_polygon = dilated_contour.reshape((-1, 2)).tolist()\n",
    "    else:\n",
    "        dilated_polygon = []\n",
    "\n",
    "    return dilated_polygon\n",
    "\n",
    "def mask_image_with_coco_masks(image_path, coco_json_path, output_folder, counter, dilation_distance=5):\n",
    "    # Load the COCO JSON file\n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    image_shape = (image.height, image.width)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Iterate over annotations and apply each mask\n",
    "    for annotation in coco_data[\"annotations\"]:\n",
    "        original_segmentation = annotation[\"segmentation\"][0]  # Assuming one polygon per annotation\n",
    "\n",
    "        # Dilate the polygon\n",
    "        dilated_segmentation = dilate_polygon(original_segmentation, dilation_distance, image_shape)\n",
    "\n",
    "        # Flatten the list of tuples into a flat list of coordinates\n",
    "        flat_dilated_segmentation = [coord for point in dilated_segmentation for coord in point]\n",
    "\n",
    "        # Draw the dilated mask with black color\n",
    "        draw.polygon(flat_dilated_segmentation, fill=(0, 0, 0))\n",
    "\n",
    "    # Construct output file name with counter\n",
    "    base_name = os.path.basename(image_path)\n",
    "    new_file_name = f\"masked_{counter}\" + os.path.splitext(base_name)[1]\n",
    "    output_path = os.path.join(output_folder, new_file_name)\n",
    "\n",
    "    # Save the masked image\n",
    "    image.save(output_path)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "def compile_coco_json(output_folder, output_json, image_name):\n",
    "    compiled_annotations = {\n",
    "        \"images\": [],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": []  # This should be populated based on your categories\n",
    "    }\n",
    "\n",
    "    # Get image dimensions\n",
    "    image_path = os.path.join(output_folder, image_name)\n",
    "    with Image.open(image_path) as img:\n",
    "        width, height = img.size\n",
    "\n",
    "    compiled_annotations[\"images\"].append({\"id\": 1, \"file_name\": image_name, \"width\": width, \"height\": height})\n",
    "\n",
    "    annotation_id = 1\n",
    "    for json_file in glob.glob(os.path.join(output_folder, \"full_predictions_*.json\")):\n",
    "        with open(json_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            # Assuming categories are the same across all files and already populated\n",
    "            for annotation in data[\"annotations\"]:\n",
    "                # Update annotation ID and set image_id to 1\n",
    "                annotation[\"id\"] = annotation_id\n",
    "                annotation[\"image_id\"] = 1\n",
    "                compiled_annotations[\"annotations\"].append(annotation)\n",
    "                annotation_id += 1\n",
    "\n",
    "            # If categories are not populated, update them from the first file\n",
    "            if not compiled_annotations[\"categories\"]:\n",
    "                compiled_annotations[\"categories\"] = data[\"categories\"]\n",
    "\n",
    "    output_path = os.path.join(output_folder, output_json)\n",
    "\n",
    "    # Save the compiled annotations to a single JSON file\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(compiled_annotations, f)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def visualize_coco_masks(image_path, coco_json_path, output_path, figure_size=(50, 50), font_size=20):\n",
    "    # Load the COCO JSON file\n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert('RGB')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Load a font for drawing text\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", size=font_size)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    # Iterate over annotations and draw each segmentation\n",
    "    for i, annotation in enumerate(coco_data[\"annotations\"], 1):\n",
    "        segmentation = annotation[\"segmentation\"][0]  # Assuming one polygon per annotation\n",
    "        \n",
    "        # Choose brighter and more saturated colors\n",
    "        color = tuple(random.choices(range(100, 256), k=3))  # Random, brighter color\n",
    "\n",
    "        # Draw the segmentation\n",
    "        draw.polygon(segmentation, outline=color, fill=None)\n",
    "\n",
    "        # Calculate the centroid of the mask\n",
    "        centroid_x = sum(segmentation[::2]) / (len(segmentation) / 2)\n",
    "        centroid_y = sum(segmentation[1::2]) / (len(segmentation) / 2)\n",
    "\n",
    "        # Draw the index of the mask, slightly to the right of the centroid\n",
    "        draw.text((centroid_x + 5, centroid_y), str(annotation['id']), fill=color, font=font)\n",
    "\n",
    "    # Display the image in a larger figure\n",
    "    plt.figure(figsize=figure_size)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Save the image with masks\n",
    "    image.save(output_path)\n",
    "\n",
    "def visualize_coco_masks_bbox(image_path, coco_json_path, output_path, figure_size=(50, 50), font_size=20):\n",
    "    # Load the COCO JSON file\n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    image = image.convert('RGB')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Load a font for drawing text\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", size=font_size)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    # Iterate over annotations and draw each segmentation and bounding box\n",
    "    for i, annotation in enumerate(coco_data[\"annotations\"], 1):\n",
    "        segmentation = annotation[\"segmentation\"][0]  # Assuming one polygon per annotation\n",
    "        bbox = annotation[\"bbox\"]  # [x, y, width, height]\n",
    "\n",
    "        # Choose brighter and more saturated colors\n",
    "        color = tuple(random.choices(range(100, 256), k=3))  # Random, brighter color\n",
    "\n",
    "        # Draw the segmentation\n",
    "        draw.polygon(segmentation, outline=color, fill=None)\n",
    "\n",
    "        # Draw the bounding box\n",
    "        # Convert COCO bbox format [x, y, width, height] to [x0, y0, x1, y1]\n",
    "        bbox_coords = [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1] + bbox[3]]\n",
    "        draw.rectangle(bbox_coords, outline=color, width=2)\n",
    "\n",
    "        # Calculate the centroid of the mask for placing the index text\n",
    "        centroid_x = sum(segmentation[::2]) / (len(segmentation) / 2)\n",
    "        centroid_y = sum(segmentation[1::2]) / (len(segmentation) / 2)\n",
    "\n",
    "        # Draw the index of the mask, slightly to the right of the centroid\n",
    "        draw.text((centroid_x + 5, centroid_y), str(annotation['id']), fill=color, font=font)\n",
    "\n",
    "    # Display the image in a larger figure\n",
    "    plt.figure(figsize=figure_size)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Save the image with masks and bounding boxes\n",
    "    image.save(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ac4990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to Clean Annotations\n",
    "def clean_annotations(image_path, json_path, output_path, intensity_threshold=1):\n",
    "    #Load the image using PIL and convert to numpy array for processing\n",
    "    try:\n",
    "        pil_image = Image.open(image_path).convert('L')\n",
    "        image = np.array(pil_image)\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(f\"Failed to load image at path {image_path}: {str(e)}\")\n",
    "\n",
    "    # Load the JSON data\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    annotations = data.get('annotations', [])\n",
    "\n",
    "    # Calculate the average intensity of the entire image\n",
    "    average_intensity_image = np.mean(image)\n",
    "    print(\"Average intensity of the entire image:\", average_intensity_image)\n",
    "\n",
    "    # Initialize a list to hold annotations that pass the filter\n",
    "    filtered_annotations = []\n",
    "\n",
    "    for annotation in annotations:\n",
    "        if 'segmentation' not in annotation or not isinstance(annotation['segmentation'][0], list):\n",
    "            continue\n",
    "\n",
    "        # Create a mask for the current annotation\n",
    "        mask = np.zeros(image.shape, dtype=np.uint8)\n",
    "        for polygon in annotation['segmentation']:\n",
    "            points = np.array(polygon, dtype=np.int32).reshape((-1, 1, 2))\n",
    "            cv2.fillPoly(mask, [points], 255)\n",
    "\n",
    "        if np.count_nonzero(mask) == 0:\n",
    "            print(f\"Annotation ID {annotation['id']} has an empty mask. Check polygon coordinates.\")\n",
    "            continue\n",
    "\n",
    "        # Extract the pixel values where the mask is applied\n",
    "        masked_pixels = image[mask > 0]\n",
    "        if masked_pixels.size == 0:\n",
    "            print(f\"Annotation ID {annotation['id']} has a zero-area mask after application.\")\n",
    "            continue\n",
    "\n",
    "        # Calculate the average intensity of the pixels within the mask\n",
    "        average_intensity_mask = np.mean(masked_pixels)\n",
    "        #print(f\"Annotation ID {annotation['id']} - Mask Average Intensity:\", average_intensity_mask)\n",
    "\n",
    "        # Compare the mask's average intensity to the image's average intensity\n",
    "        if average_intensity_mask >= average_intensity_image * intensity_threshold:\n",
    "            filtered_annotations.append(annotation)\n",
    "        else:\n",
    "            print(f\"Annotation ID {annotation['id']} removed due to low intensity.\")\n",
    "    \n",
    "    # Update the JSON data with filtered annotations\n",
    "    data['annotations'] = filtered_annotations\n",
    "\n",
    "    # Save the cleaned data to a JSON file\n",
    "    with open(output_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "    print(f\"Cleaned annotations saved to {output_path}\")\n",
    "\n",
    "def convert_seconds_to_hms(seconds):\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "    return int(hours), int(minutes), seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e15147b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for Merging\n",
    "def calculate_polygon_area(polygon):\n",
    "    x = [point[0] for point in polygon]\n",
    "    y = [point[1] for point in polygon]\n",
    "    return 0.5 * abs(sum(x[i] * y[(i + 1) % len(polygon)] for i in range(len(polygon))) -\n",
    "                     sum(y[i] * x[(i + 1) % len(polygon)] for i in range(len(polygon))))\n",
    "\n",
    "def calculate_centroid(points):\n",
    "    area = C_x = C_y = 0.0\n",
    "    for i in range(len(points)):\n",
    "        j = (i + 1) % len(points)\n",
    "        cross = points[i][0] * points[j][1] - points[j][0] * points[i][1]\n",
    "        area += cross\n",
    "        C_x += (points[i][0] + points[j][0]) * cross\n",
    "        C_y += (points[i][1] + points[j][1]) * cross\n",
    "    area *= 0.5\n",
    "    if area != 0:\n",
    "        C_x /= (6 * area)\n",
    "        C_y /= (6 * area)\n",
    "    return (C_x, C_y)\n",
    "\n",
    "def calculate_principal_axes(points):\n",
    "    points_array = np.array(points)\n",
    "    centroid = points_array.mean(axis=0)\n",
    "    centered_points = points_array - centroid\n",
    "    covariance_matrix = np.cov(centered_points.T)  # Note the transpose for correct dimensionality\n",
    "    eigenvalues, _ = np.linalg.eig(covariance_matrix)\n",
    "    axes_lengths = np.sqrt(eigenvalues)\n",
    "    major_axis_length, minor_axis_length = np.sort(axes_lengths)[::-1]\n",
    "    return major_axis_length, minor_axis_length\n",
    "\n",
    "def calculate_orientation(points):\n",
    "    points_array = np.array(points)\n",
    "    centroid = points_array.mean(axis=0)\n",
    "    centered_points = points_array - centroid\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(centered_points)\n",
    "    major_axis_vector = pca.components_[0]  # First principal component\n",
    "    angle = np.arctan2(major_axis_vector[1], major_axis_vector[0])\n",
    "    return np.degrees(angle) % 360\n",
    "\n",
    "def create_polygon_from_points(points):\n",
    "    return Polygon(points)\n",
    "\n",
    "def calculate_intersection_area(poly1, poly2):\n",
    "    intersection = poly1.intersection(poly2)\n",
    "    return intersection.area\n",
    "\n",
    "def calculate_line_overlap_length(poly1, poly2):\n",
    "    # Ensure polygons are valid and simplifying them might resolve some edge cases\n",
    "    poly1 = poly1.simplify(0.01, preserve_topology=True)\n",
    "    poly2 = poly2.simplify(0.01, preserve_topology=True)\n",
    "\n",
    "    centroid1 = poly1.centroid.coords[0]\n",
    "    centroid2 = poly2.centroid.coords[0]\n",
    "    line = LineString([centroid1, centroid2])\n",
    "\n",
    "    intersection_line = line.intersection(poly1.intersection(poly2))\n",
    "    \n",
    "    # If there's no intersection or if it's a Point, there's no overlap length\n",
    "    if intersection_line.is_empty or intersection_line.geom_type == 'Point':\n",
    "        return 0\n",
    "\n",
    "    # Make sure we're calculating the length of a LineString\n",
    "    if isinstance(intersection_line, LineString):\n",
    "        return intersection_line.length\n",
    "    elif hasattr(intersection_line, 'geoms'):  # It's a MultiLineString or similar\n",
    "        return sum(geom.length for geom in intersection_line.geoms)\n",
    "    else:\n",
    "        # Log unexpected geometry types\n",
    "        print(f\"Unexpected geometry type: {intersection_line.geom_type}\")\n",
    "        return 0\n",
    "\n",
    "def euclidean_distance(point1, point2):\n",
    "    return np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)\n",
    "\n",
    "def angle_difference(angle1, angle2):\n",
    "    direct_diff = abs(angle1 - angle2) % 360\n",
    "    reverse_diff = abs(180 - direct_diff)\n",
    "    return direct_diff, reverse_diff\n",
    "\n",
    "def convex_hull_area(points1, points2):\n",
    "    points = np.vstack((points1, points2))\n",
    "    hull = ConvexHull(points)\n",
    "    return hull.volume\n",
    "\n",
    "def combine_areas(area1, area2):\n",
    "    return area1 + area2\n",
    "\n",
    "def plot_orientation(ax, centroid, length, angle, color):\n",
    "    # This function adds an orientation line to the plot based on centroid, length, and angle\n",
    "    rad = np.deg2rad(angle)\n",
    "    x = [centroid[0], centroid[0] + length * np.cos(rad)]\n",
    "    y = [centroid[1], centroid[1] + length * np.sin(rad)]\n",
    "    ax.plot(x, y, color=color, linestyle='dashed')\n",
    "\n",
    "def visualize_annotations(ann1, ann2):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Plot the polygons\n",
    "    poly1 = create_polygon_from_points(ann1['polygon_points'])\n",
    "    poly2 = create_polygon_from_points(ann2['polygon_points'])\n",
    "    \n",
    "    x1, y1 = poly1.exterior.xy\n",
    "    x2, y2 = poly2.exterior.xy\n",
    "    \n",
    "    ax.fill(x1, y1, alpha=0.5, fc='red', label=f'Annotation {ann1[\"annotation_id\"]}')\n",
    "    ax.fill(x2, y2, alpha=0.5, fc='blue', label=f'Annotation {ann2[\"annotation_id\"]}')\n",
    "    \n",
    "    # Plot orientation vectors\n",
    "    plot_orientation(ax, ann1['centroid'], ann1['principal_axes_lengths']['major_axis_length'], ann1['orientation'], 'red')\n",
    "    plot_orientation(ax, ann2['centroid'], ann2['principal_axes_lengths']['major_axis_length'], ann2['orientation'], 'blue')\n",
    "    \n",
    "    # Settings and labels\n",
    "    ax.set_xlabel('X coordinate')\n",
    "    ax.set_ylabel('Y coordinate')\n",
    "    ax.set_title('Annotation Comparison')\n",
    "    ax.legend()\n",
    "\n",
    "    plt.axis('equal')\n",
    "    plt.show()\n",
    "\n",
    "def process_coco_json(file_path):\n",
    "    with open(file_path) as f:\n",
    "        data = json.load(f)\n",
    "    annotations = data.get('annotations', [])\n",
    "    results = []\n",
    "\n",
    "    # First, parse all annotations and calculate necessary geometric properties\n",
    "    for annotation in annotations:\n",
    "        if 'segmentation' in annotation:\n",
    "            for polygon_data in annotation['segmentation']:\n",
    "                if isinstance(polygon_data, list):\n",
    "                    if all(isinstance(elem, list) for elem in polygon_data):\n",
    "                        polygon_data = [coord for sublist in polygon_data for coord in sublist]\n",
    "                    polygon_points = [(polygon_data[i], polygon_data[i + 1]) for i in range(0, len(polygon_data), 2)]\n",
    "                    area = calculate_polygon_area(polygon_points)\n",
    "                    centroid = calculate_centroid(polygon_points)\n",
    "                    orientation = calculate_orientation(polygon_points)\n",
    "                    major_axis_length, minor_axis_length = calculate_principal_axes(polygon_points)\n",
    "                    result = {\n",
    "                        'annotation_id': annotation['id'],\n",
    "                        'area': area,\n",
    "                        'bbox': annotation['bbox'],\n",
    "                        'centroid': centroid,\n",
    "                        'orientation': orientation,\n",
    "                        'principal_axes_lengths': {\n",
    "                            'major_axis_length': major_axis_length,\n",
    "                            'minor_axis_length': minor_axis_length\n",
    "                        },\n",
    "                        'polygon_points': polygon_points,\n",
    "                        'nearby_ids': [],  # Placeholder for nearby annotation IDs\n",
    "                        'segmentation': annotation['segmentation']\n",
    "                    }\n",
    "                    results.append(result)\n",
    "\n",
    "    # Calculate proximity of annotations based on the specified criteria\n",
    "    for i, ann1 in enumerate(results):\n",
    "        for j, ann2 in enumerate(results):\n",
    "            if i != j:\n",
    "                distance = euclidean_distance(ann1['centroid'], ann2['centroid'])\n",
    "                if distance < 3 * ann1['principal_axes_lengths']['major_axis_length']:\n",
    "                    ann1['nearby_ids'].append(ann2['annotation_id'])\n",
    "\n",
    "    return results\n",
    "\n",
    "def merge_annotations(ann1, ann2):\n",
    "    # Combine all polygon points from both annotations\n",
    "    all_points = ann1['polygon_points'] + ann2['polygon_points']\n",
    "\n",
    "    # Create a MultiPoint object and calculate the convex hull\n",
    "    multipoint = MultiPoint(all_points)\n",
    "    convex_hull = multipoint.convex_hull\n",
    "\n",
    "    # Convert convex hull to a list of tuples (polygon points)\n",
    "    if convex_hull.geom_type == 'Polygon':\n",
    "        hull_points_list = list(convex_hull.exterior.coords)\n",
    "    else:\n",
    "        # Handle edge case where the convex hull is not a polygon (line or point)\n",
    "        hull_points_list = all_points\n",
    "\n",
    "    new_area = calculate_polygon_area(hull_points_list)\n",
    "    new_centroid = calculate_centroid(hull_points_list)\n",
    "    new_orientation = calculate_orientation(hull_points_list)\n",
    "    new_major_axis_length, new_minor_axis_length = calculate_principal_axes(hull_points_list)\n",
    "    bbox = [int(coord) for coord in convex_hull.bounds]\n",
    "\n",
    "    # Merging ancestors from both annotations to track history\n",
    "    ancestors = set(ann1.get('ancestors', [ann1['annotation_id']]) + ann2.get('ancestors', [ann2['annotation_id']]))\n",
    "\n",
    "    return {\n",
    "        'annotation_id': f\"merged-{ann1['annotation_id']}-{ann2['annotation_id']}\",\n",
    "        'area': new_area,\n",
    "        'centroid': new_centroid,\n",
    "        'orientation': new_orientation,\n",
    "        'principal_axes_lengths': {\n",
    "            'major_axis_length': new_major_axis_length,\n",
    "            'minor_axis_length': new_minor_axis_length\n",
    "        },\n",
    "        'polygon_points': hull_points_list,\n",
    "        'segmentation': [coord for point in hull_points_list for coord in point],\n",
    "        'bbox': bbox,\n",
    "        'ancestors': list(ancestors)  # Track all contributing annotations\n",
    "    }\n",
    "\n",
    "def check_compatibility_single(ann1, ann2):\n",
    "    # Prevent merging if any ancestors are the same\n",
    "    if set(ann1.get('ancestors', [ann1['annotation_id']])) & set(ann2.get('ancestors', [ann2['annotation_id']])):\n",
    "        return False  # Annotations share a common ancestor, so do not merge\n",
    "    \n",
    "    # Create polygons from the points in the annotations\n",
    "    poly1 = Polygon(ann1['polygon_points'])\n",
    "    poly2 = Polygon(ann2['polygon_points'])\n",
    "    \n",
    "    # Calculate intersection area and check if overlap exceeds 50%\n",
    "    intersection = poly1.intersection(poly2)\n",
    "    intersection_area = intersection.area\n",
    "    \n",
    "    overlap_percent_ann1 = intersection_area / ann1['area'] if ann1['area'] > 0 else 0\n",
    "    overlap_percent_ann2 = intersection_area / ann2['area'] if ann2['area'] > 0 else 0\n",
    "    \n",
    "    if overlap_percent_ann1 >= 0.5 or overlap_percent_ann2 >= 0.5:\n",
    "        return True  # Combine the annotations due to significant overlap\n",
    "\n",
    "    # Use pre-calculated principal axes lengths to compute the diagonal ratio\n",
    "    major_axis_length_ann1 = ann1['principal_axes_lengths']['major_axis_length']\n",
    "    minor_axis_length_ann1 = ann1['principal_axes_lengths']['minor_axis_length']\n",
    "    diagonal_ratio_ann1 = minor_axis_length_ann1 / major_axis_length_ann1 if major_axis_length_ann1 > 0 else 0\n",
    "\n",
    "    major_axis_length_ann2 = ann2['principal_axes_lengths']['major_axis_length']\n",
    "    minor_axis_length_ann2 = ann2['principal_axes_lengths']['minor_axis_length']\n",
    "    diagonal_ratio_ann2 = minor_axis_length_ann2 / major_axis_length_ann2 if major_axis_length_ann2 > 0 else 0\n",
    "\n",
    "    # Check if diagonal ratio for either polygon exceeds 0.8\n",
    "    ignore_orientation = diagonal_ratio_ann1 > 0.33 or diagonal_ratio_ann2 > 0.33\n",
    "    \n",
    "    # Additional checks\n",
    "    overlap_length = calculate_line_overlap_length(poly1, poly2)\n",
    "    dist = euclidean_distance(ann1['centroid'], ann2['centroid'])\n",
    "    direct_diff, reverse_diff = angle_difference(ann1['orientation'], ann2['orientation'])\n",
    "    orientation_diff = min(direct_diff, reverse_diff)\n",
    "    convex_hull_ratio = convex_hull_area(np.array(ann1['polygon_points']), np.array(ann2['polygon_points'])) / combine_areas(ann1['area'], ann2['area'])\n",
    "\n",
    "    # Evaluate compatibility considering the diagonal ratio effect on orientation difference\n",
    "    return (dist < 3 * max(major_axis_length_ann1, major_axis_length_ann2) and\n",
    "            (ignore_orientation or orientation_diff <= 5) and\n",
    "            convex_hull_ratio < 1.3 and\n",
    "            dist > ((major_axis_length_ann1 + major_axis_length_ann2 - overlap_length)))\n",
    "\n",
    "def progressive_merge_annotations(annotations):\n",
    "    # If there are fewer than 2 annotations, there's nothing to merge.\n",
    "    if len(annotations) < 2:\n",
    "        return annotations\n",
    "\n",
    "    # Extract centroids and initialize the kd-tree\n",
    "    points = np.array([ann['centroid'] for ann in annotations])\n",
    "    kd_tree = cKDTree(points)\n",
    "\n",
    "    remaining_annotations = copy.deepcopy(annotations)\n",
    "    merged = True\n",
    "\n",
    "    while merged:\n",
    "        merged = False\n",
    "        new_annotations = []\n",
    "        new_points = []  # This will collect the new list of points corresponding to remaining_annotations\n",
    "        skip_indices = set()\n",
    "\n",
    "        for i, ann1 in enumerate(remaining_annotations):\n",
    "            if i in skip_indices:\n",
    "                continue\n",
    "\n",
    "            # Query for close neighbors (within some threshold distance).\n",
    "            # Adjust k=50 and the distance threshold to suit your needs.\n",
    "            distances, knn_indices = kd_tree.query(points[i], k=50)\n",
    "            # Convert single int to list if there's only 1 neighbor\n",
    "            if isinstance(knn_indices, (int, np.integer)):\n",
    "                knn_indices = [knn_indices]\n",
    "\n",
    "            # Remove out-of-bound indices, skip indices, and the point itself\n",
    "            knn_indices = [\n",
    "                idx for idx in knn_indices \n",
    "                if 0 <= idx < len(remaining_annotations)\n",
    "                and idx not in skip_indices \n",
    "                and idx != i\n",
    "            ]\n",
    "\n",
    "            for j in knn_indices:\n",
    "                ann2 = remaining_annotations[j]\n",
    "                if check_compatibility_single(ann1, ann2):\n",
    "                    merged_annotation = merge_annotations(ann1, ann2)\n",
    "\n",
    "                    # Recalculate the segmentation and bounding box\n",
    "                    merged_annotation['segmentation'] = [\n",
    "                        coord for point in merged_annotation['polygon_points'] for coord in point\n",
    "                    ]\n",
    "                    merged_annotation['centroid'] = calculate_centroid(merged_annotation['polygon_points'])\n",
    "                    x_coords, y_coords = zip(*merged_annotation['polygon_points'])\n",
    "                    merged_annotation['bbox'] = [\n",
    "                        int(min(x_coords)), \n",
    "                        int(min(y_coords)), \n",
    "                        int(max(x_coords) - min(x_coords)), \n",
    "                        int(max(y_coords) - min(y_coords))\n",
    "                    ]\n",
    "                    merged_annotation['nearby_ids'] = list(\n",
    "                        set(ann1.get('nearby_ids', []) + ann2.get('nearby_ids', [])) \n",
    "                        - {ann1['annotation_id'], ann2['annotation_id']}\n",
    "                    )\n",
    "\n",
    "                    new_point = [\n",
    "                        merged_annotation['bbox'][0] + merged_annotation['bbox'][2] / 2,\n",
    "                        merged_annotation['bbox'][1] + merged_annotation['bbox'][3] / 2\n",
    "                    ]\n",
    "\n",
    "                    new_annotations.append(merged_annotation)\n",
    "                    new_points.append(new_point)\n",
    "\n",
    "                    skip_indices.update({i, j})\n",
    "                    merged = True\n",
    "                    break\n",
    "\n",
    "            if i not in skip_indices:\n",
    "                new_annotations.append(ann1)\n",
    "                new_points.append(points[i])\n",
    "\n",
    "        # Rebuild the k-d tree with the new set of points for the next iteration\n",
    "        points = np.array(new_points)\n",
    "        kd_tree = cKDTree(points)\n",
    "        remaining_annotations = new_annotations\n",
    "\n",
    "    return remaining_annotations\n",
    "\n",
    "def flatten_segmentation(segmentation):\n",
    "    \"\"\" Ensure segmentation is a flat list of coordinates. \"\"\"\n",
    "    if isinstance(segmentation, list):\n",
    "        if all(isinstance(item, list) for item in segmentation):\n",
    "            # Flatten a list of lists\n",
    "            return [coord for sublist in segmentation for coord in sublist]\n",
    "        elif all(isinstance(item, (int, float)) for item in segmentation):\n",
    "            # Already a flat list\n",
    "            return segmentation\n",
    "    # Return None if the format is neither\n",
    "    return None\n",
    "\n",
    "def update_json_file(annotations, file_path, image_path, category_id):\n",
    "    # Load the image to get dimensions and file name\n",
    "    with Image.open(image_path) as img:\n",
    "        width, height = img.size\n",
    "        file_name = os.path.basename(image_path)\n",
    "\n",
    "    # Define a new structure for the COCO format\n",
    "    coco_data = {\n",
    "        \"images\": [{\"id\": 1, \"width\": width, \"height\": height, \"file_name\": file_name}],\n",
    "        \"annotations\": [],\n",
    "        \"categories\": [{\"id\": category_id, \"name\": \"none\", \"supercategory\": \"none\"}]\n",
    "    }\n",
    "\n",
    "    for idx, ann in enumerate(annotations, start=1):\n",
    "        # Standardize segmentation data format\n",
    "        standardized_segmentation = flatten_segmentation(ann['segmentation'])\n",
    "        if standardized_segmentation is not None:\n",
    "            coco_ann = {\n",
    "                \"id\": idx,\n",
    "                \"image_id\": 1,\n",
    "                \"category_id\": category_id,\n",
    "                \"segmentation\": [standardized_segmentation],\n",
    "                \"area\": ann['area'],\n",
    "                \"bbox\": ann['bbox'],\n",
    "                \"iscrowd\": 0\n",
    "            }\n",
    "            coco_data['annotations'].append(coco_ann)\n",
    "        else:\n",
    "            print(f\"Error in segmentation format for annotation {idx}, unable to process.\")\n",
    "\n",
    "    # Write the updated data to a JSON file\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(coco_data, f, indent=4)\n",
    "\n",
    "def check_specific_annotations(annotations, id1, id2):\n",
    "    # Retrieve the annotations by IDs\n",
    "    ann1 = next((ann for ann in annotations if ann['annotation_id'] == id1), None)\n",
    "    ann2 = next((ann for ann in annotations if ann['annotation_id'] == id2), None)\n",
    "\n",
    "    if ann1 is None or ann2 is None:\n",
    "        print(f\"Annotation not found for given IDs: {id1}, {id2}\")\n",
    "        return\n",
    "    \n",
    "    # Convert points to polygons\n",
    "    poly1 = create_polygon_from_points(ann1['polygon_points'])\n",
    "    poly2 = create_polygon_from_points(ann2['polygon_points'])\n",
    "    \n",
    "    # Perform compatibility check\n",
    "    is_compatible = check_compatibility_single(ann1, ann2)\n",
    "    \n",
    "    # Print details of the compatibility checks\n",
    "    print(f\"Checking compatibility between Annotation {id1} and Annotation {id2}:\")\n",
    "    print(f\"Compatible: {is_compatible}\")\n",
    "\n",
    "    # Use pre-calculated principal axes lengths to compute the diagonal ratio\n",
    "    major_axis_length_ann1 = ann1['principal_axes_lengths']['major_axis_length']\n",
    "    minor_axis_length_ann1 = ann1['principal_axes_lengths']['minor_axis_length']\n",
    "    diagonal_ratio_ann1 = minor_axis_length_ann1 / major_axis_length_ann1 if major_axis_length_ann1 > 0 else 0\n",
    "\n",
    "    major_axis_length_ann2 = ann2['principal_axes_lengths']['major_axis_length']\n",
    "    minor_axis_length_ann2 = ann2['principal_axes_lengths']['minor_axis_length']\n",
    "    diagonal_ratio_ann2 = minor_axis_length_ann2 / major_axis_length_ann2 if major_axis_length_ann2 > 0 else 0\n",
    "\n",
    "    # Check if diagonal ratio for either polygon exceeds 0.8\n",
    "    ignore_orientation = diagonal_ratio_ann1 > 0.33 or diagonal_ratio_ann2 > 0.33    \n",
    "\n",
    "    # Additional details\n",
    "    intersection_area = calculate_intersection_area(poly1, poly2)\n",
    "    overlap_length = calculate_line_overlap_length(poly1, poly2)\n",
    "    dist = euclidean_distance(ann1['centroid'], ann2['centroid'])\n",
    "    direct_diff, reverse_diff = angle_difference(ann1['orientation'], ann2['orientation'])\n",
    "    orientation_diff = min(direct_diff, reverse_diff)\n",
    "    convex_hull_ratio = convex_hull_area(np.array(ann1['polygon_points']), np.array(ann2['polygon_points'])) / combine_areas(ann1['area'], ann2['area'])\n",
    "    dist_Threshold = ann1['principal_axes_lengths']['major_axis_length'] + ann2['principal_axes_lengths']['major_axis_length'] - overlap_length\n",
    "\n",
    "    print(f\"Intersection Area: {intersection_area}\")\n",
    "    print(f\"Overlap Length: {overlap_length}\")\n",
    "    print('----------')\n",
    "    print(f\"Distance Thresholds: {dist_Threshold}\")\n",
    "    print(f\"Distance Between Centroids: {dist}\")\n",
    "    print(f\"Should be greater than Threshold\")\n",
    "    print('----------')\n",
    "    print(f'Ignore Orientation:{ignore_orientation}')\n",
    "    print(f'Orientation Threshold: 5 degrees')\n",
    "    print(f\"Orientation Difference: {orientation_diff}\")\n",
    "    print(f\"Direct Difference: {direct_diff}\")\n",
    "    print(f\"Reverse Difference: {reverse_diff}\")\n",
    "    print(f\"Should be less than Threshold\")\n",
    "    print('----------')\n",
    "    print(f\"Convex Hull Threshold: 1.3\")\n",
    "    print(f\"Convex Hull Ratio: {convex_hull_ratio}\")\n",
    "    print(f\"Should be less than Threshold\")\n",
    "\n",
    "    # Optional: Visualize the annotations for visual inspection\n",
    "    visualize_annotations(ann1, ann2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9fc58d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjustment Functions\n",
    "def add_annotations_to_coco_json(file1_path, file2_path, output_path):\n",
    "    # Load the first file\n",
    "    with open(file1_path, 'r') as f:\n",
    "        data1 = json.load(f)\n",
    "\n",
    "    # Check if the second file exists\n",
    "    if not os.path.exists(file2_path):\n",
    "        # Save the first file's data to the output path and return\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(data1, f)\n",
    "        return data1\n",
    "    \n",
    "    # Load the second file\n",
    "    with open(file2_path, 'r') as f:\n",
    "        data2 = json.load(f)\n",
    "    \n",
    "    # Start IDs from the maximum found in the second file + 1\n",
    "    max_annotation_id = max([ann['id'] for ann in data2['annotations']], default=0) + 1\n",
    "    max_image_id = max([img['id'] for img in data2['images']], default=0) + 1\n",
    "\n",
    "    # Update the first file's IDs\n",
    "    annotation_id_map = {}\n",
    "    image_id_map = {}\n",
    "\n",
    "    for image in data1['images']:\n",
    "        original_id = image['id']\n",
    "        new_id = max_image_id\n",
    "        image_id_map[original_id] = new_id\n",
    "        image['id'] = new_id\n",
    "        max_image_id += 1\n",
    "\n",
    "    for annotation in data1['annotations']:\n",
    "        original_id = annotation['id']\n",
    "        new_id = max_annotation_id\n",
    "        annotation_id_map[original_id] = new_id\n",
    "        annotation['id'] = new_id\n",
    "        annotation['image_id'] = image_id_map[annotation['image_id']]\n",
    "        max_annotation_id += 1\n",
    "\n",
    "    # Append the updated annotations and images from the first file to the second file\n",
    "    data2['annotations'].extend(data1['annotations'])\n",
    "    data2['images'].extend(data1['images'])\n",
    "\n",
    "    # Save the updated data\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(data2, f)\n",
    "\n",
    "    return data2\n",
    "\n",
    "def adjust_coco_annotations(coco_json_path, top_padding, bottom_padding, left_padding, right_padding, output_json_path):\n",
    "    with open(coco_json_path) as file:\n",
    "        coco_data = json.load(file)\n",
    "\n",
    "    for image in coco_data['images']:\n",
    "        image['height'] -= (top_padding + bottom_padding)\n",
    "        image['width'] -= (left_padding + right_padding)\n",
    "\n",
    "    for annotation in coco_data['annotations']:\n",
    "        bbox = annotation['bbox']\n",
    "        bbox[0] -= left_padding\n",
    "        bbox[1] -= top_padding\n",
    "        annotation['bbox'] = bbox\n",
    "\n",
    "        if 'segmentation' in annotation:\n",
    "            new_segmentation = []\n",
    "            for segment in annotation['segmentation']:\n",
    "                if isinstance(segment, list):\n",
    "                    new_polygon = []\n",
    "                    points = iter(segment)\n",
    "                    for x, y in zip(points, points):\n",
    "                        new_x = x - left_padding\n",
    "                        new_y = y - top_padding\n",
    "                        new_polygon.extend([new_x, new_y])\n",
    "                    new_segmentation.append(new_polygon)\n",
    "            annotation['segmentation'] = new_segmentation\n",
    "\n",
    "    with open(output_json_path, 'w') as file:\n",
    "        json.dump(coco_data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c12c43d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics Functions\n",
    "def load_json(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def load_coco_annotations(annotation_path):\n",
    "    coco = COCO(annotation_path)\n",
    "    return coco\n",
    "\n",
    "def calc_bounds(mean, std_dev, upper_multiplier, lower_multiplier):\n",
    "    return mean + upper_multiplier * std_dev, mean - lower_multiplier * std_dev\n",
    "\n",
    "def filter_coordinates(x, y, z, rounds, background_value=0):\n",
    "    filtered_x, filtered_y, filtered_z = cp.array(x), cp.array(y), cp.array(z)\n",
    "    for round_details in rounds:\n",
    "        mask = filtered_z > background_value\n",
    "        filtered_x, filtered_y, filtered_z = filtered_x[mask], filtered_y[mask], filtered_z[mask]\n",
    "\n",
    "        mean_z, std_dev_z = cp.mean(filtered_z), cp.std(filtered_z)\n",
    "        upper_z, lower_z = calc_bounds(mean_z, std_dev_z, *round_details['z'])\n",
    "        z_mask = (filtered_z <= upper_z) & (filtered_z >= lower_z)\n",
    "\n",
    "        mean_x, std_dev_x = cp.mean(filtered_x[z_mask]), cp.std(filtered_x[z_mask])\n",
    "        upper_x, lower_x = calc_bounds(mean_x, std_dev_x, *round_details['x'])\n",
    "        x_mask = (filtered_x[z_mask] <= upper_x) & (filtered_x[z_mask] >= lower_x)\n",
    "\n",
    "        mean_y, std_dev_y = cp.mean(filtered_y[z_mask][x_mask]), cp.std(filtered_y[z_mask][x_mask])\n",
    "        upper_y, lower_y = calc_bounds(mean_y, std_dev_y, *round_details['y'])\n",
    "        y_mask = (filtered_y[z_mask][x_mask] <= upper_y) & (filtered_y[z_mask][x_mask] >= lower_y)\n",
    "\n",
    "        filtered_x, filtered_y, filtered_z = filtered_x[z_mask][x_mask][y_mask], filtered_y[z_mask][x_mask][y_mask], filtered_z[z_mask][x_mask][y_mask]\n",
    "\n",
    "    return filtered_x, filtered_y, filtered_z\n",
    "\n",
    "def trim_noise_based_on_coordinate(coordinate, z_values, upper_std_dev, lower_std_dev, background_value=0):\n",
    "    unique_coords = cp.unique(coordinate)\n",
    "    filtered_indices = []\n",
    "    for unique_val in unique_coords:\n",
    "        local_mask = coordinate == unique_val\n",
    "        local_z_values = z_values[local_mask]\n",
    "        if local_z_values.size == 0:  # Check if local_z_values is empty\n",
    "            continue  # Skip this unique coordinate if there's no data\n",
    "\n",
    "        local_mean, local_std = cp.mean(local_z_values), cp.std(local_z_values)\n",
    "        upper, lower = calc_bounds(local_mean, local_std, upper_std_dev, lower_std_dev)\n",
    "        \n",
    "        valid_indices = cp.where((local_z_values <= upper) & (local_z_values >= max(lower, background_value)))[0]\n",
    "        \n",
    "        if valid_indices.size > 0:  # Ensure we only append non-empty results\n",
    "            filtered_indices.append(valid_indices)\n",
    "    \n",
    "    # Check if filtered_indices is empty before concatenation\n",
    "    if not filtered_indices:\n",
    "        return cp.array([])  # Return an empty array if no valid indices found\n",
    "    \n",
    "    return cp.concatenate(filtered_indices)\n",
    "\n",
    "def filter_gaussian_test(x, y, z, final_adjust=0.66, background_value=0):\n",
    "    rounds = [\n",
    "        {'z': (3, 1), 'x': (2, 2), 'y': (2, 2)},\n",
    "        {'z': (3.25, 1), 'x': (2, 2), 'y': (2, 2)},\n",
    "        {'z': (3.5, 1), 'x': (2, 2), 'y': (2, 2)},\n",
    "        {'z': (3.75, 1), 'x': (2, 2), 'y': (2, 2)},\n",
    "        {'z': (4, 1), 'x': (2, 2), 'y': (2, 2)},\n",
    "        {'z': (3.85, 0.96), 'x': (2, 2), 'y': (2, 2)},\n",
    "        {'z': (3.7, 0.92), 'x': (2, 2), 'y': (2, 2)},\n",
    "        {'z': (3.55, 0.88), 'x': (2, 2), 'y': (2, 2)},\n",
    "        {'z': (3.4, 0.84), 'x': (2, 2), 'y': (2, 2)},\n",
    "        {'z': (3.25, 0.8), 'x': (1.5, 1.5), 'y': (1.5, 1.5)},\n",
    "    ]\n",
    "    \n",
    "    # Assuming filter_coordinates is defined elsewhere\n",
    "    x_filtered, y_filtered, z_filtered = filter_coordinates(x, y, z, rounds, background_value)\n",
    "\n",
    "    filtered_indices_x = trim_noise_based_on_coordinate(x_filtered, z_filtered, final_adjust, 5 * final_adjust)\n",
    "    filtered_indices_y = trim_noise_based_on_coordinate(y_filtered, z_filtered, final_adjust, 5 * final_adjust)\n",
    "    \n",
    "    # Find the intersection of indices in x and y\n",
    "    if filtered_indices_x.size > 0 and filtered_indices_y.size > 0:\n",
    "        intersection_indices = cp.intersect1d(filtered_indices_x, filtered_indices_y)\n",
    "    else:\n",
    "        intersection_indices = cp.array([])  # No intersection if either is empty\n",
    "\n",
    "    # Find max_z or return None if intersection_indices is empty\n",
    "    max_z = cp.max(z_filtered[intersection_indices]) if intersection_indices.size > 0 else None\n",
    "    return max_z\n",
    "\n",
    "def extract_surface_data(coco, image_path, annotation_id):\n",
    "    annotation = coco.loadAnns(annotation_id)[0]\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    mask = Image.new('L', (image.width, image.height), 0)\n",
    "    if 'segmentation' in annotation and annotation['segmentation']:\n",
    "        for segment in annotation['segmentation']:\n",
    "            ImageDraw.Draw(mask).polygon(segment, outline=1, fill=1)\n",
    "    mask = cp.array(mask)\n",
    "\n",
    "    region = cp.where(mask == 1)\n",
    "    image_np = cp.array(image)\n",
    "    pixel_values = image_np[region[0], region[1]] if image_np.ndim == 2 else image_np[region[0], region[1], 0]\n",
    "    \n",
    "    return region[1], region[0], pixel_values\n",
    "\n",
    "def calculate_centroid_seg(segmentation):\n",
    "    x = cp.array(segmentation[0::2])\n",
    "    y = cp.array(segmentation[1::2])\n",
    "    return float(cp.mean(x)), float(cp.mean(y))\n",
    "\n",
    "def calculate_principal_diagonal_from_segmentation(segmentation):\n",
    "    x, y = cp.array(segmentation[0::2]), cp.array(segmentation[1::2])\n",
    "    coordinates = cp.vstack((x, y)).T\n",
    "    pca = PCA(n_components=1)\n",
    "    pca.fit(coordinates.get())  # Move data to CPU for PCA\n",
    "    projections = cp.dot(coordinates, cp.array(pca.components_[0]))\n",
    "    return cp.max(projections) - cp.min(projections)\n",
    "\n",
    "def create_nan_masked_image(coco, image_path):\n",
    "    image = Image.open(image_path)\n",
    "    image_np = cp.array(image, dtype=cp.float32)\n",
    "    overlap_mask = cp.zeros((image_np.shape[0], image_np.shape[1]), dtype=cp.uint8)\n",
    "    \n",
    "    for annotation in coco.anns.values():\n",
    "        mask = Image.new('L', (image.width, image.height), 0)\n",
    "        if 'segmentation' in annotation and annotation['segmentation']:\n",
    "            for segment in annotation['segmentation']:\n",
    "                ImageDraw.Draw(mask).polygon(segment, outline=1, fill=1)\n",
    "        overlap_mask += cp.array(mask)\n",
    "\n",
    "    image_nan_masked = cp.where(overlap_mask > 1, cp.nan, image_np)\n",
    "    return image_nan_masked\n",
    "\n",
    "def convert_seconds_to_hms(seconds):\n",
    "    hours = seconds // 3600\n",
    "    minutes = (seconds % 3600) // 60\n",
    "    seconds = seconds % 60\n",
    "    return int(hours), int(minutes), seconds\n",
    "\n",
    "def process_annotation(annotation, coco, image_path, image_nan_masked):\n",
    "    annotation_id = annotation['id']\n",
    "    \n",
    "    # Extract data, ensuring the returned values are compatible with cupy\n",
    "    x, y, z = extract_surface_data(coco, image_path, annotation_id)\n",
    "    \n",
    "    # If x and y are CuPy arrays, convert them to NumPy for indexing\n",
    "    y_np = y.get() if isinstance(y, cp.ndarray) else y\n",
    "    x_np = x.get() if isinstance(x, cp.ndarray) else x\n",
    "    \n",
    "    # Use the already-loaded NumPy array image_nan_masked for masked values\n",
    "    z_nan_masked = cp.array([image_nan_masked[y_np[i], x_np[i]] for i in range(len(x_np))])\n",
    "\n",
    "    # Calculate centroid and principal diagonal\n",
    "    segmentation = annotation['segmentation'][0]\n",
    "    centroid_x, centroid_y = calculate_centroid_seg(segmentation)\n",
    "    principal_diagonal = calculate_principal_diagonal_from_segmentation(segmentation)\n",
    "\n",
    "    # Calculate the maximum intensity values\n",
    "    fitted_max_value = filter_gaussian_test(x, y, z_nan_masked, final_adjust=0.66, background_value=0)\n",
    "    fitted_max_value_nm = fitted_max_value * conversion_factor if fitted_max_value is not None else None\n",
    "\n",
    "    return {\n",
    "        'Annotation ID': annotation_id,\n",
    "        'Centroid X': centroid_x,\n",
    "        'Centroid Y': centroid_y,\n",
    "        'Principal Diagonal': principal_diagonal,\n",
    "        'Fitted Max Value': fitted_max_value,\n",
    "        'Fitted Max Value (nm)': fitted_max_value_nm\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52266ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization Functions\n",
    "def create_black_duplicate(image_path):\n",
    "    with Image.open(image_path) as img:\n",
    "        img_array = np.array(img)\n",
    "        shape = img_array.shape\n",
    "        black_array = np.zeros(shape, dtype=img_array.dtype)\n",
    "        black_image = Image.fromarray(black_array)\n",
    "        base, extension = os.path.splitext(image_path)\n",
    "        black_image_path = f\"{base}_black{extension}\"\n",
    "        black_image.save(black_image_path)\n",
    "        return black_image_path\n",
    "\n",
    "def get_color_from_colormap(value, min_value, max_value):\n",
    "    # Convert CuPy arrays to NumPy if needed\n",
    "    if isinstance(min_value, cp.ndarray):\n",
    "        min_value = min_value.get()\n",
    "    if isinstance(max_value, cp.ndarray):\n",
    "        max_value = max_value.get()\n",
    "    if isinstance(value, cp.ndarray):\n",
    "        value = value.get()\n",
    "\n",
    "    # Normalize 'Fitted Max Value' between 0 and 1\n",
    "    norm = mcolors.Normalize(vmin=min_value, vmax=max_value)\n",
    "    \n",
    "    # Choose a colormap\n",
    "    cmap = cm.rainbow\n",
    "    \n",
    "    # Map the normalized value to a color from the colormap\n",
    "    rgba_color = cmap(norm(value))\n",
    "    \n",
    "    # Convert RGBA color from 0-1 scale to 0-255 scale suitable for PIL, ignoring alpha\n",
    "    color = tuple(int(255 * x) for x in rgba_color[:3])\n",
    "    \n",
    "    return color\n",
    "\n",
    "def visualize_nodes_on_image_with_colormap(image_path, df_clean, output_path, black=False):\n",
    "    if black:\n",
    "        image_path = create_black_duplicate(image_path)\n",
    "\n",
    "    # Load the image\n",
    "    image = Image.open(image_path).convert(\"RGBA\")\n",
    "    \n",
    "    # Create a drawing context\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Determine the range of 'Fitted Max Value' for the colormap\n",
    "    min_value = df_clean['Fitted Max Value'].min()\n",
    "    max_value = df_clean['Fitted Max Value'].max()\n",
    "    \n",
    "    # Convert CuPy arrays to NumPy if needed\n",
    "    if isinstance(min_value, cp.ndarray):\n",
    "        min_value = min_value.get()\n",
    "    if isinstance(max_value, cp.ndarray):\n",
    "        max_value = max_value.get()\n",
    "    \n",
    "    norm_values = (df_clean['Fitted Max Value'] - min_value) / (max_value - min_value)\n",
    "\n",
    "    # Loop through DataFrame to draw each node with color from the colormap\n",
    "    for idx, row in df_clean.iterrows():\n",
    "        x, y = row['Centroid X'], row['Centroid Y']\n",
    "        value = row['Fitted Max Value']\n",
    "        \n",
    "        # Get color from colormap\n",
    "        color = get_color_from_colormap(value, min_value, max_value)\n",
    "        \n",
    "        # Calculate node size (this could be adjusted to scale with 'Fitted Max Value' if desired)\n",
    "        norm_value = norm_values.iloc[idx]\n",
    "\n",
    "        # Scale size based on normalized value\n",
    "        size = 10 + 20 * norm_value  \n",
    "       \n",
    "        # Calculate bounding box for the node\n",
    "        bbox = [x - size, y - size, x + size, y + size]\n",
    "        \n",
    "        # Draw the filled node\n",
    "        draw.ellipse(bbox, fill=color)\n",
    "    \n",
    "    # Convert the image back to RGB to discard alpha and save\n",
    "    image = image.convert(\"RGB\")\n",
    "    image.save(output_path)\n",
    "    \n",
    "    # Display the image inline\n",
    "    plt.figure(figsize=(30, 30))  # Adjust the figure size as needed\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')  # Hide the axes\n",
    "    plt.show()\n",
    "\n",
    "def visualize_and_connect_nodes(image_path, df_clean, output_path, black=False):\n",
    "    if black:\n",
    "        image_path = create_black_duplicate(image_path)\n",
    "    image = Image.open(image_path).convert(\"RGBA\")\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    # Determine the range of 'Fitted Max Value' for the colormap\n",
    "    min_value = df_clean['Fitted Max Value'].min()\n",
    "    max_value = df_clean['Fitted Max Value'].max()\n",
    "\n",
    "    # Convert CuPy arrays to NumPy if needed\n",
    "    if isinstance(min_value, cp.ndarray):\n",
    "        min_value = min_value.get()\n",
    "    if isinstance(max_value, cp.ndarray):\n",
    "        max_value = max_value.get()\n",
    "\n",
    "    # Coordinates\n",
    "    coords = df_clean[['Centroid X', 'Centroid Y']].values\n",
    "\n",
    "    # Only compute NearestNeighbors if we have at least 2 points\n",
    "    if len(coords) > 1:\n",
    "        # Use min(6, len(coords)) as the number of neighbors\n",
    "        n_neighbors = min(6, len(coords))\n",
    "        nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='ball_tree').fit(coords)\n",
    "        distances, indices = nbrs.kneighbors(coords)\n",
    "\n",
    "        # Add connections to DataFrame, excluding the point itself (the first entry)\n",
    "        df_clean['Connections'] = [list(ind[1:]) for ind in indices]\n",
    "    else:\n",
    "        # If there's only 0 or 1 point, no connections can be made\n",
    "        df_clean['Connections'] = [[] for _ in range(len(df_clean))]\n",
    "\n",
    "    # Plot each node and its connections\n",
    "    for idx, row in df_clean.iterrows():\n",
    "        x, y = row['Centroid X'], row['Centroid Y']\n",
    "        value = row['Fitted Max Value']\n",
    "\n",
    "        # Convert CuPy array to NumPy, if necessary\n",
    "        if isinstance(value, cp.ndarray):\n",
    "            value = value.get()\n",
    "\n",
    "        color = get_color_from_colormap(value, min_value, max_value)\n",
    "\n",
    "        # Draw connections\n",
    "        for neighbor_idx in row['Connections']:\n",
    "            neighbor_x = df_clean.loc[neighbor_idx, 'Centroid X']\n",
    "            neighbor_y = df_clean.loc[neighbor_idx, 'Centroid Y']\n",
    "            draw.line((x, y, neighbor_x, neighbor_y), fill=color, width=2)\n",
    "\n",
    "        # Draw the node as a circle\n",
    "        size = 10 + 20 * ((value - min_value) / (max_value - min_value))\n",
    "        bbox = [x - size, y - size, x + size, y + size]\n",
    "        draw.ellipse(bbox, fill=color)\n",
    "\n",
    "    # Convert image back to RGB and save\n",
    "    image = image.convert(\"RGB\")\n",
    "    image.save(output_path)\n",
    "\n",
    "    # Display the image\n",
    "    plt.figure(figsize=(30, 30))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def create_colorbar(min_value, max_value, colormap):\n",
    "    # Convert CuPy arrays to NumPy if needed\n",
    "    if isinstance(min_value, cp.ndarray):\n",
    "        min_value = min_value.get()\n",
    "    if isinstance(max_value, cp.ndarray):\n",
    "        max_value = max_value.get()\n",
    "\n",
    "    # Create a figure and a single subplot\n",
    "    fig, ax = plt.subplots(figsize=(6, 1))\n",
    "    fig.subplots_adjust(bottom=0.5)\n",
    "\n",
    "    # Set the colormap and normalization\n",
    "    norm = mcolors.Normalize(vmin=min_value, vmax=max_value)\n",
    "    cmap = colormap\n",
    "\n",
    "    # Create a colorbar in the figure\n",
    "    cb = cm.ScalarMappable(norm=norm, cmap=cmap)\n",
    "    cb.set_array([])  # You need this dummy call to add the array to the ScalarMappable\n",
    "    cbar = plt.colorbar(cb, orientation='horizontal', cax=ax)\n",
    "    cbar.set_label('Slip Intensity Values (nm)')\n",
    "\n",
    "    # Show the colorbar\n",
    "    plt.show()\n",
    "\n",
    "def visualize_coco_masks_bbox_black(image_path, coco_json_path, output_path, figure_size=(50, 50), font_size=20, black=False, bbox=False):\n",
    "    if black:\n",
    "        image_path = create_black_duplicate(image_path)\n",
    "    \n",
    "    with open(coco_json_path, 'r') as f:\n",
    "        coco_data = json.load(f)\n",
    "\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", size=font_size)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()\n",
    "\n",
    "    for annotation in coco_data[\"annotations\"]:\n",
    "        segmentation = annotation[\"segmentation\"][0]\n",
    "        color = tuple(random.choices(range(100, 256), k=3))\n",
    "\n",
    "        draw.polygon(segmentation, outline=color, fill=None)\n",
    "        centroid_x = sum(segmentation[::2]) / (len(segmentation) / 2)\n",
    "        centroid_y = sum(segmentation[1::2]) / (len(segmentation) / 2)\n",
    "        draw.text((centroid_x + 5, centroid_y), str(annotation['id']), fill=color, font=font)\n",
    "\n",
    "        if bbox:\n",
    "            bbox_coords = [annotation[\"bbox\"][0], annotation[\"bbox\"][1], annotation[\"bbox\"][0] + annotation[\"bbox\"][2], annotation[\"bbox\"][1] + annotation[\"bbox\"][3]]\n",
    "            draw.rectangle(bbox_coords, outline=color, width=2)\n",
    "\n",
    "    plt.figure(figsize=figure_size)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    image.save(output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cd728d",
   "metadata": {},
   "source": [
    "# Code Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be5623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Machine_Learning\\DARPA_Graphs_Reruns\\A024_316L_to_In625_T1\\A024S2A_I3_BlN_step1_predictions.json\n",
      "No previous predictions found for D:\\Machine_Learning\\DARPA_Graphs_Reruns\\A024_316L_to_In625_T1\\A024S2A_I3_BlN_step1.tif\n",
      "\n",
      "0: 1024x1024 (no detections), 20.7ms\n",
      "Speed: 10.9ms preprocess, 20.7ms inference, 60.2ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 4.5ms preprocess, 14.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.6ms\n",
      "Speed: 4.0ms preprocess, 20.6ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 21.6ms\n",
      "Speed: 4.0ms preprocess, 21.6ms inference, 0.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.5ms\n",
      "Speed: 4.5ms preprocess, 15.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 19.5ms\n",
      "Speed: 4.5ms preprocess, 19.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 13.4ms\n",
      "Speed: 5.0ms preprocess, 13.4ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.1ms\n",
      "Speed: 4.0ms preprocess, 20.1ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.0ms\n",
      "Speed: 5.0ms preprocess, 15.0ms inference, 7.2ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 19.6ms\n",
      "Speed: 5.0ms preprocess, 19.6ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.9ms\n",
      "Speed: 4.0ms preprocess, 20.9ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.0ms\n",
      "Speed: 5.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 12.1ms\n",
      "Speed: 4.0ms preprocess, 12.1ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.9ms\n",
      "Speed: 4.0ms preprocess, 15.9ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.5ms\n",
      "Speed: 5.0ms preprocess, 20.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.1ms\n",
      "Speed: 5.5ms preprocess, 15.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 4.0ms preprocess, 14.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 11.1ms\n",
      "Speed: 4.0ms preprocess, 11.1ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 13.6ms\n",
      "Speed: 4.0ms preprocess, 13.6ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 13.7ms\n",
      "Speed: 4.0ms preprocess, 13.7ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 19.2ms\n",
      "Speed: 4.0ms preprocess, 19.2ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.3ms\n",
      "Speed: 4.0ms preprocess, 14.3ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 8.2ms\n",
      "Speed: 4.5ms preprocess, 8.2ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 10.5ms preprocess, 14.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.5ms\n",
      "Speed: 5.0ms preprocess, 20.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 18.4ms\n",
      "Speed: 5.0ms preprocess, 18.4ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 16.2ms\n",
      "Speed: 4.0ms preprocess, 16.2ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 9.0ms\n",
      "Speed: 4.5ms preprocess, 9.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 10.4ms\n",
      "Speed: 4.0ms preprocess, 10.4ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 10.2ms\n",
      "Speed: 4.0ms preprocess, 10.2ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 21.0ms\n",
      "Speed: 5.1ms preprocess, 21.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.1ms\n",
      "Speed: 4.5ms preprocess, 15.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 13.5ms\n",
      "Speed: 4.1ms preprocess, 13.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.6ms\n",
      "Speed: 3.0ms preprocess, 14.6ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 19.5ms\n",
      "Speed: 4.0ms preprocess, 19.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.5ms\n",
      "Speed: 3.0ms preprocess, 20.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.5ms\n",
      "Speed: 4.0ms preprocess, 15.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.0ms\n",
      "Speed: 3.5ms preprocess, 15.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.0ms\n",
      "Speed: 10.1ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 19.1ms\n",
      "Speed: 4.0ms preprocess, 19.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 22.0ms\n",
      "Speed: 4.5ms preprocess, 22.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.0ms\n",
      "Speed: 4.1ms preprocess, 14.0ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.6ms\n",
      "Speed: 5.0ms preprocess, 14.6ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 19.5ms\n",
      "Speed: 5.0ms preprocess, 19.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.1ms\n",
      "Speed: 5.1ms preprocess, 20.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.1ms\n",
      "Speed: 5.0ms preprocess, 20.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 13.1ms\n",
      "Speed: 5.0ms preprocess, 13.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.0ms\n",
      "Speed: 5.0ms preprocess, 20.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 21.1ms\n",
      "Speed: 4.5ms preprocess, 21.1ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 4.0ms preprocess, 14.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.0ms\n",
      "Speed: 6.1ms preprocess, 20.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.6ms\n",
      "Speed: 4.1ms preprocess, 20.6ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.2ms\n",
      "Speed: 10.0ms preprocess, 14.2ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 12.0ms\n",
      "Speed: 5.1ms preprocess, 12.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.0ms\n",
      "Speed: 4.0ms preprocess, 20.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 13.3ms\n",
      "Speed: 4.0ms preprocess, 13.3ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 12.7ms\n",
      "Speed: 4.0ms preprocess, 12.7ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 4.0ms preprocess, 14.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 17.0ms\n",
      "Speed: 3.5ms preprocess, 17.0ms inference, 6.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.2ms\n",
      "Speed: 4.0ms preprocess, 14.2ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 19.8ms\n",
      "Speed: 4.0ms preprocess, 19.8ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 16.5ms\n",
      "Speed: 4.0ms preprocess, 16.5ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.4ms\n",
      "Speed: 3.5ms preprocess, 15.4ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 21.1ms\n",
      "Speed: 4.0ms preprocess, 21.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 13.6ms\n",
      "Speed: 4.0ms preprocess, 13.6ms inference, 6.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 9.0ms\n",
      "Speed: 4.5ms preprocess, 9.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.8ms\n",
      "Speed: 5.0ms preprocess, 20.8ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.0ms\n",
      "Speed: 4.4ms preprocess, 14.0ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 19.0ms\n",
      "Speed: 5.0ms preprocess, 19.0ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 22.0ms\n",
      "Speed: 5.5ms preprocess, 22.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 18.8ms\n",
      "Speed: 6.0ms preprocess, 18.8ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 16.0ms\n",
      "Speed: 5.5ms preprocess, 16.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 19.7ms\n",
      "Speed: 5.0ms preprocess, 19.7ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 16.0ms\n",
      "Speed: 9.7ms preprocess, 16.0ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 21.7ms\n",
      "Speed: 5.0ms preprocess, 21.7ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 16.3ms\n",
      "Speed: 3.5ms preprocess, 16.3ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.1ms\n",
      "Speed: 4.0ms preprocess, 15.1ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 16.5ms\n",
      "Speed: 4.0ms preprocess, 16.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 18.3ms\n",
      "Speed: 5.0ms preprocess, 18.3ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 19.5ms\n",
      "Speed: 8.0ms preprocess, 19.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 5.0ms preprocess, 14.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.5ms\n",
      "Speed: 4.1ms preprocess, 14.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.6ms\n",
      "Speed: 4.0ms preprocess, 14.6ms inference, 4.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.0ms\n",
      "Speed: 3.5ms preprocess, 20.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.5ms\n",
      "Speed: 4.1ms preprocess, 15.5ms inference, 7.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.0ms\n",
      "Speed: 4.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.1ms\n",
      "Speed: 5.1ms preprocess, 20.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 21.1ms\n",
      "Speed: 4.0ms preprocess, 21.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 10.0ms\n",
      "Speed: 3.0ms preprocess, 10.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.0ms\n",
      "Speed: 5.5ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.1ms\n",
      "Speed: 4.0ms preprocess, 15.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 3.0ms preprocess, 14.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 21.6ms\n",
      "Speed: 4.0ms preprocess, 21.6ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.5ms\n",
      "Speed: 4.0ms preprocess, 15.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 4.0ms preprocess, 14.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.2ms\n",
      "Speed: 5.0ms preprocess, 20.2ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 11.1ms\n",
      "Speed: 5.0ms preprocess, 11.1ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.5ms\n",
      "Speed: 5.0ms preprocess, 14.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 19.1ms\n",
      "Speed: 4.1ms preprocess, 19.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 4.0ms preprocess, 14.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 10.1ms\n",
      "Speed: 5.0ms preprocess, 10.1ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.0ms\n",
      "Speed: 4.1ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.5ms\n",
      "Speed: 5.1ms preprocess, 20.5ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.0ms\n",
      "Speed: 6.1ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.5ms\n",
      "Speed: 3.5ms preprocess, 15.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 16.8ms\n",
      "Speed: 8.0ms preprocess, 16.8ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.1ms\n",
      "Speed: 4.0ms preprocess, 20.1ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 13.9ms\n",
      "Speed: 4.0ms preprocess, 13.9ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 21.7ms\n",
      "Speed: 3.0ms preprocess, 21.7ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 18.9ms\n",
      "Speed: 3.5ms preprocess, 18.9ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 4.0ms preprocess, 14.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.3ms\n",
      "Speed: 4.0ms preprocess, 14.3ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 18.9ms\n",
      "Speed: 4.0ms preprocess, 18.9ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.8ms\n",
      "Speed: 4.0ms preprocess, 15.8ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.8ms\n",
      "Speed: 4.0ms preprocess, 14.8ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.6ms\n",
      "Speed: 3.0ms preprocess, 14.6ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 9.5ms preprocess, 14.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.4ms\n",
      "Speed: 3.0ms preprocess, 20.4ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 9.0ms\n",
      "Speed: 4.0ms preprocess, 9.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 5.0ms preprocess, 14.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 18.2ms\n",
      "Speed: 5.0ms preprocess, 18.2ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 13.8ms\n",
      "Speed: 10.0ms preprocess, 13.8ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.2ms\n",
      "Speed: 5.0ms preprocess, 20.2ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 13.3ms\n",
      "Speed: 4.1ms preprocess, 13.3ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.8ms\n",
      "Speed: 4.0ms preprocess, 20.8ms inference, 0.4ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 16.0ms\n",
      "Speed: 4.5ms preprocess, 16.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.0ms\n",
      "Speed: 4.5ms preprocess, 14.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 5.0ms preprocess, 14.1ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 19.5ms\n",
      "Speed: 4.0ms preprocess, 19.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 8.0ms\n",
      "Speed: 4.0ms preprocess, 8.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.0ms\n",
      "Speed: 4.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 19.8ms\n",
      "Speed: 4.4ms preprocess, 19.8ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.0ms\n",
      "Speed: 6.0ms preprocess, 15.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 16.1ms\n",
      "Speed: 5.5ms preprocess, 16.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.0ms\n",
      "Speed: 3.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 19.5ms\n",
      "Speed: 4.3ms preprocess, 19.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 18.5ms\n",
      "Speed: 5.0ms preprocess, 18.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.0ms\n",
      "Speed: 10.2ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.0ms\n",
      "Speed: 4.0ms preprocess, 14.0ms inference, 3.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.5ms\n",
      "Speed: 3.0ms preprocess, 15.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.9ms\n",
      "Speed: 4.1ms preprocess, 20.9ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.0ms\n",
      "Speed: 3.1ms preprocess, 14.0ms inference, 6.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 5.0ms preprocess, 14.1ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.5ms\n",
      "Speed: 3.0ms preprocess, 20.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 11.0ms\n",
      "Speed: 4.0ms preprocess, 11.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.0ms\n",
      "Speed: 4.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 5.0ms preprocess, 14.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 19.9ms\n",
      "Speed: 4.0ms preprocess, 19.9ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 8.0ms\n",
      "Speed: 5.0ms preprocess, 8.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 23.6ms\n",
      "Speed: 5.0ms preprocess, 23.6ms inference, 4.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 10.5ms\n",
      "Speed: 5.0ms preprocess, 10.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 9.1ms\n",
      "Speed: 4.0ms preprocess, 9.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 9.1ms\n",
      "Speed: 4.0ms preprocess, 9.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.5ms\n",
      "Speed: 4.4ms preprocess, 14.5ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.3ms\n",
      "Speed: 4.5ms preprocess, 20.3ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 22.6ms\n",
      "Speed: 4.0ms preprocess, 22.6ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 16.9ms\n",
      "Speed: 5.0ms preprocess, 16.9ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 21.0ms\n",
      "Speed: 3.5ms preprocess, 21.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 5.0ms preprocess, 14.1ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 22.5ms\n",
      "Speed: 5.1ms preprocess, 22.5ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 21.3ms\n",
      "Speed: 4.0ms preprocess, 21.3ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 21.8ms\n",
      "Speed: 3.4ms preprocess, 21.8ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 12.1ms\n",
      "Speed: 4.0ms preprocess, 12.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.0ms\n",
      "Speed: 5.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 29.7ms\n",
      "Speed: 6.0ms preprocess, 29.7ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 16.9ms\n",
      "Speed: 4.0ms preprocess, 16.9ms inference, 6.7ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 4.0ms preprocess, 14.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 17.1ms\n",
      "Speed: 4.5ms preprocess, 17.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 21.7ms\n",
      "Speed: 4.0ms preprocess, 21.7ms inference, 4.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 10.0ms\n",
      "Speed: 4.0ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 16.0ms\n",
      "Speed: 4.5ms preprocess, 16.0ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.5ms\n",
      "Speed: 24.4ms preprocess, 20.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 10.0ms\n",
      "Speed: 4.5ms preprocess, 10.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.2ms\n",
      "Speed: 11.5ms preprocess, 15.2ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.3ms\n",
      "Speed: 5.0ms preprocess, 20.3ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 12.0ms preprocess, 14.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 21.2ms\n",
      "Speed: 5.0ms preprocess, 21.2ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.0ms\n",
      "Speed: 9.5ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 16.4ms\n",
      "Speed: 6.0ms preprocess, 16.4ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 21.5ms\n",
      "Speed: 16.5ms preprocess, 21.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.6ms\n",
      "Speed: 3.0ms preprocess, 15.6ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 8.0ms\n",
      "Speed: 6.0ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 19.2ms\n",
      "Speed: 4.0ms preprocess, 19.2ms inference, 0.5ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 28.5ms\n",
      "Speed: 27.8ms preprocess, 28.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.2ms\n",
      "Speed: 4.0ms preprocess, 14.2ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 19.3ms\n",
      "Speed: 13.2ms preprocess, 19.3ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.0ms\n",
      "Speed: 4.0ms preprocess, 14.0ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 21.6ms\n",
      "Speed: 3.1ms preprocess, 21.6ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 21.9ms\n",
      "Speed: 4.0ms preprocess, 21.9ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 10.5ms\n",
      "Speed: 4.0ms preprocess, 10.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.0ms\n",
      "Speed: 11.3ms preprocess, 14.0ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 21.8ms\n",
      "Speed: 15.0ms preprocess, 21.8ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 8.4ms\n",
      "Speed: 5.0ms preprocess, 8.4ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.5ms\n",
      "Speed: 10.0ms preprocess, 14.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.2ms\n",
      "Speed: 4.0ms preprocess, 20.2ms inference, 3.2ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 11.7ms\n",
      "Speed: 4.0ms preprocess, 11.7ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.0ms\n",
      "Speed: 4.5ms preprocess, 15.0ms inference, 3.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 13.0ms\n",
      "Speed: 4.0ms preprocess, 13.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.0ms\n",
      "Speed: 4.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 21.1ms\n",
      "Speed: 4.0ms preprocess, 21.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 19.2ms\n",
      "Speed: 4.5ms preprocess, 19.2ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 9.0ms\n",
      "Speed: 4.0ms preprocess, 9.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.0ms\n",
      "Speed: 4.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.0ms\n",
      "Speed: 3.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.0ms\n",
      "Speed: 4.0ms preprocess, 14.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 19.5ms\n",
      "Speed: 4.0ms preprocess, 19.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.2ms\n",
      "Speed: 4.0ms preprocess, 15.2ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 26.3ms\n",
      "Speed: 5.0ms preprocess, 26.3ms inference, 1.3ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 12.0ms\n",
      "Speed: 5.0ms preprocess, 12.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 16.3ms\n",
      "Speed: 33.0ms preprocess, 16.3ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 19.5ms\n",
      "Speed: 5.0ms preprocess, 19.5ms inference, 1.6ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 9.5ms\n",
      "Speed: 4.3ms preprocess, 9.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.5ms\n",
      "Speed: 29.8ms preprocess, 14.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.5ms\n",
      "Speed: 5.0ms preprocess, 20.5ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.2ms\n",
      "Speed: 5.0ms preprocess, 14.2ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 21.7ms\n",
      "Speed: 5.0ms preprocess, 21.7ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.0ms\n",
      "Speed: 4.0ms preprocess, 15.0ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 4.0ms preprocess, 14.1ms inference, 6.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.2ms\n",
      "Speed: 4.0ms preprocess, 20.2ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 4.1ms preprocess, 14.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.1ms\n",
      "Speed: 4.5ms preprocess, 15.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 19.1ms\n",
      "Speed: 5.0ms preprocess, 19.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 4.0ms preprocess, 14.1ms inference, 2.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.5ms\n",
      "Speed: 4.0ms preprocess, 20.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 4.0ms preprocess, 14.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.6ms\n",
      "Speed: 5.0ms preprocess, 20.6ms inference, 1.1ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 19.1ms\n",
      "Speed: 6.1ms preprocess, 19.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 5.0ms preprocess, 14.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 19.1ms\n",
      "Speed: 5.0ms preprocess, 19.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.5ms\n",
      "Speed: 4.0ms preprocess, 14.5ms inference, 2.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.5ms\n",
      "Speed: 4.0ms preprocess, 20.5ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 20.0ms\n",
      "Speed: 6.0ms preprocess, 20.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 14.1ms\n",
      "Speed: 6.0ms preprocess, 14.1ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 18.6ms\n",
      "Speed: 5.0ms preprocess, 18.6ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 16.1ms\n",
      "Speed: 6.0ms preprocess, 16.1ms inference, 0.0ms postprocess per image at shape (1, 3, 1024, 1024)\n",
      "\n",
      "0: 1024x1024 (no detections), 15.0ms\n",
      "Speed: 5.0ms preprocess, 15.0ms inference, 1.0ms postprocess per image at shape (1, 3, 1024, 1024)\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------Main Execution Code ----------------------------\n",
    "# Increase the maximum number of pixels that can be loaded.\n",
    "Image.MAX_IMAGE_PIXELS = None  # This disables the decompression bomb check entirely.\n",
    "# Alternatively, set a new limit that's specific to your image size, if known and reasonable\n",
    "\n",
    "# Start of Code\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "for image_path in image_paths:\n",
    "    output_folder = Path(base_output_folder) / Path(image_path).stem\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    #--------------------------- Initial Predictions -------------------------------------------------\n",
    "    raw_image_path = image_path\n",
    "\n",
    "    #Find previous Annotations from prior Steps\n",
    "    base_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "    previous_json_path = os.path.join(os.path.dirname(image_path), f\"{base_name}_predictions.json\")\n",
    "    print(previous_json_path)\n",
    "\n",
    "    #Preprocess the Image\n",
    "    raw_image_path = apply_previous_predictions(image_path, previous_json_path, output_folder)\n",
    "    preprocessed_image_path = preprocess(raw_image_path, output_folder)\n",
    "    preprocessed_original_image_path = preprocess(image_path, output_folder)\n",
    "\n",
    "    #Padd the Image\n",
    "    padded_image_path, top_padding, bottom_padding, left_padding, right_padding = pad_image(preprocessed_image_path, output_folder, min_padding=256)\n",
    "    final_image_path = padded_image_path\n",
    "    image_name = os.path.basename(final_image_path)\n",
    "\n",
    "    # Loop through the steps for counters 0 to 64 \n",
    "    for counter in range(10):\n",
    "        # Slice the Image\n",
    "        shift_distance = 128\n",
    "        slice_folder_path = slice_image_to_1024(padded_image_path, output_folder, counter, shift_distance)\n",
    "\n",
    "        # Predict on Image\n",
    "        model = YOLO(model_path)\n",
    "        model.to('cuda')\n",
    "        output_json_path = process_images(slice_folder_path, output_folder, model, counter, min_size=200, area_threshold=30)\n",
    "\n",
    "        # Convert to large image\n",
    "        output_file_path = convert_coco_to_large_image(output_json_path, output_folder, padded_image_path, counter)\n",
    "\n",
    "        # Mask the Image\n",
    "        masked_image_path = mask_image_with_coco_masks(padded_image_path, output_file_path, output_folder, counter, dilation_distance=5)\n",
    "\n",
    "        # Update the padded_image_path for the next iteration\n",
    "        padded_image_path = masked_image_path\n",
    "\n",
    "    output_json = 'Final_Compiled_Predictions.json'\n",
    "    final_compiled_predictions_path = compile_coco_json(output_folder, output_json, image_name)\n",
    "\n",
    "    #--------------------------- Visualize Initial Predictions -------------------------------------------------\n",
    "    initial_masked_image_path = os.path.join(output_folder, \"Initial_Masked_Image.jpg\")\n",
    "    visualize_coco_masks(final_image_path, final_compiled_predictions_path, initial_masked_image_path, figure_size=(100, 100), font_size=20)\n",
    "\n",
    "    #-------------------------- Remove Padding from the Images --------------------------------------------------\n",
    "    coco_json_path = final_compiled_predictions_path\n",
    "    file_name = 'Final_Compiled_Predictions_Adjusted.json'\n",
    "    adjusted_predictions_path = os.path.join(output_folder, file_name)\n",
    "    adjust_coco_annotations(coco_json_path, top_padding, bottom_padding, left_padding, right_padding, adjusted_predictions_path)\n",
    "\n",
    "    #-------------------------- Add in Previous Predictions -------------------------------------------------\n",
    "    full_adjusted_predictions_path = os.path.join(output_folder, 'Final_Compiled_Predictions_Cleaned.json')\n",
    "    add_annotations_to_coco_json(adjusted_predictions_path, previous_json_path, full_adjusted_predictions_path)\n",
    "\n",
    "    #\n",
    "    masked_image_path = os.path.join(output_folder, \"Masked_Image.jpg\")\n",
    "    visualize_coco_masks(preprocessed_original_image_path, full_adjusted_predictions_path, masked_image_path ,figure_size=(100, 100), font_size=20)\n",
    "    \n",
    "    #--------------------------- Clean Initial Predictions -------------------------------------------------\n",
    "    clean_annotations(preprocessed_original_image_path, full_adjusted_predictions_path, full_adjusted_predictions_path, intensity_threshold=1)\n",
    "\n",
    "    clean_masked_image_path = os.path.join(output_folder, \"Clean_Masked_Image.jpg\")\n",
    "    visualize_coco_masks(preprocessed_original_image_path, full_adjusted_predictions_path, clean_masked_image_path, figure_size=(100, 100), font_size=20)\n",
    "\n",
    "    #--------------------------- Merge Predictions -------------------------------------------------\n",
    "    full_adjusted_cleaned_predictions_path = os.path.join(output_folder, 'Final_Compiled_Predictions_Cleaned_Adjusted.json')\n",
    "\n",
    "    # Load initial annotations\n",
    "    start_time_merging = time.perf_counter()\n",
    "\n",
    "    annotations = process_coco_json(full_adjusted_predictions_path)\n",
    "    print('Intial Number of Features:', len(annotations))\n",
    "    final_annotations = progressive_merge_annotations(annotations)\n",
    "    print('Final Number of Features:', len(final_annotations))\n",
    "    update_json_file(final_annotations, full_adjusted_cleaned_predictions_path, preprocessed_original_image_path, category_id=1)\n",
    "    print(\"Combination complete. Updated annotations saved.\")\n",
    "\n",
    "    end_time_merging = time.perf_counter()\n",
    "    elapsed_seconds = end_time_merging - start_time_merging\n",
    "    hours, minutes, seconds = convert_seconds_to_hms(elapsed_seconds)\n",
    "    print(f\"Execution time for Merging: {hours} hour(s), {minutes} minute(s), {seconds:.2f} second(s)\")\n",
    "\n",
    "    #-------------------------- Visualize the Merged Predictions -------------------------------------------------\n",
    "    final_masked_image_cleaned_path = os.path.join(output_folder, \"Initial_Masked_Image_Cleaned.jpg\")\n",
    "    visualize_coco_masks(preprocessed_original_image_path, full_adjusted_cleaned_predictions_path, final_masked_image_cleaned_path ,figure_size=(100, 100), font_size=20)\n",
    "\n",
    "    final_masked_image_cleaned_bbox_path = os.path.join(output_folder, \"Initial_Masked_Image_Cleaned_bbox.jpg\")\n",
    "    visualize_coco_masks_bbox(preprocessed_original_image_path, full_adjusted_cleaned_predictions_path, final_masked_image_cleaned_bbox_path ,figure_size=(100, 100), font_size=20)\n",
    "\n",
    "    #-------------------------- Statistics Postprcessing --------------------------------------------------\n",
    "    start_time_stats = time.perf_counter()\n",
    "    # Load JSON and coco annotations\n",
    "    data = load_json(full_adjusted_cleaned_predictions_path)\n",
    "    coco = load_coco_annotations(full_adjusted_cleaned_predictions_path)\n",
    "    data_list = []\n",
    "\n",
    "    NaN_Image_name = 'nan_masked_image.tif'\n",
    "    mask_save_path = os.path.join(output_folder, NaN_Image_name)\n",
    "\n",
    "    # Create or load the NaN-masked image\n",
    "    image_nan_masked = tiff.imread(mask_save_path) if os.path.exists(mask_save_path) else create_nan_masked_image(coco, image_path)\n",
    "    if not os.path.exists(mask_save_path):\n",
    "        tiff.imwrite(mask_save_path, image_nan_masked.get(), dtype=np.float32)\n",
    "\n",
    "    # Collect Annotations \n",
    "    annotation_ids_to_analyze = []\n",
    "    annotations = data['annotations'] if not annotation_ids_to_analyze else [\n",
    "        annotation for annotation in data['annotations'] if annotation['id'] in annotation_ids_to_analyze\n",
    "    ]\n",
    "\n",
    "    # Parallel processing of annotations with concurrent.futures\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(process_annotation, annotation, coco, image_path, image_nan_masked) for annotation in annotations]\n",
    "        data_list = [future.result() for future in concurrent.futures.as_completed(futures)]\n",
    "\n",
    "    # Convert CuPy arrays in data_list to NumPy if necessary\n",
    "    data_list = [\n",
    "        {k: (v.get() if isinstance(v, cp.ndarray) else v) for k, v in item.items()}\n",
    "        for item in data_list\n",
    "    ]\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    filename = 'BlN_Stats.csv'\n",
    "    BLN_Stats_Path = os.path.join(output_folder, filename)\n",
    "    df = pd.DataFrame(data_list)\n",
    "    # Only sort if 'Annotation ID' is a column in df\n",
    "    if not df.empty and 'Annotation ID' in df.columns:\n",
    "        df = df.sort_values(by='Annotation ID')\n",
    "        df.to_csv(BLN_Stats_Path, index=False)\n",
    "\n",
    "    end_time_stats = time.perf_counter()\n",
    "    elapsed_seconds = end_time_stats - start_time_stats\n",
    "    hours, minutes, seconds = convert_seconds_to_hms(elapsed_seconds)\n",
    "    print(f\"Execution time for Statistics: {hours} hour(s), {minutes} minute(s), {seconds:.2f} second(s)\")\n",
    "\n",
    "    #------------------------- Node Visualization ---------------------------------------------------\n",
    "    # Assuming the DataFrame 'df' is already loaded and filtered for NaNs\n",
    "    if not df.empty:\n",
    "        df_clean = df.dropna(subset=['Centroid X', 'Centroid Y', 'Fitted Max Value'])\n",
    "        df_clean = df_clean.reset_index(drop=True)\n",
    "\n",
    "        #Example Usage\n",
    "        filename = 'Node_Overlay.jpg'  # Specify the filename for your plot\n",
    "        output_path = f'{output_folder}/{filename}'\n",
    "\n",
    "        visualize_nodes_on_image_with_colormap(preprocessed_original_image_path, df_clean, output_path, black=True)\n",
    "\n",
    "    #------------------------- Node Visualization with Connections --------------------------------------------------\n",
    "        df_clean = df.dropna(subset=['Centroid X', 'Centroid Y', 'Fitted Max Value'])\n",
    "        df_clean = df_clean.reset_index(drop=True)\n",
    "        \n",
    "        filename = 'Node_Overlay_with_Connections.jpg'\n",
    "        output_path = f'{output_folder}/{filename}'\n",
    "        visualize_and_connect_nodes(preprocessed_original_image_path, df_clean, output_path, black=False)\n",
    "        filename = 'Node_Overlay_with_Connections_Black.jpg'\n",
    "        output_path = f'{output_folder}/{filename}'\n",
    "        visualize_and_connect_nodes(preprocessed_original_image_path, df_clean, output_path, black=True)\n",
    "\n",
    "    #------------------------- Display the colorbar ---------------------------------------------------\n",
    "        min_value = df_clean['Fitted Max Value'].min() * 22.46\n",
    "        max_value = df_clean['Fitted Max Value'].max() * 22.46\n",
    "        create_colorbar(min_value, max_value, cm.rainbow)\n",
    "\n",
    "    #------------------------- Final Visualizations --------------------------------------------------\n",
    "    final_masked_image_cleaned_adjusted_path = os.path.join(output_folder, \"Final_Masked_Image_Cleaned_Adjusted.jpg\")\n",
    "    visualize_coco_masks_bbox_black(preprocessed_original_image_path, full_adjusted_cleaned_predictions_path, final_masked_image_cleaned_adjusted_path ,figure_size=(100, 100), font_size=20, black=False, bbox=False)\n",
    "\n",
    "    final_masked_image_cleaned_adjusted_bbox_path = os.path.join(output_folder, \"Final_Masked_Image_Cleaned_Adjusted_bbox.jpg\")\n",
    "    visualize_coco_masks_bbox_black(preprocessed_original_image_path, full_adjusted_cleaned_predictions_path, final_masked_image_cleaned_adjusted_bbox_path ,figure_size=(100, 100), font_size=20, black=False, bbox=True)\n",
    "\n",
    "    final_masked_image_cleaned_adjusted_black_path = os.path.join(output_folder, \"Final_Masked_Image_Cleaned_Adjusted_black.jpg\")\n",
    "    visualize_coco_masks_bbox_black(preprocessed_original_image_path, full_adjusted_cleaned_predictions_path, final_masked_image_cleaned_adjusted_black_path ,figure_size=(100, 100), font_size=20, black=True, bbox=False)\n",
    "\n",
    "    final_masked_image_cleaned_adjusted_bbox_black_path = os.path.join(output_folder, \"Final_Masked_Image_Cleaned_Adjusted_bbox_black.jpg\")\n",
    "    visualize_coco_masks_bbox_black(preprocessed_original_image_path, full_adjusted_cleaned_predictions_path, final_masked_image_cleaned_adjusted_bbox_black_path ,figure_size=(100, 100), font_size=20, black=True, bbox=True)\n",
    "\n",
    "    #------------------------- Create Histogram --------------------------------------------------\n",
    "    if not df.empty:\n",
    "        # Multiply the 'Fitted Max Value' by the conversion factor and store in a new column\n",
    "        df_clean['Converted Max Value'] = df_clean['Fitted Max Value'] * conversion_factor\n",
    "\n",
    "        # Now, use this new column for the plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.violinplot(y='Converted Max Value', data=df_clean, inner=None, color='red')\n",
    "        sns.stripplot(y='Converted Max Value', data=df_clean, size=4, jitter=True, color='k', edgecolor='k', alpha=0.7)\n",
    "\n",
    "        # Adjust the violins to only show one side\n",
    "        ax = plt.gca()\n",
    "        violins = [child for child in ax.get_children() if isinstance(child, PolyCollection)]\n",
    "        for violin in violins:\n",
    "            paths = violin.get_paths()\n",
    "            for path in paths:\n",
    "                vertices = path.vertices\n",
    "                median = np.median(vertices[:, 0])\n",
    "                vertices[vertices[:, 0] > median, 0] = median  # Adjust to keep only the left side of the violin\n",
    "\n",
    "        plt.title('Distribution of Maximum Slip Intensity Across the Dataset')\n",
    "        plt.ylabel('Max Slip Intensity (nm)')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    #------------------------- Organize Files ---------------------------------------------------\n",
    "    base_dir = output_folder\n",
    "\n",
    "    # List of files to move to the 'Clean_Results' folder\n",
    "    clean_results_files = [\n",
    "        \"Final_Compiled_Predictions_Cleaned_Adjusted.json\",\n",
    "        \"Final_Masked_Image_Cleaned_Adjusted.jpg\",\n",
    "        \"Final_Masked_Image_Cleaned_Adjusted_bbox.jpg\",\n",
    "        \"Final_Masked_Image_Cleaned_Adjusted_black.jpg\",\n",
    "        \"Final_Masked_Image_Cleaned_Adjusted_bbox_black.jpg\",\n",
    "        \"BlN_Stats.csv\",\n",
    "        \"Node_Overlay_with_Connections.jpg\",\n",
    "        \"Node_Overlay_with_Connections_Black.jpg\"\n",
    "    ]\n",
    "\n",
    "    # Ensure 'Clean_Results' folder exists\n",
    "    clean_results_dir = os.path.join(base_dir, 'Clean_Results')\n",
    "    os.makedirs(clean_results_dir, exist_ok=True)\n",
    "\n",
    "    # Ensure 'Processing_Files' folder exists\n",
    "    processing_files_dir = os.path.join(base_dir, 'Processing_Files')\n",
    "    os.makedirs(processing_files_dir, exist_ok=True)\n",
    "\n",
    "    # Move specified files to 'Clean_Results'\n",
    "    for file_name in clean_results_files:\n",
    "        source_path = os.path.join(base_dir, file_name)\n",
    "        if os.path.exists(source_path):\n",
    "            shutil.move(source_path, os.path.join(clean_results_dir, file_name))\n",
    "\n",
    "    # Move remaining files and folders to 'Processing_Files'\n",
    "    for item in os.listdir(base_dir):\n",
    "        item_path = os.path.join(base_dir, item)\n",
    "        if item_path not in [clean_results_dir, processing_files_dir]:\n",
    "            shutil.move(item_path, os.path.join(processing_files_dir, item))\n",
    "\n",
    "    # Copy and rename Final_Compiled_Predictions_Cleaned_Adjusted.json with incremented number\n",
    "    json_file_path = os.path.join(clean_results_dir, \"Final_Compiled_Predictions_Cleaned_Adjusted.json\")\n",
    "    if os.path.exists(json_file_path):\n",
    "        # Get the base name of the original image without extension\n",
    "        base_name = Path(image_path).stem\n",
    "\n",
    "        # Find the final number in the base name using a regex\n",
    "        match = re.search(r'(\\d+)(?!.*\\d)', base_name)\n",
    "        if match:\n",
    "            # Extract the number, increment it, and replace it in the base name\n",
    "            number = int(match.group(0)) + 1\n",
    "            new_base_name = re.sub(r'(\\d+)(?!.*\\d)', str(number), base_name)\n",
    "            new_file_name = f\"{new_base_name}_predictions.json\"\n",
    "            \n",
    "            destination_path = os.path.join(base_output_folder, new_file_name)\n",
    "\n",
    "            # Copy and rename the JSON file\n",
    "            shutil.copy(json_file_path, destination_path)\n",
    "\n",
    "    print(\"Files have been organized.\")\n",
    "\n",
    "    #--------------------- Calculate End time -------------------------------------------------------\n",
    "    end_time = time.perf_counter()\n",
    "    elapsed_seconds = end_time - start_time\n",
    "    hours, minutes, seconds = convert_seconds_to_hms(elapsed_seconds)\n",
    "    print(f\"Execution time: {hours} hour(s), {minutes} minute(s), {seconds:.2f} second(s)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
